[["index.html", "Introduction aux Data Sciences Chapitre 1 Préface 1.1 Plan du manuel 1.2 Les jeux de données 1.3 Le cadre technique et les packages utilisés 1.4 A faire", " Introduction aux Data Sciences Christophe Benavent - Université Paris Dauphine 2022-03-27 Chapitre 1 Préface Ce bookdown reprend les éléments dun cours de data science avec r. Il a pour but dêtre reproductible, cest pourquoi le choix de ce support et des jeux de données associés. Il sera dynamique, modifié à mesure de nos cours, séminaires et ateliers. Lillustration représente lévolution de la longueur des films de la base Imbd et raconte en chiffres une part de lhistoire du cinema. Jusquau années la longueur est hétérogène mais au tournant des années 30, la distribution se stabilise, les court-métragessont de lordre de 15mn, et voient leurs durée se raccourcir avec les décénnies, le genre menace de disparaitredans les années 80 et reprend du poil de la bête dans les années 2000. Les films longs voient leur longueur saccroitre et se stabiliser autur de un peu moins de 100 mn, soit une heure et trente minutes. On observera enfin quau cours des années 1990 les film de taille intermédiaires réapparaissent. Dans ce graphique il y a tous les éléments des data sciences modernes : un jeu de donnée riche et systématique, un modèle statistique fondamentale avec la notion de densité de probabilité, une mesure, un critère de comparaison. Les diagrammes ridges sont inspirée de la pochette de lalbum Unknown Pleasures de Joy division sorti en pleine New Wave , en 1979. Un article de Vice en rappele lorigine et le destin de ce graphisme 1.1 Plan du manuel Les chapitres projetés 1 - Lenvironnement r 2 - Installation et prise en main 3 - Usage de ggplot - uni et bivarié 4 - Usage de ggplot - multivarié 6 - Analyse de variance et régression linéraire 5 - Tables 6 - Modèles factoriels ( Psych) 7 - AFC 8 - MDS 9 - Clustering 10 - Analyse de réseaux 10 - Modèle déquations structurelles (Lavaan) 11 - Modèle linéaire généralisé 12 - Modèles à décomposition derreur 13 - Times series 14 - Analyse geospatiale 15 - Machine learning 1.2 Les jeux de données Au cours du développement, plusieurs cas pratiques - souvent réduit en volume pour rester exemplaire, seront employés. Les donées seront partagées. En voici la présentation systématique. disponibles dans le repositery avec le code du book. Les amendements et améliorations sont souhaitées et attendues. 1.3 Le cadre technique et les packages utilisés Ce livre est écrit en Markdown (Allaire et al. 2021) et avec le package Bookdown (Xie 2021) Le code sappuie très largement sur tidyverse et emploie largement les ressources de ggplot . Les packages seront introduits au fur et à mesure. En voici la liste complète. options(tinytex.verbose = TRUE) knitr::opts_chunk$set(echo = TRUE, include=TRUE, cache=TRUE, message=FALSE, warning=FALSE) #boite à outils et dataviz library(tidyverse) # inclut ggplot pour la viz, readr et library(cowplot) #pour créer des graphiques composés library(ggridges) # le joy division touch library(ggmosaic) #networks library(igraph) library(ggraph) # Accéder aux données library(rtweet) # une interface efficace pour interroger l&#39;api de Twitter # NLP library(tokenizers) library(quanteda) library(quanteda.textstats) library(udpipe) #annotation syntaxique library(tidytext) library(cleanNLP) #annotation syntaxique #sentiment library(syuzhet) #analyse du sentimeent #mise en page des tableaux library(flextable) #statistiques et modèles library(lme4) #pour des modèles plus complexe que les mco library(jtools) #une série d&#39;utiltaire pour bien représenter les résultats library(interactions) #traitement des interactions library(corrplot) library(psych) library(&quot;FactoMineR&quot;) library(&quot;factoextra&quot;) #ML library(caret) #utilitaires library(rcompanion) #graphismes library(ggthemes) theme_set(theme_bw()) #palettes library(colorspace) #pour les couleurs library(wesanderson) # Utilitaires library(citr) #pour insérer des références dans le markdown Lensemble du code est disponible sur github. A ce stade cest encore très embryonnaire. Les proches pourrons cependant y voir lévolution du projet et de la progression Quelques conventions décriture du code r On appele les dataframes de manière générale df, les tableaux intermédiaires sont appelé systématiquement foo Gestion des palettes de couleurs ** une couleur :\" royalblue\" ** deux couleurs ** 3 à 7 couleurs On emploie autant que possible le dialecte tidy. Les chunks sont notés en 4 chiffre : 2 pour le chapitre et deux pour le chunck. 0502 est le second chunk du chapitre 5. On commente au maximum les lignes de code pour épargner le corps du texte et le rendre lisible 1.4 A faire todo list : insérer un compteur google analytics ( voir https://stackoverflow.com/questions/41376989/how-to-include-google-analytics-in-an-rmarkdown-generated-github-page) modifier le titre en haut à gauche vérifier le système de références voir ( https://doc.isara.fr/tuto-zothero-5-bibtex-rmarkdown-zotero/) Vérifier la publication en pdf References "],["intro.html", "Chapitre 2 Introduction aux data sciences 2.1 Objectif et sommaire 2.2 Science ou technique ? 2.3 histoires des logiciels statistiques 2.4 Le processus de traitement des données 2.5 Les facteurs de développement des datasciences", " Chapitre 2 Introduction aux data sciences 2.1 Objectif et sommaire Lobjet du manuel est de donner un aperçu général des méthodes danalyses de données et de data science. 2.2 Science ou technique ? Plûtôt que le terme consacré de Data sciences, il vaudrait mieux parler de data ingiénérie dans la mesure où le data scientiste participe à un processus de production qui va de lacquisition des donnée à leur propagation dans lorganisation ou la société. La technique domine sur la science et lunité se trouve dans lintégration de ce processus. La révolution des données vient de linteropérabilité croissante de ces techniques et dune intégration qui fluidifie le passage dune étape à une autre. Standards et langages en sont les éléments clés. Du côté des sciences, ce dont bénéficie lunivers des data sciences, cest lhéritage de cultures statistiques foisonnantes qui après sêtre développées dans leur cocon displinaire, se retrouvent désormais rassemblées dans un même langage. Bien sur il y a de manière sous-jascente à ces cultures les mathématiques et les statistiques mathématiques qui construisent les fondements des modèles et des techniques. Mais le développement sest fait souvent quand le scientifique se retrouve face à un problème où une observation. Prenons le cas des psychologues qui ont inventé lanalyse factorielle dans le but de pouvoir tester certains de leurs concepts : un degré dintelligence, une personnalité, des attitudes. Ou celui des écologues qui souhatent estimer une population de poisson dans une rivière, problème qui a donné naissance aux modèles de capture recapture. On pourrait ajouter les géographes avec les modèles danalyse spatiale, les financiers face à la variabilité des cours des places boursières, etc. Celui des économètres est peut-être le plus évident. Les biostatisticiens sont des contributeurs importants. Ce que la technique apporte cest lintégration par un langage et donc un ensemble de conventions, incarnées par r et python, dalgoritmes, et de programmes qui ne sont plus spécifique à un domaine, mais peuvent circuler de lun à lautre. Cest ainsi que le catalogues de toutes les techniques psychométriques devient accessible aux autres disciplines par le biais dun pachage en particulier , psych. De la même manière loutillage des linguiste devient accessible aux autres disciplines, pensons aux économiste qui intéègre dans le indicateurs des sources textuelle telle que lanalyse du sentiment. Linteropérabilité apportée par ces langages ne se définit pas que par lalgorithme qui aurait été porté dun autre langage vers celui-ci ( des cas de réécriture ?) mais aussi par des programme passerelle qui à partir de r permettent dactivité des algorithme écrit en C, en javascript ou tout autre langage plus informatiques et souvent plus éfficace. 2.3 histoires des logiciels statistiques Et cest ce quon observe dans lévolution des logiciels 1980 : statitcf 1980 : SAS comme accès à r 1990 : SPSS http://www.deenov.com/blog-deenov/histoire-du-logiciel-spad.aspx des système portable intégration graphique la modularisation : base /fonction/ packages 2.4 Le processus de traitement des données Acquisition Codification , filtrage et correction derreur Structuration des données : api, open data Exploration Modélisation : validation : tests versus AB testing Simulation et décision Vizualisation et sensemaking Déploiement : Contrôle : Publication : dash board, pdf , slide etc, webb site 2.5 Les facteurs de développement des datasciences Ces développements sont favorisés par un environnement fertile dont trois facteurs se renforcent mutuellement. 2.5.1 Une lingua franca histoire de r histoire de python 2.5.2 Une communauté Le second facteur , intimement lié au premier, est la constitution dune large communauté de développeurs et dutilisateurs qui se retrouvent aujourdhui dans des plateformes de dépots (Github, Gitlab), de plateformes de type quora (StalkOverFlow), de tutoriaux, de blogs (BloggeR), de journaux (Journal of Statistical Software) et de bookdown. Des ressources abondantes sont ainsi disponibles et facilitent la formation des chercheurs et des data scientists. Toutes les conditions sont réunies pour engendrer une effervescence créative. 2.5.3 La multiplication des sources de données. Le troisième est la multiplication des sources de données et leur facilité daccès. Les données privées, et en particulier celles des réseaux sociaux, même si un péage doit être payé pour accéder aux APIs, popularisent le traitement de données massives. Le mouvement des données ouvertes (open data) proposent et facilitent laccès à des milliers de corps de données : retards de la SNCF, grand débat, le formidable travail de lInsee, european survey etc. 2.5.4 du ML à lIA Le retour au boites noires dans les années 2000. Ce qui distingue les statistiques traditionnelles de lapproche machine learning réside dabord par une approche de la modèlisation différente. Les modèles statistiques et économétriques considèrent non seulement une structure ( moddèle linéaires par ex), la spécification du modèle, mais aussi des modèles de distribution qui définissent le cadre destimation. Lévaluation passe par le test du respects des hypothèses de constructions ( distribution des erreurs), et de la qualité dajustement. Le machine learning, se concentre sur la valeur prédictive, et considère nimporte quelle spécification même si elle est peu intelligible et comprend de grandes quantité de paramètres. KNN, SVM, rf et le retour des réseaux de neurones. La révolution des convolutions et la multiplication des architectures "],["prise-en-main.html", "Chapitre 3 Prise en main 3.1 La convention du Rmarkdown 3.2 Lire les données 3.3 Dplyr pour manipuler les données 3.4 Pour aller plus loin", " Chapitre 3 Prise en main Pour démarrer : 1 - Télécharger et installer r sur le site du Comprehensive r Archive Network 2 - Télécharger et installer Rstudio.(version free) 3 - Dans le cadre de cet atelier, on adopte la méthode du rmarkdown. On recommande fortement de lire louvrage de référence, même si la prise en main est très rapide. Il est désormais indispensable dutiliser le package tidyverse et en particulier les fonctions de manipulation et de pipe ( %&gt;%) fournies par dplyr 3.1 La convention du Rmarkdown Différentes manières dinteragir avec r sont possibles : la première est le mode console, pour de petite opérations et un utilisateur chevronné, celà peut être commode car rapide mais très rapidement on sera amené à enregistrer les opérations dans des scripts. Une idée novatrice a été dintégrer lensemble des élements dans un seul document : le script découpé en petits éléments : des chunks, le commentaire et lanalyse verbabe dans un format texte, et le résultat. Dans lunivers python il sagit des carnets Jupiter, pour r cest le rmarkdown. Cest un dialecte du markdown générique adapté au langage r. On recommande au lecteur den lire le manuel et de le garder dans ses onglets. Quelques éléments de base : un document markdown est composé de plusieurs éléments Yalm dans cet entête les éléments essentiels sont définis et paramétrés Texte : il suit les conventions de mise en forme du html : ** des # pour les niveau de titres ** (x)[*.html] pour des liens et pour des images ** Les chunks sont isolé par 3 tiks au début et à la fin. Résultats apparaissent sous les chunks https://rmarkdown.rstudio.com/authoring_pandoc_markdown.html#Footnotes Ce document peut être excécuté et publié sous différents formats : html, lpadf ou même word avec les éléments suivants * plan * texte * code * résultats * Bibliographie * Références * liens * images 3.2 Lire les données La première étape cest la lecture des données. On commence par le plus simple la lecture de fichiers locaux, dont les formats sont multiples : csv, tsv, xlsx, Spss, etc Le package readr contribue à cette tâche. df &lt;- read_csv(&quot;./Data/BXL_listings.csv&quot;) Il est possible aussi daccéder en direct aux données du web, cest bien utile pour sassurer que les données sont bien fraiches. Par exemple une connexion à Nsppolls qui propose une compilation de tous les sondages dintention de vote de la présidentielle 2022. df_pol &lt;- read_delim(&quot;https://raw.githubusercontent.com/nsppolls/nsppolls/master/presidentielle.csv&quot;, delim = &quot;,&quot;, escape_double = FALSE, trim_ws = TRUE) 3.2.1 La diversité des formats Peu de formats échappent à r, ils peuvent faire appel à des packages spécifiques excell Json shape et autre GIS : les formats bibliographique : bib et ris 3.2.2 Les autres accès aux données Interfaces le génie des API : ne pas se soucier de la mise à jour à chaque fois quon lance un calcul. exemples : base darchive de presse api des réseaux sociaux : tweetr open data 3.3 Dplyr pour manipuler les données Cest un des packages essentiels de la suite tidyverse. Il permet de manipuler aisément les données et mérite une étude approfondie. Un point de départ ou en français : dplyr . 3.3.1 Des pipes %&gt;% Une grand part de lintérêt de dplyr est de reprendre un opérateur de maggritr très utiles : le pipe : %&gt;%. Celui ci permet de passer le résultats de lopération à gauche, dans la fonction de droite. Un exemple simple . Dans la ligne de code suivante, une première fonction lit le fichier CSV, et envoie le résultat de cette lecture dans une fonction graphique élémentaire: compter le nombre doccurences des modalité de la variable room_type. On reviendra longuement sur ggplot, à ce stade ce qui compte cest de retenir le procédé. g &lt;- read_csv(&quot;./Data/BXL_listings.csv&quot;) %&gt;% ggplot(aes(x=price))+ geom_histogram() g 3.3.2 Des verbes Loriginalité de dplyr est de définir des fonctions comme des verbes. Chaque verbe désigne un type daction. On va les examiner progressivement : Ils sont simples à comprendre : tansformer une variables, filter les obsersation selon un critère, isoler des variables, les groupper pour en calculer des résultats statistiqyes ( somme, moyenne, variance, max min etc) * les déployer selon un format long ou les distribuer en différents critères, les fusionner enfin selon les grandes modalité du SQL) 3.3.2.1 Mutate En Français cest transformer. On modifie la valeur dune variable par une fonction plus ou moins complexe, éventuellement en ajoutant des conditions. Dans notre exemple, faisant au plus simple, puisque la distribution est asymétrique, une transformation du prix par les log peut donner des résultats intéressants. Et cest le cas. On retrouve une distribution qui semble être gaussienne. g &lt;- read_csv(&quot;./Data/BXL_listings.csv&quot;) %&gt;% mutate(price=log10(price))%&gt;% ggplot(aes(x=price))+geom_histogram() g 3.3.2.2 Filter On peut vouloir se concentrer sur une sous population. par exemple les chambres privées. g &lt;- read_csv(&quot;./Data/BXL_listings.csv&quot;) %&gt;% filter(room_type==&quot;Private room&quot;)%&gt;% # on note que le signe == est double, c&#39;est pour dire que la variable prend la valeur, ou non, qui est proposée mutate(price=log10(price))%&gt;% ggplot(aes(x=price))+geom_histogram() g 3.3.2.3 select On peut selectionner des colonnes pour créer un tableau spécifique foo &lt;- read_csv(&quot;./Data/BXL_listings.csv&quot;) %&gt;% dplyr::select(room_type,price) %&gt;% head(5) #afficher les 5 premières observations foo ## # A tibble: 5 x 2 ## room_type price ## &lt;chr&gt; &lt;dbl&gt; ## 1 Entire home/apt 91 ## 2 Entire home/apt 74 ## 3 Hotel room 120 ## 4 Entire home/apt 200 ## 5 Entire home/apt 74 3.3.2.4 Group_by et summarize cest une opération clé, en groupant selon les modalités dune ou pluseirs variables, on peut construire des tableaux aggrégés.On lassociera à summarize qui permet de calculer les statistique aggrégé selon le groupe que lon a définit. foo &lt;- read_csv(&quot;./Data/BXL_listings.csv&quot;)%&gt;% dplyr::select(neighbourhood, price)%&gt;% group_by(neighbourhood ) %&gt;% summarise(averageprice=mean(price)) head(foo, 5) ## # A tibble: 5 x 2 ## neighbourhood averageprice ## &lt;chr&gt; &lt;dbl&gt; ## 1 Anderlecht 71.9 ## 2 Auderghem 66.3 ## 3 Berchem-Sainte-Agathe 65.9 ## 4 Bruxelles 91.0 ## 5 Etterbeek 75.8 3.3.2.5 Pivot_wider et pivot_longer Si pour lhabitué des feuilles excell les données croisent des observations avec des variables, ce format nest pas le seul moyen de réprésenter des données, et pas forcément le meilleur. merge merge 3.3.2.6 merge On sera souvent amené à fabriquer des tableaux de donnée en les enrichissant dun autre. On sera amené à fusionner les données. Le cas le plus simples est dajouter dautres observation à un fichier de données, si les variables sont identitiques on peut concaténer diffrents jeux de données avec la fonction de base rbind au contraire si les observation sont les mêmes, et que seules les variables sont différentes on peut utiliser cbind. Léquivalent de DPLYR est row_bind et column_bind mais très souvent on sera dans des cas différents et la fusion des données devra suivre des index merge quatre types de fusion genérale fusion à gauche fusion à droite https://coletl.github.io/tidy_intro/lessons/dplyr_join/dplyr_join.html 3.4 Pour aller plus loin On engage le lecteur à poursuivre avec le bookdown au-delà du markdown une théorie des tidy data "],["analyses-univariées.html", "Chapitre 4 Analyses univariées 4.1 La grammaire des graphiques 4.2 Une étude de cas", " Chapitre 4 Analyses univariées Nous avons appris à lire des données, à les manipuler, nous avons le droit dêtre pressé de les représenter de manière immédiatement lisible, par des dataviz. On présente dabord rapidement le concept de grammaire des graphiques On se concentre ensuite sur un cas détude On décline. 4.1 La grammaire des graphiques Cest sans doute une des percées conceptuelles laplus intéressante des datasciences. La représentation graphhiques des données fait lobjet à la fois dune explosion créative mais aussi dune synthèse théorique. Cest lapport de la grammaire des graphiques. Ces outils sappuient sur lidée de grammaire des graphiques. En voici un clair résumé.En français il y a toujours le larmarange 4.1.1 Un modèle en couche Celle-ci met un ordre dans les éléments qui composent un graphique et les superpose. layers laesthetic definit les éléments que lon veut représenter : ce quon met en abscisse, ce quon met en ordonnné, les groupes que lon veut distinguer. la geométrie (geom_x)qui définit la forme de représentation les échelles (scale_x) Labelisation (labs) les templates le facetting ggplot est construit selon cette structure. Voici le book de référence, qui est au centre de ce cours. On aura besoin de manière assez systématique de manipuler les données avant de les représenter, dplyr nous permet de le faire aisément. 4.1.2 Une typologie des représentations Un point de départ fondamental est la gallery de ggplot,, elle présente de manière synthétique la plupart les types de figures qui peuvent être représentées, avec du code facilement reproductible. Une classification simple Analyse univariée Analyse bi variée Analyse multivariée ** les variables sont quantitatives : on analyse des matrices de corrélations ** les variables sont qualitatives : on analyse des tableaux croisés Analyse geospatiale Analyse de réseaux analyse darbres Diagramme de flux 4.1.3 Lesthétique Lart des couleurs tient dans les palettes on aimera celles de Wes Anderson, on peut adorer fishualize. on trouvera 4.2 Une étude de cas Les données sont extraites de lESS, une sélection est disponible ici. Elle couvre les 9 vagues et concernent la France et LAllemagne. Les variables dépendantes (celles que lon veut étudier et expliquer) sont les 9 items de la confiance, les variable considérées comme indépendantes (ou explicatives) sont une sélection de variables socio-démographiques : âge, genre, perception du pouvoir dachat, orientation politique, type dhabitat. On fait quelques opérations de recodage et on renomme les variables avoir une lecture plus aisée des variables et de leurs catégories. Le plan de recodage dun jeu de données quon va employer dans les chapitres suivants. Il sappuie sur le langage de base. Lanalyse univarié, comme son nom lindique, ne sintéresse quà une seule variable. Celle-ci peut être quantitative ou qualitative etne comporter quun nombre limité de modalités entre lesquels aucune comparaison de grandeur ne peut être faite. Les premières ont le plus souvent dans r un format numeric, les autres correspondent au format factor. (Un exercice peut être de le réécrire avec dplyr.) df&lt;-readRDS(&quot;./data/trustFrAll.rds&quot;) #quelques recodages #on renomme pour plus de clarte names(df)[names(df)==&quot;trstun&quot;] &lt;- &quot;NationsUnies&quot; names(df)[names(df)==&quot;trstep&quot;] &lt;- &quot;ParlementEurop&quot; names(df)[names(df)==&quot;trstlgl&quot;] &lt;- &quot;Justice&quot; names(df)[names(df)==&quot;trstplc&quot;] &lt;- &quot;Police&quot; names(df)[names(df)==&quot;trstplt&quot;] &lt;- &quot;Politiques&quot; names(df)[names(df)==&quot;trstprl&quot;] &lt;-&quot;Parlement&quot; names(df)[names(df)==&quot;trstprt&quot;] &lt;- &quot;Partis&quot; names(df)[names(df)==&quot;pplhlp&quot;] &lt;- &quot;help&quot; names(df)[names(df)==&quot;pplfair&quot;] &lt;- &quot;fair&quot; names(df)[names(df)==&quot;ppltrst&quot;] &lt;- &quot;trust&quot; #on construit les scores de confiance df&lt;-df %&gt;% mutate(trust_institut=(Partis+Parlement+Politiques+Police+Justice+NationsUnies+ParlementEurop)*10/7,trust_interpersonnel=(help+fair+trust)*10/3) df$Year&lt;-2000 #recodage des variables independantes df$Year[df$essround==1]&lt;-2002 df$Year[df$essround==2]&lt;-2004 df$Year[df$essround==3]&lt;-2006 df$Year[df$essround==4]&lt;-2008 df$Year[df$essround==5]&lt;-2010 df$Year[df$essround==6]&lt;-2012 df$Year[df$essround==7]&lt;-2014 df$Year[df$essround==8]&lt;-2016 df$Year[df$essround==9]&lt;-2018 df$Year&lt;-as.factor(df$Year) df$OP&lt;-&quot; &quot; #ggplot(df,aes(x=lrscale))+geom_histogram() df$OP[df$lrscale==0] &lt;- &quot;Extrême gauche&quot; df$OP[df$lrscale==1] &lt;- &quot;Gauche&quot; df$OP[df$lrscale==2] &lt;- &quot;Gauche&quot; df$OP[df$lrscale==3] &lt;- &quot;Centre Gauche&quot; df$OP[df$lrscale==4] &lt;- &quot;Centre Gauche&quot; df$OP[df$lrscale==5] &lt;- &quot;Ni G ni D&quot; df$OP[df$lrscale==6] &lt;- &quot;Centre Droit&quot; df$OP[df$lrscale==7] &lt;- &quot;Centre Droit&quot; df$OP[df$lrscale==8] &lt;- &quot;Droite&quot; df$OP[df$lrscale==9] &lt;- &quot;Droite&quot; df$OP[df$lrscale==10] &lt;- &quot;Extrême droite&quot; #la ligne suivante est pour ordonner les modalités de la variables df$OP&lt;-factor(df$OP,levels=c(&quot;Extrême droite&quot;,&quot;Droite&quot;,&quot;Centre Droit&quot;,&quot;Ni G ni D&quot;,&quot;Centre Gauche&quot;,&quot;Gauche&quot;,&quot;Extrême gauche&quot;)) df$revenu&lt;-&quot; &quot; df$revenu[df$hincfel&gt;4] &lt;- NA df$revenu[df$hincfel==1] &lt;- &quot;Vie confortable&quot; df$revenu[df$hincfel==2] &lt;- &quot;Se débrouille avec son revenu&quot; df$revenu[df$hincfel==3] &lt;- &quot;Revenu insuffisant&quot; df$revenu[df$hincfel==4] &lt;- &quot;Revenu très insuffisant&quot; df$revenu&lt;-factor(df$revenu,levels=c(&quot;Vie confortable&quot;,&quot;Se débrouille avec son revenu&quot;,&quot;Revenu insuffisant&quot;,&quot;Revenu très insuffisant&quot;)) df$habitat&lt;-&quot; &quot; df$habitat[df$domicil==1]&lt;- &quot;Big city&quot; df$habitat[df$domicil==2]&lt;-&quot;Suburbs&quot; df$habitat[df$domicil==3]&lt;-&quot;Town&quot; df$habitat[df$domicil==4]&lt;-&quot;Village&quot; df$habitat[df$domicil==5]&lt;-&quot;Countryside&quot; df$habitat&lt;-factor(df$habitat,levels=c(&quot;Big city&quot;,&quot;Suburbs&quot;,&quot;Town&quot;,&quot;Village&quot;,&quot;Countryside&quot;)) df$genre&lt;-&quot; &quot; df$genre[df$gndr==1]&lt;-&quot;H&quot; df$genre[df$gndr==2]&lt;-&quot;F&quot; df$age&lt;-&quot; &quot; df$age[df$agea&lt;26]&lt;-&quot;25&lt;&quot; df$age[df$agea&gt;25 &amp; df$agea&lt;36]&lt;-&quot;26-35&quot; df$age[df$agea&gt;35 &amp; df$agea&lt;46]&lt;-&quot;36-45&quot; df$age[df$agea&gt;45 &amp; df$agea&lt;66]&lt;-&quot;46-65&quot; df$age[df$agea&gt;65 &amp; df$agea&lt;76]&lt;-&quot;66-75&quot; df$age[df$agea&gt;75]&lt;-&quot;75&gt;&quot; df$age&lt;-factor(df$age,levels=c(&quot;25&lt;&quot;,&quot;26-35&quot;,&quot;36-45&quot;,&quot;46-65&quot;,&quot;66-75&quot;, &quot;75&gt;&quot;)) saveRDS(df, &quot;./data/dfTrust.rds)&quot;) foo&lt;-df%&gt;% dplyr::select(Year,cntry, trust_institut, trust_interpersonnel)%&gt;% group_by(Year,cntry)%&gt;% summarise(trust_institut=mean(trust_institut, na.rm=TRUE), trust_interpersonnel=mean(trust_interpersonnel, na.rm=TRUE)) foo$Year&lt;- as.character(foo$Year) foo$cntry&lt;- as.character(foo$cntry) foo&lt;-foo%&gt;%pivot_longer(!c(Year,cntry),names_to=&quot;Trust&quot;, values_to=&quot;value&quot; ) ggplot(foo,aes(x=Year, y=value, group=Trust))+ geom_line(stat=&quot;identity&quot;,aes(color=Trust), size=1.2)+ facet_wrap(vars(cntry))+ scale_color_manual(values = c(&quot;Cyan3&quot;,&quot;Orange2&quot;))+ theme( legend.position = &quot;bottom&quot;, legend.justification = c(&quot;right&quot;, &quot;top&quot;), legend.box.just = &quot;right&quot;, legend.margin = margin(6, 6, 6, 6) )+ labs(x=NULL, y=&quot;Niveau&quot;)+ylim(40,60) 4.2.1 Le cas des variables quantitatives Les variables quantitatives décrivent une variable dont les valeurs décrivent les quantités dune grandeur. Elle peuvent être discrètes (dénombrement du dun nombre dunités) - le nombre dhabitant), ou continue (le nombre de km parcourus). lhistogramme est loutil de base pour représenter la distribution dune telle variable. Il représente pour des intervalles de valeurs donnés, la fréquence des observations. Sa syntaxe simple comporte dabord la définition de la variable et de la source de données, puis une des géométrie de ggplot : la fonction geom_histogram. Dans notre exemple, on va représenter le score de confiance institutionnelle pour la France en se concentrant sur la dernière vague denquête. #On charge le fichier recodé à la fin du chapitre précédent df&lt;-readRDS(&quot;./data/dfTrust.rds)&quot;) #filtrage sur 2018 et la France. foo&lt;-df%&gt;% filter(Year==&quot;2018&quot; &amp; cntry==&quot;FR&quot; &amp; !is.na(trust_institut)) # on stocke le diagramme dans l&#39;objet g00, pour le réutiliser ultérieurement et pouvoir le compléter. g00&lt;-ggplot(foo,aes(x=trust_institut))+ geom_histogram() g00 g00+labs(title=&quot;Distribution de la confiance institutionnelle en France et en 2018&quot;, x=&quot;Confiance institutionnelle&quot;) On va améliorer laspect en modifiant la couleur et la largeur des barres, ajoutant un thème, en précisant les éléments textuels (titres, label) en calculant et en représentant la valeur moyenne et lécart-type . Pour ces statistiques, on emploie les fonction de base : mean, sd et round. On notera que le titre est défini par la concaténation de plusieurs chaines de caractères avec la fonction paste0. On peut ainsi injecter dans le graphique des éléments externes au jeu de données. #on calcule la moyenne moy=mean(foo$trust_institut, na.rm=TRUE) sd=sd(foo$trust_institut, na.rm=TRUE) #avec tous les éléments g01 &lt;-ggplot(foo,aes(x=trust_institut))+ geom_histogram(binwidth=5,fill=&quot;pink&quot;)+ labs(title= &quot;Distribution de la confiance institutionnelle&quot;, subtitle= paste0(&quot;moyenne = &quot;,round(moy,2), &quot; ecart-type = &quot;, round(sd,2)), caption=&quot;ESS2002-2018&quot;, y= &quot;frequence&quot;, x=&quot;confiance (index de 0 à 100)&quot;)+ geom_vline(xintercept=moy, color=&quot;red&quot;,size=1.5)+ geom_segment(y = 0, yend=0,x=moy-sd,xend=moy+sd, color=&quot;orange&quot;,size=1.5) g01 Diagramme de densité : Au lieu de représenter les effectifs, on ramène leffectif total à 1. g04&lt;-ggplot(foo,aes(x=trust_institut))+ geom_density(fill=&quot;pink2&quot;) + labs(title= &quot;Fonction de densité de probabilité&quot;, caption=&quot;ESS2002-2018&quot;,y= &quot;frequence&quot;,x=&quot;Confiance (index de 1 à 100)&quot;) g04 enfin on peut examiner par rapport à une distribution théorique, en loccurrence une distribution gaussienne, ou normale, de paramètres égaux à la moyenne et la variance empirique de la distribution. Lajustement est convenable même si on observe une déviation sur la droite. Cest pourquoi on calcule aussi la Kurtosis et le skewness de la distribution. #On a déjà calculé la moyenne : mean #il nous manque l&#39;écart-type et sd&lt;-sd(foo$trust_institut, na.rm=TRUE) library(moments) sk&lt;-skewness(foo$trust_institut) ks&lt;-kurtosis(foo$trust_institut) g05&lt;-ggplot(foo,aes(x=trust_institut))+ labs(title= &quot;Distribution de la confiance institutionnelle&quot;, caption=&quot;ESS2002-2018&quot;,y= &quot;frequence&quot;,x=&quot;confiance (index de 0 à 100)&quot;) + geom_density(fill=&quot;pink2&quot;)+ stat_function(fun = dnorm,color=&quot;red&quot;,size=1.2, args = list(mean =moy, sd=sd)) g05 Un grand classique du test de normalité dune distribution est le diagramme QQ g06 &lt;- ggplot(foo, aes(sample = trust_institut)) + stat_qq() + stat_qq_line()+ labs(title= &quot;QQplot confiance interpersonnelle&quot;, caption=&quot;ESS2002-2018&quot;,y= &quot;Echantillon&quot;,x=&quot;Théorique&quot;) g06 On fini cette étude détaillée par lajustement dabord dun modèle (loi normale) aux données. Ensuite dun modèle de mélange ( Mixture model) par lequel on défiit la loi de distribution sous jascente, comme un mélange entre deux populations normale de paramètres distincts. https://tinyheero.github.io/2015/10/13/mixture-model.html df0&lt;-df %&gt;% na.omit() library(MASS) fit&lt;-fitdistr(df0$trust_interpersonnel,&quot;normal&quot;) fit ## mean sd ## 52.48548790 16.57617220 ## ( 0.09344363) ( 0.06607462) g07&lt;- g05+stat_function(fun = dnorm ,color=&quot;orange&quot;,size=1.2, args = list( mean=52.48, sd=16.57)) g07 library(mixtools) trust = foo$trust_institut mixmdl = normalmixEM(trust, k=2) ## number of iterations= 505 mixmdl$mu ## [1] 39.23071 56.04167 mixmdl$sigma ## [1] 17.90246 10.56093 mixmdl$lambda ## [1] 0.6719499 0.3280501 plot(mixmdl,which=2) lines(density(trust), lty=2, lwd=2) Finalement si notre distribution est univariée, car nétudiant quune variable, on peut quand distinguer deux population distinctes. 4.2.1.1 Dautres méthodes Il ny a pas que lhistogram ou le diagramme de densité, dautres méthodes sont utiles, surtout quand on veut comparer des groupes ( ce sera lobjet du prochain chapitre). Il sagit du diagramme à moustache et du diagramme en violon. g0306 &lt;- ggplot(foo, aes(y = trust_institut, x=1)) + geom_boxplot(fill=&quot;Grey&quot;) g0307 &lt;- ggplot(foo, aes(x=1,y = trust_institut)) + geom_violin(fill=&quot;Gold&quot;) + labs(x=&quot;density&quot;) plot_grid(g0306, g0307, labels = c(&quot;Boxplot&quot;,&quot;Violin plot&quot;), label_size = 12 ) 4.2.2 Quand la variable est qualitative Quand la variable est qualitative, que ses variables sont discrètes, la manière de représenter la plus commune est le fameux camembert que les experts écartent. Un diagramme en barre représente mieux les proportions. Un premier exemple pour représenter les vagues denquêtes g08&lt;-ggplot(df,aes(x=age))+ geom_bar(fill=&quot;skyblue&quot;)+ labs(title= &quot;Distribution par classe d&#39;âge&quot;, caption=&quot;ESS2002-2018&quot;,y= &quot;frequence&quot;,x=&quot;Vague d&#39;enquête&quot;) g08 Avec quelques améliorations : contôle de la couleurs des barres, ajout des % et pivot pour une meilleure lecture. foo&lt;-df %&gt;% filter(!is.na(age)) g10&lt;-ggplot(foo,aes(x=age, y = prop.table(stat(count)),label = scales::percent(prop.table(stat(count)))))+ geom_bar(aes(fill = age)) + coord_flip()+ labs(title= &quot;Répartition de la population par classe d&#39;âge&quot;, caption=&quot;ESS2002-2018&quot;,y= &quot;%&quot;,x=&quot;classes d&#39;age&quot;) + scale_y_continuous(labels = scales::percent)+ #contrôle de l&#39;échelle des % et du format scale_fill_brewer()+ geom_text(stat = &#39;count&#39;,position = position_dodge(.9), hjust = 1, size = 3) g10 si on tient au diagramme en secteur foo&lt;-df %&gt;%filter(!is.na(age)) g10&lt;-ggplot(foo,aes(x=&quot;&quot;, y = prop.table(stat(count)),label = scales::percent(prop.table(stat(count)))))+ geom_bar(aes(fill = age)) + labs(title= &quot;Répartition de la population par classe d&#39;âge&quot;, caption=&quot;ESS2002-2018&quot;,y= &quot;%&quot;,x=&quot;classes d&#39;age&quot;) + geom_text(stat = &#39;count&#39;,position = position_dodge(.9), hjust = 1, size = 3) + coord_polar(&quot;y&quot;, start=0) g10 https://cran.r-project.org/web/packages/treemapify/vignettes/introduction-to-treemapify.html si on tient au diagramme en cercle, autant opter pour un treemap avec la bibliothèque treemapifi library(treemapify) tree1&lt;-df %&gt;% mutate(n=1)%&gt;%group_by(age) %&gt;% summarize(n=sum(n)) %&gt;% filter(!is.na(age)) g11 &lt;- ggplot(tree1, aes(area = n, fill=n),label=age) + geom_treemap() + geom_treemap_text(aes(label=age),colour = &quot;white&quot;, place = &quot;centre&quot;,grow = FALSE)+ labs(title= &quot;Répartition de la population par classe d&#39;âge&quot;, caption=&quot;ESS2002-2018&quot;,y= NULL,x=NULL) g11 "],["analyse-bi-variée.html", "Chapitre 5 Analyse bi variée 5.1 Diagrammes xy - la magie des corrélations 5.2 Comparer les distributions et des moyennes", " Chapitre 5 Analyse bi variée Comme son nom lindique, il sagit dexaminer la relation entre deux variables et détudier leur distribution conjointe. On distinguera 3 situations et on examinera pour chaune les modes de représentations graphiques ainsi que les tests associés qui permette de sassurer que la relation apparente est effective. Deux variables quantitatives : scatterplot et corélations deux variable qualitatives : tableau croisé et test du chi2 une variable quanti et une variable quali. Compariaons de moyennes et ANOVA par comparer des distribution de plusieurs groupes (variables catégorielles) par comparer des moyennes dune variable dépendante en fonction de plusieurs variables indépendantes catégorielle mesurer lassociation entre deux variables qualitatives 5.1 Diagrammes xy - la magie des corrélations Venons en à analyser les relations entre deux variables quantitatives. foo&lt;-df %&gt;% filter(cntry==&quot;FR&quot; &amp; Year==&quot;2018&quot;) #selection de l&#39;echantillon g31&lt;- ggplot(foo, aes(x= trust_interpersonnel,y=trust_institut)) + geom_point( size=0.1) g31 Ce graphe est peu clair, il y a trop de points qui prennent des valeurs discrètes. Une astuce est de donner une position aléatoire pour sur disperser, on fait mieux apparaitre la densité de points. On ajoute la représentation de deux courbe dajustement, lune linéraire et lautre non linéaires. Mais en attendant en voici un calcul élémentaire. le calcul de la variance \\[{SS}_{xx} = \\sum (x - \\bar{x})^2 = \\sum x^2 - \\frac {(\\sum x)^2}{n}\\] le calcul de la covariance \\[{SS}_{xy} = \\sum (x - \\bar{x})(y - \\bar{y}) = \\sum xy - \\frac {(\\sum x)(\\sum y)}{n}\\] et la corrélation qui est le rapport de la covariance sur la racine carrée du produit des variances de x et y. \\[r = \\frac {{SS}_{xy}}{\\sqrt {{SS}_{xx}{SS}_{yy}}}\\] La corrélation est de lordre dun peu plus de 0,40 ce qui est assez élevé mais laisse une certaine indépendance des variables. Elle désignent des objets liés mais distinct. On peut tester lhypothèse quen réalité cette corrélation est nulle. Le test conduit au rejet de lhypothèse nulle de manière très nette, compte-tenu de léchantillon lintervalle de confiance est compris entre 0.36 et 0.44. #psych r&lt;-cor.test(foo$trust_interpersonnel, foo$trust_institut) #le test vient du package psych r ## ## Pearson&#39;s product-moment correlation ## ## data: foo$trust_interpersonnel and foo$trust_institut ## t = 18.861, df = 1821, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.3651235 0.4419644 ## sample estimates: ## cor ## 0.404257 rp&lt;-round(r$estimate,3) rp ## cor ## 0.404 Améliorons le graphe On peut souhaiter ajouter une droite des moindre carrés (calculée pour chaque vague denquête pour évaluer la stabilité de la relation dans le temps). Les lignes sont parallèles, la corrélation ne change pas dans le temps, cest une relation stable. Les deux formes de confiance vont dans le meme sens. On verra dans un autre chapitre comment calculer ces droites de corrélations. library(ggExtra) g32&lt;-ggplot(foo, aes(x= trust_interpersonnel,y=trust_institut)) + geom_point(position = &quot;jitter&quot;, size=0.1, color=&quot;grey&quot;)+ geom_smooth(method=&quot;lm&quot;, se=TRUE) + geom_smooth(method=&quot;gam&quot;,color=&quot;red&quot;) + labs(title = &quot;Relation entre confiance \\ninstitutionnelle et interpersonnelle&quot;, subtitle = paste(&quot;r de pearson: &quot;,rp ), x= &quot;Confiance interpersonnelle&quot;, y=&quot; Confiance institutionnelle&quot;) ggMarginal(g32 ,type = &quot;density&quot;, fill = &quot;Royalblue1&quot;, alpha=.5) Une autre façon de représenter est celle de carte de densité de probabilité. g32&lt;-ggplot(foo, aes(x= trust_interpersonnel,y=trust_institut)) + geom_point(position = &quot;jitter&quot;, size=0.1, color=&quot;grey&quot;)+geom_density2d()+ labs(title = &quot;Relation entre confiance institutionnelle et interpersonnelles&quot;, subtitle = paste(&quot;r de pearson: &quot;,rp )) g33&lt;-ggplot(foo, aes(x= trust_interpersonnel,y=trust_institut)) + geom_density2d_filled(aes(fill = ..level.., color = ..level..), contour_var = &quot;density&quot;)+ labs(title = &quot;Relation entre confiance institutionnelle et interpersonnelles&quot;, subtitle = paste(&quot;r de pearson: &quot;,rp ))+theme(legend.position = &quot;none&quot;) plot_grid(g32, g33, labels = c(&#39;A&#39;, &#39;B&#39;), label_size = 12) 5.2 Comparer les distributions et des moyennes Dans notre base on a pris les données de lAllemagne et de la France. On va comparer leur distribution. Et tant quà faire, puisque quon a deux variables, on va faire deux comparaisons : par pays et par type de confiance. A cette fin, nous construisons un tableau de données spécifique. #on recode en facteur la variable foo &lt;- df %&gt;% dplyr::select(cntry,trust_institut, Year,trust_interpersonnel) %&gt;% filter( Year==&quot;2018&quot;) %&gt;% dplyr::select(-Year)%&gt;% drop_na() %&gt;% gather(variable, value, -cntry) #attention plutôt utiliser pivot_longer head(foo) ## # A tibble: 6 x 3 ## cntry variable value ## &lt;chr+lbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 DE [Germany] trust_institut 58.6 ## 2 DE [Germany] trust_institut 65.7 ## 3 DE [Germany] trust_institut 58.6 ## 4 DE [Germany] trust_institut 65.7 ## 5 DE [Germany] trust_institut 48.6 ## 6 DE [Germany] trust_institut 37.1 Pour la représentation, en plus de la représentation en terme de densité, on va choisir une méthode de violon et de boxplot. On utilise une couche de facetting pour éclater ainsi la distribution des deux variables selon un critère de pays. #on peut utiliser &quot;facet&quot; g20&lt;-ggplot(foo,aes(x=value))+ geom_density(binwidth=10, fill=&quot;pink&quot;)+ facet_grid(cntry~variable)+ labs(title= &quot;Confiance institutionnnelle&quot;, caption=&quot;ESS2002-2018&quot;,y= &quot;frequence&quot;,x=&quot;Confiance&quot;) g20 g21&lt;-ggplot(foo,aes(x=variable, y=value))+ geom_violin( fill=&quot;pink&quot;) + geom_boxplot(width=0.1)+ facet_grid(cntry~.)+ labs(title= &quot;Confiance institutionnnelle&quot;, caption=&quot;ESS2002-2018&quot;,y= &quot;frequence&quot;,x=&quot;Confiance&quot;) g21 5.2.1 Comparaison de moyennes Comparer des distributions est une étape initiale nécesséaire, mais en général on sera plutôt intéresser de comparer des moyennes. Par exemple, on souhaiterais savoir si les degrés de confiances institutionnnelle et interpersonnelles varient en France selon les situations de revenu. Calculons dabord ces moyennes avec la fonction group_by et summarise. df_wave&lt;-df %&gt;% filter(cntry==&quot;FR&quot; &amp; Year==&quot;2018&quot;) %&gt;% group_by(revenu) %&gt;% summarise(trust_interpersonnel=mean(trust_interpersonnel, na.rm=TRUE), trust_institut =mean(trust_institut, na.rm=TRUE)) %&gt;% filter(!is.na(revenu)) %&gt;% #filtrer les valeurs manquantes gather(variable, value, -revenu) #fichier long ( pivot longer is better) head(df_wave) ## # A tibble: 6 x 3 ## revenu variable value ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Vie confortable trust_interpersonnel 55.6 ## 2 Se débrouille avec son revenu trust_interpersonnel 51.7 ## 3 Revenu insuffisant trust_interpersonnel 46.7 ## 4 Revenu très insuffisant trust_interpersonnel 41.4 ## 5 Vie confortable trust_institut 50.2 ## 6 Se débrouille avec son revenu trust_institut 44.1 Représentons ces moyennes graphiquement avec un geom_bar. g06a&lt;-ggplot(df_wave,aes(x=revenu,y=value, group=variable))+ geom_bar(stat=&quot;identity&quot;,aes(fill=variable), position =position_dodge())+ #dodge pour mettre les barres l&#39;une à côté de l&#39;autre labs(title= &quot;Confiance institutionnnelle&quot;, caption=&quot;ESS2002-2018&quot;,y= &quot;frequence&quot;,x=&quot;Niveau de Confiance&quot;)+ coord_flip() g06a On a une solution mais pas la meilleure, on perd lidée de variance et ce serait bien dajouter des barres dintervalle de confiances , un diagramme en lignes serait plus élégant. On en profite pour corriger laspect des labels peu lisibles en les inclinants, et à choisir une échelle qui omettent les valeur supérieur à 70 et inférieure à 30 pour donner une vision plus respectueuses de la totalité de léchelle qui va de 0 à 100. Au passage on emploie à nouveau cowplot pour combiner les graphes, et ici plus précisément partager la légende des deux graphiques. On observera que si le niveau de confiance diminue avec le revenu, la confiance interpersonnelle est plus forte, et de manière parallèle, à la confiance institutionnelle. On remarquera enfin que cest pour les revenu les plus faibles que lestimation est la plus imprécise ou la variance la plus grande. df_wave2&lt;-df %&gt;% filter(cntry==&quot;FR&quot; &amp; Year==&quot;2018&quot;)%&gt;% group_by(revenu) %&gt;% mutate(n=1) %&gt;% summarise(trust_interpersonnel_se=sd(trust_interpersonnel, na.rm=TRUE), #on calcule l&#39;écartype des deux variables trust_institut_se =sd(trust_institut, na.rm=TRUE), n=sum(n), trust_interpersonnel_se= 2*trust_interpersonnel_se/sqrt(n), # on calcule l&#39;erreur type d&#39;échantillonnage trust_institut_se=2*trust_institut_se/sqrt(n) ) %&gt;% dplyr::select(-n) %&gt;% filter(!is.na(revenu)) %&gt;% gather(variable, value, -revenu) %&gt;% #on passe en format long dplyr::select(-revenu,-variable)%&gt;% rename(se=value) df_wave3&lt;-cbind(df_wave,df_wave2) #on concatène les moyennes et les erreurs types #on peut enfin produire le graphique g06a&lt;-ggplot(df_wave3,aes(x=revenu,y=value, group=variable))+ geom_line(stat=&quot;identity&quot;,aes(color=variable), size=1.5)+ geom_errorbar(aes(ymin=value-se, ymax=value+se, color=variable), width=.2,position=position_dodge(0))+ labs(title= &quot;Confiance et revenu&quot;,y= &quot;Moyenne&quot;,x=NULL)+ theme(axis.text.x = element_text( angle=45, hjust =1)) #on controle l&#39;angle et la position horizontale du label g06b&lt;-ggplot(df_wave3,aes(x=revenu,y=value, group=variable))+ geom_line(stat=&quot;identity&quot;,aes(color=variable), size=1.5)+ geom_errorbar(aes(ymin=value-se, ymax=value+se, color=variable), width=.2,position=position_dodge(0))+ ylim(0,100)+ labs(title= &quot;&quot;,y= &quot;Moyenne&quot;,x=NULL)+ theme(axis.text.x = element_text( angle=45, hjust =1)) #on controle l&#39;angle et la position horizontale du label prow &lt;- plot_grid( g06a + theme(legend.position=&quot;none&quot;), g06b + theme(legend.position=&quot;none&quot;), align = &#39;vh&#39;, labels = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), hjust = -1, nrow = 1 ) # extract a legend that is laid out horizontally legend_b &lt;- get_legend( g06a + guides(color = guide_legend(nrow = 1)) + theme(legend.position = &quot;bottom&quot;) ) # add the legend underneath the row we made earlier. Give it 10% # of the height of one plot (via rel_heights). plot_grid(prow, legend_b, ncol = 1, rel_heights = c(1, .1)) La visualisation est utile, encore faut-il quon soit bien certain que les variations ne soit pas le produit du hasard, des fluctuations déchantillonnage. Si en moyenne la perception du pouvoir dachat est associée à des moyennes de confiance décroissantes, les différences observées sont-elle significatives? Dans les représentations précédentes cest le choix de léchelle qui oriente lanalyse. On a un besoin dun test plus objectif. Celui est le très classique test danalyse de variance (ANOVA). Celui-çi est le test danalyse de variance qui consiste à comparer la variance à lintérieur des groupes ( intra), et la variance entre les moyennes des groupes (inter ou between). On note quici on introduit la méthode flextable pour présenter des tableaux au formats scientifique. Lastuce ici est dutiliser aussi xtable. foo&lt;-df %&gt;% filter(cntry==&quot;FR&quot; &amp; Year==&quot;2018&quot;) %&gt;% drop_na() #selection des données fit&lt;-lm(trust_institut~revenu, foo) #calcul du modèle linéaire anova(fit) #test d&#39;analyse de variance ## Analysis of Variance Table ## ## Response: trust_institut ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## revenu 3 27651 9217.1 32.052 &lt; 2.2e-16 *** ## Residuals 1686 484846 287.6 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 library(xtable) #xtable transforme en table certains type d&#39;objet dont les résultats de l&#39;anova ft &lt;- xtable_to_flextable(xtable(anova(fit)), hline.after = c(0,2)) #la fonction permet d&#39;exploiter flextable. ft .tabwid table{ border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-spacing: 0; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-5d2d0356{border-collapse:collapse;}.cl-5d2052aa{font-family:'Arial';font-size:11pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-5d2052ab{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-5d20799c{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-5d20799d{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-5d20c78a{width:48.1pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5d20c78b{width:52.6pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5d20c78c{width:58.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5d20c78d{width:72.5pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5d20c78e{width:65.8pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5d20c78f{width:69.5pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5d20c790{width:48.1pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5d20c791{width:58.4pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5d20c792{width:72.5pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5d20c793{width:52.6pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5d20c794{width:65.8pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5d20ee7c{width:69.5pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5d20ee7d{width:48.1pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5d20ee7e{width:52.6pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5d20ee7f{width:65.8pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5d20ee80{width:58.4pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5d20ee81{width:72.5pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5d20ee82{width:69.5pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} DfSum SqMean SqF valuePr(&gt;F)revenu327,651.49,217.132.10.0Residuals1,686484,845.7287.6 5.2.2 Deux variables qualitatives Létude de la relation éventuelle entre deux variables qualitative sapprécie traditionnellement par une méthode de tableau croisé. 5.2.2.1 Tableau croisé Pour calculer le tableau croisé on utilise la fonction très simple table et la fonction prop.table t&lt;-table(foo$revenu,foo$habitat) t ## ## Big city Suburbs Town Village Countryside ## Vie confortable 118 82 161 142 31 ## Se débrouille avec son revenu 120 109 275 227 58 ## Revenu insuffisant 48 38 129 88 22 ## Revenu très insuffisant 9 5 18 10 0 prop.table(t,2) ## ## Big city Suburbs Town Village ## Vie confortable 0.40000000 0.35042735 0.27615780 0.30406852 ## Se débrouille avec son revenu 0.40677966 0.46581197 0.47169811 0.48608137 ## Revenu insuffisant 0.16271186 0.16239316 0.22126930 0.18843683 ## Revenu très insuffisant 0.03050847 0.02136752 0.03087479 0.02141328 ## ## Countryside ## Vie confortable 0.27927928 ## Se débrouille avec son revenu 0.52252252 ## Revenu insuffisant 0.19819820 ## Revenu très insuffisant 0.00000000 Mais ce nest pas esthétique, avec la fonction proc_freq de flextable on obtient une meilleure présentation. Elle nous donne en peu de mots les effectif par cellule, les pourcentages en lignes, et en colonnes. ft1&lt;- proc_freq(foo, &quot;revenu&quot;, &quot;habitat&quot;, include.table_percent = FALSE, include.row_percent = FALSE, include.column_total = FALSE, include.column_percent = TRUE) ft1 .tabwid table{ border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-spacing: 0; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-5d88cff6{border-collapse:collapse;}.cl-5d7a7848{font-family:'Arial';font-size:11pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-5d7a7849{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-5d7a784a{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-5d7a784b{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-5d7b141a{width:90.9pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5d7b141b{width:45pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5d7b141c{width:72.5pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5d7b141d{width:79.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5d7b141e{width:168.5pt;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5d7b141f{width:90.9pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5d7b1420{width:45pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5d7b1421{width:168.5pt;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5d7b1422{width:72.5pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5d7b1423{width:79.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5d7b1424{width:168.5pt;background-color:transparent;vertical-align: top;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5d7b3b0c{width:72.5pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5d7b3b0d{width:90.9pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5d7b3b0e{width:79.2pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5d7b3b0f{width:168.5pt;background-color:transparent;vertical-align: top;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5d7b3b10{width:45pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5d7b3b11{width:90.9pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5d7b3b12{width:45pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5d7b3b13{width:79.2pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5d7b3b14{width:168.5pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5d7b3b15{width:72.5pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5d7b3b16{width:79.2pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5d7b6212{width:90.9pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5d7b6213{width:45pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5d7b6214{width:72.5pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5d7b6215{width:168.5pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} habitatrevenulabelBig citySuburbsTownVillageCountrysideTotalVie confortableFrequency1188216114231534Col Pct40%35.04%27.62%30.41%27.93%Se débrouille avec son revenuFrequency12010927522758789Col Pct40.68%46.58%47.17%48.61%52.25%Revenu insuffisantFrequency48381298822325Col Pct16.27%16.24%22.13%18.84%19.82%Revenu très insuffisantFrequency951810042Col Pct3.05%2.14%3.09%2.14%0% ft2&lt;- proc_freq(foo, &quot;revenu&quot;, &quot;habitat&quot;, include.table_percent = FALSE, include.row_percent = TRUE, include.column_percent = FALSE) ft2 .tabwid table{ border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-spacing: 0; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-5e0d0eb0{border-collapse:collapse;}.cl-5dbd4a06{font-family:'Arial';font-size:11pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-5dbd4a07{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-5dbd70f8{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-5dbd70f9{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-5dbdbedc{width:72.5pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5dbdbedd{width:48.1pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5dbdbede{width:168.5pt;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5dbdbedf{width:90.9pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5dbdbee0{width:84.8pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5dbdbee1{width:48.1pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5dbdbee2{width:90.9pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5dbdbee3{width:84.8pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5dbdbee4{width:72.5pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5dbdbee5{width:168.5pt;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5dbdbee6{width:84.8pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5dbde5d8{width:72.5pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5dbde5d9{width:168.5pt;background-color:transparent;vertical-align: top;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5dbde5da{width:90.9pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5dbde5db{width:48.1pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5dbde5dc{width:168.5pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5dbde5dd{width:84.8pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5dbde5de{width:72.5pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5dbde5df{width:48.1pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5dbde5e0{width:90.9pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5dbde5e1{width:84.8pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5dbde5e2{width:90.9pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5dbe0cd4{width:72.5pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5dbe0cd5{width:168.5pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5dbe0cd6{width:48.1pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} habitatrevenulabelBig citySuburbsTownVillageCountrysideTotalVie confortableFrequency1188216114231534Row Pct22.1%15.36%30.15%26.59%5.81%Se débrouille avec son revenuFrequency12010927522758789Row Pct15.21%13.81%34.85%28.77%7.35%Revenu insuffisantFrequency48381298822325Row Pct14.77%11.69%39.69%27.08%6.77%Revenu très insuffisantFrequency951810042Row Pct21.43%11.9%42.86%23.81%0%TotalFrequency2952345834671111690 5.2.2.2 le valeureux chi² Le test du chi2 sappuie sur une idée très simple qui de fait est un théorème : Si deux variables X et Y sont indépendantes, la fréquence de leur combinaison est le produit des fréquences marginales. On peut donc sur cette base, calculer leffectif attendu (expected frequency) puis le comparer à ce quon a observé pour chacune des cellules du tableau. On somme enfin ces écarts. \\[\\chi^2 = \\sum \\frac {(O_{ij} - E_{ij})^2}{E_{ij}}\\] Naturellement , une même valeur de cette quantité pour un petit tableau( 2x2) na pas la même signification que si le tableau est grand( par ex 20x 10). On lappréciera donc en fonction des degrés de liberté (n-1 x m-1). Le test proprement dit consiste à examiner quelles sont les chances quon obtienne la valeur du chi2 calculé, pour un nombre de degré de liberté donné. Si cette probabilité est faible on rejetera lhypothèse dindépendance des deux variables. Avec r la fonction chsq.test nous simplifie chi2&lt;-chisq.test(t) chi2 ## ## Pearson&#39;s Chi-squared test ## ## data: t ## X-squared = 23.853, df = 12, p-value = 0.0213 Lobjet chi2 est une liste # On isole les éléments qui nous intéresse #library() chi&lt;-round(chi2$statistic,2) p&lt;-round(chi2$p.value,3) V&lt;-cramerV(t, digit=3) 5.2.2.3 diagramme en mosaique Le diagramme en mosaique détermine la largeur des barres en fonction de leffectif de la variable en abcisse et leur hauteur en fonction de la variable en ordonnée. Les couleurs permettent de mieux comparer. On saperçoit ici que les plus à laise avec leur revenu sont proportionnellement plus nombreux dans les grandes villes, et que ceux qui se débrouille sont plus fréquents dans les campagnes. library(ggmosaic) g1 &lt;- ggplot(data = foo) + geom_mosaic(aes(x=product( revenu ,habitat), fill = revenu))+ theme(axis.text.x = element_text(angle = 45, hjust = -0.1, vjust = -0.2))+ theme(legend.position = &quot;none&quot;)+ labs(title=&quot;Statut vaccinal \\npar genre&quot;, subtitle=paste0(&quot;chi2 =&quot;,chi, &quot; p = &quot;, p, &quot; - V : &quot;, V))+ scale_fill_brewer(palette = &quot;RdYlGn&quot;, direction = -1) g1 5.2.2.4 les chi2s partiel et des cartes de chaleur. Une carte de chaleur représente une grandeur par un gradient de couleur pour chaque cellule définie par des variable x et y. Faisons un premier essai pour représenter les effectifs, plutôt quavoir un tableau de nombres on va obtenir un tableau de couleurs. Larbre qui apparait en ligne et en colonne correspond au résultat dune classification hiérarchique que nous développons dans le chapitre X. library(pheatmap) library(viridis) table2&lt;-as.data.frame(t) %&gt;% pivot_wider(names_from = Var1, values_from = Freq) %&gt;% column_to_rownames( var = &quot;Var2&quot;) pheatmap(table2 , color = rocket(10,direction =-1)) On utilise la même technique mais en représenant une grandeur différentes : les tests du chi2 partiels, pour apprécier les sous ou les sur-représentation. library(RColorBrewer) chi2df&lt;- as.data.frame(chi2$stdres) table2&lt;-chi2df %&gt;% pivot_wider(names_from = Var1, values_from = Freq) %&gt;% column_to_rownames( var = &quot;Var2&quot;) pheatmap(table2 , color = brewer.pal(n = 9, name = &quot;RdBu&quot;)) 5.2.2.5 Les treemaps, cest merveilleux Dautre graphiques et des emboitements library(treemapify) tree1&lt;-df %&gt;% mutate(n=1)%&gt;%group_by(cntry,genre,habitat) %&gt;% summarize(n=sum(n),mean=mean(trust_interpersonnel, na.rm=TRUE)) g10 &lt;- ggplot(tree1, aes(area = n, fill=genre,subgroup=cntry)) + geom_treemap() + geom_treemap_text(aes(label=habitat),colour = &quot;white&quot;, place = &quot;centre&quot;,grow = FALSE)+ geom_treemap_subgroup_text(color=&quot;white&quot;,grow = FALSE)+ geom_treemap_subgroup_border() g10 "],["analyse-graphique-multivariée.html", "Chapitre 6 Analyse graphique multivariée 6.1 La confiance institutionnelle, en détail 6.2 Table de corrélation 6.3 Un cas plus complexe : présidentielle2020 6.4 une boucle pour produire de multiple graphe en variant un paramètre 6.5 Modéliser le biais du sondeur", " Chapitre 6 Analyse graphique multivariée Dans ce chapitre, on généralise à des ensembles de variables. 6.1 La confiance institutionnelle, en détail On veut reprénter 6 variables, correspondant à 5 types dhabitats et 2 pays. df&lt;-readRDS(&quot;./data/dfTrust.rds)&quot;) rad&lt;-df %&gt;% group_by (habitat,cntry) %&gt;% summarize(Partis=mean(Partis, na.rm=TRUE), Parlement=mean(Parlement, na.rm=TRUE), Politiques=mean(Politiques, na.rm=TRUE), Police=mean(Police, na.rm=TRUE), Justice=mean(Justice, na.rm=TRUE), NationsUnies=mean(NationsUnies, na.rm=TRUE), ParlementEurop=mean(ParlementEurop, na.rm=TRUE)) %&gt;% filter(!is.na(habitat)) %&gt;% gather(variable, value, -habitat, -cntry) ggplot(rad, aes(x=reorder(variable, value),y=value, group=habitat))+ geom_line(aes(color=habitat), size=2)+ facet_grid(.~cntry) +coord_flip()+ scale_color_brewer(type=&quot;div&quot;,palette=3)+labs(title= &quot;Les éléments de la confiance institutionnelle&quot;, caption=&quot;ESS2002-2018&quot;,y= &quot;confiance (de 1 à 10)&quot;,x=&quot;institutions&quot;) Une autre variante qui donne lévolution de lévolution de les éléments de la confiance institutionnelle rad&lt;-df %&gt;% group_by (Year,cntry) %&gt;% summarize(Partis=mean(Partis, na.rm=TRUE), Parlement=mean(Parlement, na.rm=TRUE), Politiques=mean(Politiques, na.rm=TRUE), Police=mean(Police, na.rm=TRUE), Justice=mean(Justice, na.rm=TRUE), NationsUnies=mean(NationsUnies, na.rm=TRUE), ParlementEurop=mean(ParlementEurop, na.rm=TRUE)) %&gt;% gather(variable, value, -Year, -cntry) ggplot(rad, aes(x=Year,y=value, group=variable))+ geom_line(aes(color=variable), size=1.2)+ facet_wrap(.~cntry, nrow=1) + scale_color_brewer(palette=&quot;Spectral&quot;)+labs(title= &quot;Les éléments de la confiance institutionnelle&quot;, caption=&quot;ESS2002-2018&quot;,y= &quot;confiance (de 1 à 10)&quot;,x=&quot;institutions&quot;) La différence entre les deux pays est claire, la rupture est accusée plus fortement en France quen Allemagne. Lexplication nest sans doute pas culturelle mais démographique, un coup doeil à la carte des densité permet de comprendre mieux : https://www.populationdata.net/cartes/allemagne-france-densite-de-population-2011/. On pourra tenté un graphe en radar. Mais il nest pas si convaincant. library(fmsb) rad&lt;-df %&gt;% filter(cntry==&quot;FR&quot;) %&gt;% group_by (habitat) %&gt;% summarize(Partis=mean(Partis, na.rm=TRUE), Parlement=mean(Parlement, na.rm=TRUE), Politiques=mean(Politiques, na.rm=TRUE), Police=mean(Police, na.rm=TRUE), Justice=mean(Justice, na.rm=TRUE), NationsUnies=mean(NationsUnies, na.rm=TRUE), ParlementEurop=mean(ParlementEurop, na.rm=TRUE)) %&gt;% filter(!is.na(habitat)) %&gt;% dplyr::select(-habitat) #on doit indiquer les valeurs minimale et maximale - la fonction rep permet de repeter (ici 7 fois pour les 7 variables/col) data &lt;- rbind(rep(7,7) , rep(3,7) , rad) #l&#39;autre method c&#39;est ce choisir maxmin=FALSE #rownames(rad) &lt;- c(&quot;big city&quot;, &quot;suburbs&quot; ,&quot;town&quot;,&quot;village&quot;, &quot;countryside&quot;) radarchart(rad, axistype=0, seg=4, title=&quot;Moyenne par institution&quot;, maxmin=FALSE) legend(x=0.7, y=1, legend = rownames(rad), bty = &quot;n&quot;,text.col = &quot;grey&quot;, cex=1.2, pt.cex=3) 6.2 Table de corrélation Comparer les moyennes est une chose, on souhaiter en plus savoir quelle structure de corrélation les caractérisent. Rien de plus simple library(ggcorrplot) df&lt;-readRDS(&quot;./data/dfTrust.rds)&quot;)%&gt;%filter(Year==2018) foo&lt;-df %&gt;% dplyr::select(NationsUnies,ParlementEurop, Parlement, Justice, Police, Politiques, Partis) %&gt;% drop_na() r&lt;-cor(foo) ggcorrplot(r, hc.order = TRUE, type = &quot;lower&quot;, lab = TRUE) g&lt;-paste0(&quot;./plot/g1&quot;,&quot;.jpg&quot;) ggsave(g,plot=last_plot(), width = 27, height = 19, units = &quot;cm&quot;) 6.3 Un cas plus complexe : présidentielle2020 Nsppolls cumulent les sondages publiés des grands instituts. On utilise ces données , ainsi quune boucle, pour explorer différents paramètre dun modèle de lissage. Le but : mieux percevoir les tendance par une sorte de méta-analyse des différents sondages : 6.4 une boucle pour produire de multiple graphe en variant un paramètre library(lubridate) alph&lt;-.5 for (alph in seq(from=0, to= 1, by=.05)){ df_pol &lt;- read_delim(&quot;https://raw.githubusercontent.com/nsppolls/nsppolls/master/presidentielle.csv&quot;, delim = &quot;,&quot;, escape_double = FALSE, trim_ws = TRUE)%&gt;% filter(tour==&quot;Premier tour&quot;) %&gt;%filter(candidat==&quot;Eric Zemmour&quot;| candidat== &quot;Marine Le Pen&quot;| candidat== &quot;Emmanuel Macron&quot;| candidat== &quot;Jean-Luc Mélenchon&quot;| candidat== &quot;Yannick Jadot&quot;| candidat== &quot;Valérie Pécresse&quot;| candidat==&quot;Fabien Roussel&quot;| candidat==&quot;Anne Hidalgo&quot;) %&gt;% filter(fin_enquete&gt;ymd(&quot;2022-01-09&quot;)) # on commence en septembre , octobre est-il meilleur ? table(df_pol$candidat) SensiP1&lt;-c(&quot;pink&quot;, &quot;orange&quot;, &quot;gray20&quot;, &quot;red&quot;,&quot;firebrick&quot;, &quot;Royalblue&quot;, &quot; skyblue&quot;, &quot;Chartreuse&quot;) ggplot(df_pol, aes(y=intentions, x=fin_enquete))+ geom_point(aes(color=candidat), size=.5, alpha=1-alph)+ geom_smooth(span = alph, aes(col=candidat,fill=candidat), alpha=0.2)+ scale_color_manual(values=SensiP1)+ scale_fill_manual(values=SensiP1)+ labs(title= &quot;Evolution des intentions de vote #présidentielle2022 1er tour&quot;, subtitle =paste(&quot;Lissage méthode loess. alpha=&quot;,alph, &quot; - ci=95%&quot;), caption = &quot;data @nsppolls viz @benavent&quot;, x=NULL)+theme_minimal()+scale_x_date(date_breaks = &quot;1 month&quot;, date_minor_breaks = &quot;1 week&quot;, date_labels = &quot;%B&quot;) sondage_nsppolls&lt;-paste0(&quot;./nsppolls/sondage_nsppolls&quot;, alph*20, &quot;.jpg&quot;) ggsave(sondage_nsppolls,plot=last_plot(), width = 27, height = 19, units = &quot;cm&quot;) } n&lt;-df_pol%&gt;% mutate(n=1)%&gt;% group_by(id)%&gt;%summarise(n=sum(n)) #nombre de sondage n&lt;-nrow(n) Pour créer le gif on emplie magick. On a pris soin de sauvegarder les graphes dans un répertoire propre, ça facilite la lecture en boucle et la fabrication du gif. library(magick) #gif #on constitue une liste des noms des fichier *.jpg que l&#39;on veut associer frames &lt;- paste0(&quot;./nsppolls/&quot;,&quot;sondage_nsppolls&quot;, 0:20,&quot;.jpg&quot;) #on lit et on stoke dans m les images m &lt;- image_read(frames) #on fabrique et on sauvergarde le gif m &lt;- image_animate(m, fps=1) image_write(m, &quot;./plot/sondages_lissage.gif&quot;) 6.4.1 effet sondeur pour anticiper sur le chapitre suivant foo&lt;-df_pol%&gt;% dplyr::select(candidat, intentions, fin_enquete, echantillon,nom_institut)%&gt;% group_by(nom_institut, candidat)%&gt;% summarise(moy=mean(intentions, na.rm=TRUE), std=sd(intentions, na.rm=TRUE)) SensiP2&lt;-c(&quot;gray90&quot;,&quot;gray20&quot;, &quot;Royalblue&quot;, &quot;skyblue&quot;, &quot;orange&quot;, &quot;yellow&quot;, &quot;pink&quot;, &quot;firebrick&quot;, &quot;green&quot;) g&lt;-ggplot(foo,aes(x=candidat,y=moy))+ geom_segment(aes(x = candidat, y = -std+moy, xend = candidat, yend = std+moy, color = nom_institut), size=1.2)+ geom_point(aes(color=nom_institut), size=2)+ scale_color_manual(values = SensiP2)+ theme_minimal()+ coord_flip() g 6.5 Modéliser le biais du sondeur http://www.stat.yale.edu/Courses/1997-98/101/anovareg.htm df_pol$tps&lt;-2 df_pol$tps[df_pol$fin_enquete &lt; ymd(&quot;2022-01-31&quot;)]&lt;-1 df_pol$tps[df_pol$fin_enquete &gt; ymd(&quot;2022-03-01&quot;)]&lt;-3 df_pol$tps&lt;- as.factor(df_pol$tps) fit1&lt;- lm(intentions~candidat*tps,data=df_pol) anova(fit1) ## Analysis of Variance Table ## ## Response: intentions ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## candidat 7 91455 13065.0 13327.297 &lt; 2.2e-16 *** ## tps 2 22 11.1 11.336 1.291e-05 *** ## candidat:tps 14 2591 185.1 188.785 &lt; 2.2e-16 *** ## Residuals 1608 1576 1.0 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 fit2&lt;- lm(intentions~candidat*tps+candidat*nom_institut,data=df_pol) anova(fit2) ## Analysis of Variance Table ## ## Response: intentions ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## candidat 7 91455 13065.0 18131.7707 &lt; 2.2e-16 *** ## tps 2 22 11.1 15.4233 2.332e-07 *** ## nom_institut 8 22 2.7 3.7658 0.0002207 *** ## candidat:tps 14 2591 185.1 256.8418 &lt; 2.2e-16 *** ## candidat:nom_institut 56 442 7.9 10.9564 &lt; 2.2e-16 *** ## Residuals 1544 1113 0.7 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(fit1,fit2) ## Analysis of Variance Table ## ## Model 1: intentions ~ candidat * tps ## Model 2: intentions ~ candidat * tps + candidat * nom_institut ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 1608 1576.3 ## 2 1544 1112.5 64 463.81 10.057 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(fit1) ## ## Call: ## lm(formula = intentions ~ candidat * tps, data = df_pol) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.7639 -0.5882 0.0062 0.5741 4.0741 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.1765 0.1386 22.911 &lt; 2e-16 *** ## candidatEmmanuel Macron 21.5392 0.1961 109.854 &lt; 2e-16 *** ## candidatEric Zemmour 9.7549 0.1961 49.752 &lt; 2e-16 *** ## candidatFabien Roussel -0.8039 0.1961 -4.100 4.33e-05 *** ## candidatJean-Luc Mélenchon 6.4118 0.1961 32.701 &lt; 2e-16 *** ## candidatMarine Le Pen 13.7353 0.1961 70.053 &lt; 2e-16 *** ## candidatValérie Pécresse 13.2255 0.1961 67.453 &lt; 2e-16 *** ## candidatYannick Jadot 2.7255 0.1961 13.901 &lt; 2e-16 *** ## tps2 -0.6765 0.1812 -3.733 0.000196 *** ## tps3 -0.8431 0.1770 -4.764 2.07e-06 *** ## candidatEmmanuel Macron:tps2 0.6844 0.2563 2.671 0.007648 ** ## candidatEric Zemmour:tps2 2.0645 0.2563 8.056 1.52e-15 *** ## candidatFabien Roussel:tps2 2.1511 0.2563 8.394 &lt; 2e-16 *** ## candidatJean-Luc Mélenchon:tps2 1.6438 0.2563 6.414 1.86e-10 *** ## candidatMarine Le Pen:tps2 0.5842 0.2563 2.279 0.022772 * ## candidatValérie Pécresse:tps2 -0.9616 0.2563 -3.752 0.000181 *** ## candidatYannick Jadot:tps2 -0.1630 0.2563 -0.636 0.524863 ## candidatEmmanuel Macron:tps3 5.5534 0.2503 22.187 &lt; 2e-16 *** ## candidatEric Zemmour:tps3 -0.4956 0.2503 -1.980 0.047850 * ## candidatFabien Roussel:tps3 2.2607 0.2503 9.032 &lt; 2e-16 *** ## candidatJean-Luc Mélenchon:tps3 3.4956 0.2503 13.966 &lt; 2e-16 *** ## candidatMarine Le Pen:tps3 1.6968 0.2503 6.779 1.69e-11 *** ## candidatValérie Pécresse:tps3 -4.0650 0.2503 -16.241 &lt; 2e-16 *** ## candidatYannick Jadot:tps3 0.4967 0.2503 1.985 0.047363 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9901 on 1608 degrees of freedom ## Multiple R-squared: 0.9835, Adjusted R-squared: 0.9833 ## F-statistic: 4172 on 23 and 1608 DF, p-value: &lt; 2.2e-16 summary(fit2) ## ## Call: ## lm(formula = intentions ~ candidat * tps + candidat * nom_institut, ## data = df_pol) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.0364 -0.4365 -0.0473 0.5315 4.0598 ## ## Coefficients: ## Estimate Std. Error ## (Intercept) 2.996288 0.343330 ## candidatEmmanuel Macron 21.420362 0.485542 ## candidatEric Zemmour 10.309018 0.485542 ## candidatFabien Roussel -0.698575 0.485542 ## candidatJean-Luc Mélenchon 6.675419 0.485542 ## candidatMarine Le Pen 13.939990 0.485542 ## candidatValérie Pécresse 13.036636 0.485542 ## candidatYannick Jadot 2.896003 0.485542 ## tps2 -0.677181 0.156273 ## tps3 -0.904913 0.158066 ## nom_institutCluster17 -0.653941 0.419813 ## nom_institutElabe -0.404567 0.405893 ## nom_institutHarris Interactive 0.203762 0.400643 ## nom_institutIfop 0.232704 0.341681 ## nom_institutIpsos 0.458974 0.373498 ## nom_institutKantar Public -0.333952 0.586121 ## nom_institutOdoxa -0.455241 0.681187 ## nom_institutOpinion Way 0.377135 0.335325 ## candidatEmmanuel Macron:tps2 0.746163 0.221004 ## candidatEric Zemmour:tps2 2.129084 0.221004 ## candidatFabien Roussel:tps2 2.123362 0.221004 ## candidatJean-Luc Mélenchon:tps2 1.701235 0.221004 ## candidatMarine Le Pen:tps2 0.573065 0.221004 ## candidatValérie Pécresse:tps2 -0.988110 0.221004 ## candidatYannick Jadot:tps2 -0.145368 0.221004 ## candidatEmmanuel Macron:tps3 5.641284 0.223539 ## candidatEric Zemmour:tps3 -0.355324 0.223539 ## candidatFabien Roussel:tps3 2.285824 0.223539 ## candidatJean-Luc Mélenchon:tps3 3.717399 0.223539 ## candidatMarine Le Pen:tps3 1.943484 0.223539 ## candidatValérie Pécresse:tps3 -3.945058 0.223539 ## candidatYannick Jadot:tps3 0.379679 0.223539 ## candidatEmmanuel Macron:nom_institutCluster17 -0.411213 0.593705 ## candidatEric Zemmour:nom_institutCluster17 1.545945 0.593705 ## candidatFabien Roussel:nom_institutCluster17 0.563482 0.593705 ## candidatJean-Luc Mélenchon:nom_institutCluster17 3.078867 0.593705 ## candidatMarine Le Pen:nom_institutCluster17 -1.052262 0.593705 ## candidatValérie Pécresse:nom_institutCluster17 -0.357875 0.593705 ## candidatYannick Jadot:nom_institutCluster17 0.198241 0.593705 ## candidatEmmanuel Macron:nom_institutElabe 0.691749 0.574020 ## candidatEric Zemmour:nom_institutElabe -0.982306 0.574020 ## candidatFabien Roussel:nom_institutElabe 0.325718 0.574020 ## candidatJean-Luc Mélenchon:nom_institutElabe 0.769716 0.574020 ## candidatMarine Le Pen:nom_institutElabe 0.085361 0.574020 ## candidatValérie Pécresse:nom_institutElabe 0.236341 0.574020 ## candidatYannick Jadot:nom_institutElabe 0.486314 0.574020 ## candidatEmmanuel Macron:nom_institutHarris Interactive 0.336971 0.566595 ## candidatEric Zemmour:nom_institutHarris Interactive -0.007437 0.566595 ## candidatFabien Roussel:nom_institutHarris Interactive -0.645601 0.566595 ## candidatJean-Luc Mélenchon:nom_institutHarris Interactive 0.543168 0.566595 ## candidatMarine Le Pen:nom_institutHarris Interactive 0.198796 0.566595 ## candidatValérie Pécresse:nom_institutHarris Interactive -0.823119 0.566595 ## candidatYannick Jadot:nom_institutHarris Interactive 0.072290 0.566595 ## candidatEmmanuel Macron:nom_institutIfop 0.558648 0.483210 ## candidatEric Zemmour:nom_institutIfop -0.271076 0.483210 ## candidatFabien Roussel:nom_institutIfop -0.156132 0.483210 ## candidatJean-Luc Mélenchon:nom_institutIfop -0.513746 0.483210 ## candidatMarine Le Pen:nom_institutIfop -0.037347 0.483210 ## candidatValérie Pécresse:nom_institutIfop 0.270337 0.483210 ## candidatYannick Jadot:nom_institutIfop -0.365942 0.483210 ## candidatEmmanuel Macron:nom_institutIpsos 0.311750 0.528206 ## candidatEric Zemmour:nom_institutIpsos -0.761888 0.528206 ## candidatFabien Roussel:nom_institutIpsos -0.459298 0.528206 ## candidatJean-Luc Mélenchon:nom_institutIpsos -0.818654 0.528206 ## candidatMarine Le Pen:nom_institutIpsos -1.908563 0.528206 ## candidatValérie Pécresse:nom_institutIpsos -1.004626 0.528206 ## candidatYannick Jadot:nom_institutIpsos 0.916043 0.528206 ## candidatEmmanuel Macron:nom_institutKantar Public 1.070060 0.828900 ## candidatEric Zemmour:nom_institutKantar Public 0.884836 0.828900 ## candidatFabien Roussel:nom_institutKantar Public -0.033096 0.828900 ## candidatJean-Luc Mélenchon:nom_institutKantar Public 0.779237 0.828900 ## candidatMarine Le Pen:nom_institutKantar Public 0.573332 0.828900 ## candidatValérie Pécresse:nom_institutKantar Public -0.577227 0.828900 ## candidatYannick Jadot:nom_institutKantar Public -0.767333 0.828900 ## candidatEmmanuel Macron:nom_institutOdoxa 0.635914 0.963344 ## candidatEric Zemmour:nom_institutOdoxa -0.445898 0.963344 ## candidatFabien Roussel:nom_institutOdoxa 0.743981 0.963344 ## candidatJean-Luc Mélenchon:nom_institutOdoxa 0.115264 0.963344 ## candidatMarine Le Pen:nom_institutOdoxa 1.801735 0.963344 ## candidatValérie Pécresse:nom_institutOdoxa -0.820052 0.963344 ## candidatYannick Jadot:nom_institutOdoxa -0.013158 0.963344 ## candidatEmmanuel Macron:nom_institutOpinion Way -0.493794 0.474222 ## candidatEric Zemmour:nom_institutOpinion Way -1.309243 0.474222 ## candidatFabien Roussel:nom_institutOpinion Way -0.073700 0.474222 ## candidatJean-Luc Mélenchon:nom_institutOpinion Way -1.004659 0.474222 ## candidatMarine Le Pen:nom_institutOpinion Way -0.240672 0.474222 ## candidatValérie Pécresse:nom_institutOpinion Way 0.632251 0.474222 ## candidatYannick Jadot:nom_institutOpinion Way -0.380987 0.474222 ## t value Pr(&gt;|t|) ## (Intercept) 8.727 &lt; 2e-16 *** ## candidatEmmanuel Macron 44.116 &lt; 2e-16 *** ## candidatEric Zemmour 21.232 &lt; 2e-16 *** ## candidatFabien Roussel -1.439 0.150424 ## candidatJean-Luc Mélenchon 13.748 &lt; 2e-16 *** ## candidatMarine Le Pen 28.710 &lt; 2e-16 *** ## candidatValérie Pécresse 26.850 &lt; 2e-16 *** ## candidatYannick Jadot 5.964 3.04e-09 *** ## tps2 -4.333 1.56e-05 *** ## tps3 -5.725 1.24e-08 *** ## nom_institutCluster17 -1.558 0.119510 ## nom_institutElabe -0.997 0.319050 ## nom_institutHarris Interactive 0.509 0.611114 ## nom_institutIfop 0.681 0.495938 ## nom_institutIpsos 1.229 0.219314 ## nom_institutKantar Public -0.570 0.568919 ## nom_institutOdoxa -0.668 0.504039 ## nom_institutOpinion Way 1.125 0.260898 ## candidatEmmanuel Macron:tps2 3.376 0.000753 *** ## candidatEric Zemmour:tps2 9.634 &lt; 2e-16 *** ## candidatFabien Roussel:tps2 9.608 &lt; 2e-16 *** ## candidatJean-Luc Mélenchon:tps2 7.698 2.46e-14 *** ## candidatMarine Le Pen:tps2 2.593 0.009604 ** ## candidatValérie Pécresse:tps2 -4.471 8.35e-06 *** ## candidatYannick Jadot:tps2 -0.658 0.510788 ## candidatEmmanuel Macron:tps3 25.236 &lt; 2e-16 *** ## candidatEric Zemmour:tps3 -1.590 0.112144 ## candidatFabien Roussel:tps3 10.226 &lt; 2e-16 *** ## candidatJean-Luc Mélenchon:tps3 16.630 &lt; 2e-16 *** ## candidatMarine Le Pen:tps3 8.694 &lt; 2e-16 *** ## candidatValérie Pécresse:tps3 -17.648 &lt; 2e-16 *** ## candidatYannick Jadot:tps3 1.698 0.089617 . ## candidatEmmanuel Macron:nom_institutCluster17 -0.693 0.488651 ## candidatEric Zemmour:nom_institutCluster17 2.604 0.009306 ** ## candidatFabien Roussel:nom_institutCluster17 0.949 0.342721 ## candidatJean-Luc Mélenchon:nom_institutCluster17 5.186 2.43e-07 *** ## candidatMarine Le Pen:nom_institutCluster17 -1.772 0.076531 . ## candidatValérie Pécresse:nom_institutCluster17 -0.603 0.546743 ## candidatYannick Jadot:nom_institutCluster17 0.334 0.738497 ## candidatEmmanuel Macron:nom_institutElabe 1.205 0.228351 ## candidatEric Zemmour:nom_institutElabe -1.711 0.087231 . ## candidatFabien Roussel:nom_institutElabe 0.567 0.570503 ## candidatJean-Luc Mélenchon:nom_institutElabe 1.341 0.180143 ## candidatMarine Le Pen:nom_institutElabe 0.149 0.881803 ## candidatValérie Pécresse:nom_institutElabe 0.412 0.680595 ## candidatYannick Jadot:nom_institutElabe 0.847 0.397011 ## candidatEmmanuel Macron:nom_institutHarris Interactive 0.595 0.552111 ## candidatEric Zemmour:nom_institutHarris Interactive -0.013 0.989529 ## candidatFabien Roussel:nom_institutHarris Interactive -1.139 0.254696 ## candidatJean-Luc Mélenchon:nom_institutHarris Interactive 0.959 0.337884 ## candidatMarine Le Pen:nom_institutHarris Interactive 0.351 0.725741 ## candidatValérie Pécresse:nom_institutHarris Interactive -1.453 0.146497 ## candidatYannick Jadot:nom_institutHarris Interactive 0.128 0.898493 ## candidatEmmanuel Macron:nom_institutIfop 1.156 0.247812 ## candidatEric Zemmour:nom_institutIfop -0.561 0.574886 ## candidatFabien Roussel:nom_institutIfop -0.323 0.746652 ## candidatJean-Luc Mélenchon:nom_institutIfop -1.063 0.287860 ## candidatMarine Le Pen:nom_institutIfop -0.077 0.938404 ## candidatValérie Pécresse:nom_institutIfop 0.559 0.575929 ## candidatYannick Jadot:nom_institutIfop -0.757 0.448977 ## candidatEmmanuel Macron:nom_institutIpsos 0.590 0.555140 ## candidatEric Zemmour:nom_institutIpsos -1.442 0.149390 ## candidatFabien Roussel:nom_institutIpsos -0.870 0.384685 ## candidatJean-Luc Mélenchon:nom_institutIpsos -1.550 0.121376 ## candidatMarine Le Pen:nom_institutIpsos -3.613 0.000312 *** ## candidatValérie Pécresse:nom_institutIpsos -1.902 0.057362 . ## candidatYannick Jadot:nom_institutIpsos 1.734 0.083073 . ## candidatEmmanuel Macron:nom_institutKantar Public 1.291 0.196918 ## candidatEric Zemmour:nom_institutKantar Public 1.067 0.285921 ## candidatFabien Roussel:nom_institutKantar Public -0.040 0.968156 ## candidatJean-Luc Mélenchon:nom_institutKantar Public 0.940 0.347321 ## candidatMarine Le Pen:nom_institutKantar Public 0.692 0.489244 ## candidatValérie Pécresse:nom_institutKantar Public -0.696 0.486297 ## candidatYannick Jadot:nom_institutKantar Public -0.926 0.354734 ## candidatEmmanuel Macron:nom_institutOdoxa 0.660 0.509281 ## candidatEric Zemmour:nom_institutOdoxa -0.463 0.643526 ## candidatFabien Roussel:nom_institutOdoxa 0.772 0.440061 ## candidatJean-Luc Mélenchon:nom_institutOdoxa 0.120 0.904776 ## candidatMarine Le Pen:nom_institutOdoxa 1.870 0.061632 . ## candidatValérie Pécresse:nom_institutOdoxa -0.851 0.394760 ## candidatYannick Jadot:nom_institutOdoxa -0.014 0.989104 ## candidatEmmanuel Macron:nom_institutOpinion Way -1.041 0.297912 ## candidatEric Zemmour:nom_institutOpinion Way -2.761 0.005834 ** ## candidatFabien Roussel:nom_institutOpinion Way -0.155 0.876516 ## candidatJean-Luc Mélenchon:nom_institutOpinion Way -2.119 0.034288 * ## candidatMarine Le Pen:nom_institutOpinion Way -0.508 0.611870 ## candidatValérie Pécresse:nom_institutOpinion Way 1.333 0.182650 ## candidatYannick Jadot:nom_institutOpinion Way -0.803 0.421871 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8489 on 1544 degrees of freedom ## Multiple R-squared: 0.9884, Adjusted R-squared: 0.9877 ## F-statistic: 1508 on 87 and 1544 DF, p-value: &lt; 2.2e-16 library(jtools) library(interactions) cat_plot(fit2, pred=candidat,modx= nom_institut, color.class=&quot;Spectral&quot;)+ scale_color_manual(values = SensiP2)+coord_flip() cat_plot(fit2, pred= tps,modx=candidat, color.class=&quot;Spectral&quot;, dodge.width=0)+ scale_color_manual(values = SensiP2)+geom_line(aes(color=candidat)) "],["analyses-factorielles.html", "Chapitre 7 Analyses factorielles 7.1 Origine et histoire 7.2 Le modèle factoriel des tests psychologiques 7.3 Examen de la matrice de corrélation 7.4 Modèle factoriel 7.5 Lanalyse en composante principale 7.6 Une généralisation de lACP : lAFC 7.7 Développements", " Chapitre 7 Analyses factorielles 7.1 Origine et histoire Par analyse factorielle, on entend finalement un ensemble de méthodes dont lobjectifs est dextraire dun ensemble multivariée de données, un petit nombre de dimensions, les facteurs, qui rendent compte lessentiel des variations. Elles partagent aussi une même structure mathématique qui permet de décomposer et de réduire une matrice de données en un ensemble de matrice de dimensions réduite. On peut en distinguer deux écoles, lune alimentée par des questions de psychométrie a nourrit plusieurs decennies de recherche en traitant les tests psychométriques. Lautre française sintéressent aux variables qualitatives, et a une perspective plus descriptive et représentationnelle. 7.1.1 Une petite histoire de la psychométrie Lanalyse factorielle trouve son origine, en psychologie, dans lintuition que dans des épreuves multiples un facteur principal contrôle les variation des items (les performance à différents tests). Mais cest avec Thurstone que lidée prend toute son ampleur en permettant que plusieurs facteurs traduisent la structure de la matrice de corrélations entre les tests. Spearman, hotelling,. Dans le monde de la gestion et en particulier de la GRH et du marketing, largement inspirés par la psychologie et la psychologie sociale, ces méthodes se sont propagées et ont formalisé un processus détude largement fondé sur ces techniques. Il est bien connu par de processus de Churchill qui a synthésisé une manière de construire et de développé des instruments de mesure par questionnaire. Cest l article de historique de Churchill ( ref). 7.1.2 Lécole française de lanalyse des données appliquée aux sciences sociales Un personnage : Emile Benzekri Boudieu en premier applicateurs Une école Française : pagès, escoffier, morisseau, Husson a repris le flambleau en développant FactoMiner. Une série de logiciels : Alceste, Statitcf 7.2 Le modèle factoriel des tests psychologiques 7.2.1 Un peu de théorie La première historiquement est celle des psychologues et en particulier le modèle en terme de facteurs communs et spécifiques. Elle vise à partir de lanalyse dune matrice de corrélation à identifier des éléments de structures sous-jascents. La structure du modèle factoriel peut être présentée de manière simple. On supposera que chaque variables observées peut être décrites comme composées de facteurs généraux (\\(F_{ik}\\) ) et de facteurs spécifiques $ _{i}$. Le modèle suppose ainsi que la valeur de lindividu i pour la variable j, dépend de k facteurs sous jascents, les facteurs communs, et dun terme spécifique à litem et à lindividus $$ x_{ij}= a_{1j}F_{i1} + a_{2j}F_{i2} + + a_{jk}F_{ik}+_{ij} $$ On peut représenter celà de manière plus graphique, en utilisant les conventions symboliques des modèles structurels quon examine dans le chapitre 9. On y verra dailleurs comment ce modèle peut être spécifié de manière confirmatoire. On remarquera dans cette structure que les facteurs peuvent être corrélés. Modèle Factoriel exploratoire - EFA 7.2.2 Lestimation Certains lecteurs seront surpris de cette présentation, ils sont sans doute plus habitués à factoriser en employant une méthode de l ACP. Effectivement cette méthode sur laquelle on va revenir avec plus de détail dans la seconde section de ce chapitre, est une des techniques qui permettent dapprocher le modèle théorique que lon vient de présenter. Elle nest pas la seule. Lestimation du modèle requierts deux décisions : lune sur la méthode dextraction des facteurs, et lautres sur la méthode de rotation. les méthodes dextraction ACP ML analyse en facteur principaux et spécifiques Les méthodes de rotation. Varimax Promax Oblimin  7.2.3 Ressources On utilise principalement le package psych développé par Revelle et dédié à la psychométrie. Il couvre le plus complétement le champs de lanalyse factorielle et de la psychométrie. Sy ajoutent deux fonctions très utiles pour représenter le résultats des analyses sous une forme lisible et au standard des publications scientifiques. Elles utilisent les ressources de flextable. #library(corrplot) #library(psych) #library(flextable) # Une fonction utile pour créer flex &lt;- function(data, title=NULL) { # this grabs the data and converts it to a flextbale flextable(data) %&gt;% # this makes the table fill the page width set_table_properties(layout = &quot;autofit&quot;, width = 1) %&gt;% # font size fontsize(size=10, part=&quot;all&quot;) %&gt;% #this adds a ttitlecreates an automatic table number set_caption(title, autonum = officer::run_autonum(seq_id = &quot;tab&quot;, pre_label = &quot;Table &quot;, post_label = &quot;\\n&quot;, bkm = &quot;anytable&quot;)) %&gt;% # font type font(fontname=&quot;Times New Roman&quot;, part=&quot;all&quot;) } # et une seconde fonction pour le tableaux des loadings fa_table &lt;- function(x, cut) { #get sorted loadings loadings &lt;- fa.sort(x)$loadings %&gt;% round(3) #supress loadings loadings[loadings &lt; cut] &lt;- &quot;&quot; #get additional info add_info &lt;- cbind(x$communality, x$uniquenesses, x$complexity) %&gt;% # make it a data frame as.data.frame() %&gt;% # column names rename(&quot;Communality&quot; = V1, &quot;Uniqueness&quot; = V2, &quot;Complexity&quot; = V3) %&gt;% #get the item names from the vector rownames_to_column(&quot;item&quot;) #build table loadings %&gt;% unclass() %&gt;% as.data.frame() %&gt;% rownames_to_column(&quot;item&quot;) %&gt;% left_join(add_info) %&gt;% mutate(across(where(is.numeric), round, 3)) } Lobjectif des méthodes danalyses factorielles est de réduire un ensemble de variables à un petit nombre de dimensions qui résument lessentiel de linformation. 7.2.4 Cas dapplication Pour appliquer la méthode on va sinteresser à léchelle des valeurs de kahle qui sont mesurée dan différents pays au cours des différentes vagues de lenquête ESS? Les variables mesurées sont un ensemble de 21 questions qui proposent des niveaux dimportances accordées à 21 questions, ou items, dont voici les formulation en anglais. Les répondants ont le choix sur une échelle de 0 à 10 qui va de pas du tout important à très important. On se concentre sur les observations de la dernière vague. Cette échelle a été développée par kahle. En voici les itms dans leur formulation anglaise. IPCRTIV Important to think new ideas and being creative IMPRICH Important to be rich, have money and expensive things IPEQOPT Important that people are treated equally and have equal opportunities IPSHABT Important to show abilities and be admired IMPSAFE Important to live in secure and safe surroundings IMPDIFF Important to try new and different things in life IPFRULE Important to do what is told and follow rules IPUDRST Important to understand different people IPMODST Important to be humble and modest, not draw attention IPGDTIM Important to have a good time IMPFREE Important to make own decisions and be free IPHLPPL Important to help people and care for others well-being IPSUCES Important to be successful and that people recognise achievements IPSTRGV Important that government is strong and ensures safety IPADVNT Important to seek adventures and have an exciting life IPBHPRP Important to behave properly IPRSPOT Important to get respect from others IPLYLFR Important to be loyal to friends and devote to people close IMPENV Important to care for nature and environment IMPTRAD Important to follow traditions and customs IMPFUN Important to seek fun and things that give pleasure # On renomme les variables pour une meilleure lecture et on selectionne le tableau de données utile à l&#39;analyse. df &lt;- read_csv(&quot;./Data/ESS1-9e01_1.csv&quot;) %&gt;% rename( V_creative=ipcrtiv, V_richness= imprich, V_justice =ipeqopt, V_admiration=ipshabt, V_security=impsafe, V_novelty=impdiff, V_conformism=ipfrule, V_openmindedness=ipudrst, V_modesty=ipmodst, V_fun=ipgdtim, V_autonomy=impfree, V_Care=iphlppl, V_Success=ipsuces, V_Autority =ipstrgv, V_Adventures=ipadvnt, V_wellbehavior=ipbhprp, V_respect=iprspot, V_loyalty=iplylfr, V_environnement=impenv, V_tradition=imptrad, V_pleasure=impfun) foo1&lt;-df %&gt;% filter(essround==9)%&gt;% dplyr::select(matches(&quot;V_.*&quot;), cntry) %&gt;% #notons la selection fondée sur des regex drop_na() 7.3 Examen de la matrice de corrélation Calculons la matrice de corrélation, et présentons là en organisant lordre des variables selon leur corrélation. A ce stade indiquons quil sagit de mettre un ordre dans les variables, tel que des variables fortement corrélées soient adjascentes (on revient sur la méthode utilisée dans le chapitre suivant). On saperçoit quune structure émerge. Quatre groupes de variables peuvent être discernées: * la jouissance * le succès social * louverture aux autres * la sécurité Dans le filigrane de la matrice de corrélation, on devine une structure factorielle. foo&lt;- foo1 %&gt;% dplyr::select(matches(&quot;V_.*&quot;)) M = cor(foo) corrplot(M, method=&quot;circle&quot;, order=&quot;hclust&quot;,tl.cex = .7) 7.4 Modèle factoriel Testons un modèle danalyse factorielle à 4 dimensions. Nous laugmentons dun procédure de rotation oblimin pour un meilleur ajustement. fa &lt;- fa(foo,4, rotate=&quot;oblimin&quot;) #principal axis fa_table(fa, .30)%&gt;% flex(&quot;A Pretty Factor Analysis Table&quot;) .tabwid table{ border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-spacing: 0; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-6303d8c2{table-layout:auto;border-collapse:collapse;width:100%;}.cl-62f5e91a{font-family:'Times New Roman';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-62f6100c{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-62f6100d{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-62f684ec{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-62f684ed{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-62f684ee{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-62f684ef{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-62f684f0{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-62f684f1{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 7.1: A Pretty Factor Analysis Table itemMR1MR4MR3MR2CommunalityUniquenessComplexityV_openmindedness0.6920.4720.5281.002V_justice0.6650.3990.6011.044V_Care0.6210.5160.4841.145V_environnement0.5610.4270.5731.171V_loyalty0.5180.4930.5071.570V_autonomy0.4380.3640.6361.603V_creative0.4290.3410.6592.225V_modesty0.4210.3010.3390.6611.967V_admiration0.6970.5050.4951.049V_Success0.6440.5390.4611.094V_richness0.5260.3360.6641.402V_respect0.4190.3790.3830.6172.157V_wellbehavior0.6040.4610.5391.094V_tradition0.5310.3230.6771.111V_conformism0.4640.2650.7351.166V_security0.4450.3860.6142.047V_Autority0.3950.3690.6311.921V_pleasure0.7580.5610.4391.035V_fun0.5310.4130.5871.145V_Adventures0.5060.4650.5351.829V_novelty0.3920.4310.5692.553 fa[[&quot;Vaccounted&quot;]] %&gt;% as.data.frame() %&gt;% #select(1:5) %&gt;% Use this if you have many factors and only want to show a certain number rownames_to_column(&quot;Property&quot;) %&gt;% mutate(across(where(is.numeric), round, 3)) %&gt;% flex(&quot;Eigenvalues and Variance Explained for Rotated Factor Solution&quot;) .tabwid table{ border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-spacing: 0; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-63207572{table-layout:auto;border-collapse:collapse;width:100%;}.cl-6316b7d0{font-family:'Times New Roman';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-6316ded6{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-6316ded7{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-631705be{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-631705bf{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-631705c0{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-631705c1{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-631705c2{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-631705c3{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 7.1: Eigenvalues and Variance Explained for Rotated Factor Solution PropertyMR1MR4MR3MR2SS loadings3.1221.9581.8971.813Proportion Var0.1490.0930.0900.086Cumulative Var0.1490.2420.3320.419Proportion Explained0.3550.2230.2160.206Cumulative Proportion0.3550.5780.7941.000 Le set de données que nous avons traité est composé de 15 échantillons venant dautant de pays. Puisque nous avons réduits les 22 mesures initiales à 4 grands facteurs, il est temps danalyser les différences entre les pays. On va dabord récupérer les scores de chaque observation sur les quatre dimensions obtenues quon ajoute à notre fichier de travail pour récupérer la variable pays. #récupérer les scores scores&lt;-fa$scores scores&lt;-as.data.frame(unclass(scores)) #matcher pour récupérer la variable pays et renommer pour plus de lisibilité df_typo&lt;-cbind(foo1, scores) %&gt;% rename(F_Altruisme = MR1, F_Conservatisme=MR2, F_Performance=MR4, F_Hedonisme=MR3) # On calcule les scores moyens par pays et les erreurs d&#39;échantillonage df_g &lt;- df_typo %&gt;% dplyr::select(matches(&quot;F_.*&quot;), cntry)%&gt;% gather(variable, value,-cntry)%&gt;% mutate(n=1)%&gt;% group_by(variable,cntry)%&gt;% summarize(mean=mean(value), n=sum(n), se=sd(value)/sqrt(n)) #on représente les résultats ggplot(df_g,aes(x=cntry, y=mean))+ geom_bar(stat=&quot;identity&quot;,aes(fill=variable), size=1.5)+ coord_flip()+ geom_errorbar(aes(ymin=mean-se, ymax=mean+se), width=.2, position=position_dodge(.9)) + scale_color_brewer(palette = &quot;Set1&quot;)+ facet_wrap(vars(variable),ncol=2) 7.5 Lanalyse en composante principale LACP, dont loptique est différente dans le sens où lon cherche moins à rendre compte dune structure sous-jascente à la matrice de corrélation , quà réduire linformation dans un espace limité. 7.5.1 le problème théorique De manière intuitive lACP est la technique qui permet de représenter un poisson, une structure, sous son jour le plus intelligible, cest à dire celui qui magnifie ses variations. Examinons un poisson sous différentes projections. La première image rend mieux compte de la forme du poisson que la seconde, elle ne diffère que par la projection. De lune à lautre il ny a quy rotation à 90°C vers la droite. Cest la même image, le même phénomène mais représenté selon deux perspectives, deux bases en terme de mathématiques. On comprend que pour représenter un objet au mieux dans un faible nombre de dimensions, il faut trouver la base vectorielle qui maximise les variations de taille. résoudre ce problème est ce que fait lACP Modèle Factoriel exploratoire - EFA 7.5.2 Une représentation symbolique Lidée va donc être de décomposer une matrice de variance-covariance (ou de corrélation) en respectant une contraintes : faire en sorte que le maximum de variance soit capturée par la première dimension, puis par les suivantes successivement. La solution à ce problème se trouve dans la résolution dun problème matriciel. Il faut procéder à un changement de base, autrement dit à un changement de référentiel. La matrice de variance-covariance, ou de corrélation, si on a, au préalable, centré et standardisé les valeurs des variables, est obtenue simplement en multipliant la matrice de données (individus x variable) par sa transposée. \\[ \\Sigma = XX^t \\] Comme \\(\\Sigma\\) est symétrique, elle est diagonalisable et peut-être représentée par une matrice de score W et une matrice diagonale D. \\[ \\Sigma_{e} =WDW^T \\] où D est la matrice diagonale des valeurs propres et W la matrice des composantes comprenant les j variables ( en ligne) et les k dimensions (en colonne). Léquivalence suppose que le nombre de composantes est égal au nombre de variables initiales, Cependant lusage conduit à ne retenir quun petit nombre de dimensions de telles sorte à ce que la différence entre \\(\\Sigma\\) et \\(\\Sigma_{e}\\) soit relativement petite. La matrice de score comprend autant de lignes que dindividus et de colonnes que de dimensions-sous-jascentes. On remarquera que dans ce modèles on a autant de composantes que de variables, mais que ces dernières représentent une part décroissante de la variance. Certaines composantes nont pas de sens on se concentrera sur les premières rejoignant lidée de lanalyse factorielle : peu de composantes, de facteurs, rendent compte des variations des données. On restera cependant conscient que lACP nest au fond quune manière de représenter les données, juste une projection. Ne retenir que les premières composantes va au-delà du modèle, cest une démarche qui consiste à considérer que seules les premières composantes sont significatives, en apportant du sens, et les dernières peuvent être négligée. Cest une manière approximative de rejoindre le modèle factoriel, une solution simple pour en obtenir une solution. 7.5.3 Application En guise dapplication on va utiliser un tout petit jeu de données issu de lanalyse précédente : le tableau des profils pays, sur les 21 valeurs de Kahle. Avec cette procédure daggrégation on réduit fortement la variance individuelle, pour ne garder que des différences en moyenne dun pays à lautre. Le plus ici ne va plus être de comprendre la structure profonde des données, mais simplement de représenter ces différences dans un espace réduit. foo&lt;-foo1%&gt;% group_by(cntry)%&gt;% summarise(across(V_creative:V_pleasure, ~ mean(.x, na.rm = TRUE))) #on note la fonction qui permet de résumer plusieurs variables à la fois X&lt;- foo%&gt;% dplyr::select(-cntry)%&gt;% as.data.frame() rownames(X) &lt;- foo$cntry Plusieurs bibliothèque, en plus de la fonction de base princomp, propose une solution d ACP. On choisit dutiliser celle du package Factominer quon accompagne de la bibliothèque factoextra pour ses ressources graphiques. Les résultats portent sur 3 éléments : les valeurs propres de chacune des dimensions retenues, les coordonnées des vecteurs variables, et celles des points individus. library(&quot;FactoMineR&quot;) library(&quot;factoextra&quot;) res.pca&lt;-PCA(X, scale.unit = TRUE, ncp = 2, graph = FALSE) print(res.pca) ## **Results for the Principal Component Analysis (PCA)** ## The analysis was performed on 15 individuals, described by 21 variables ## *The results are available in the following objects: ## ## name description ## 1 &quot;$eig&quot; &quot;eigenvalues&quot; ## 2 &quot;$var&quot; &quot;results for the variables&quot; ## 3 &quot;$var$coord&quot; &quot;coord. for the variables&quot; ## 4 &quot;$var$cor&quot; &quot;correlations variables - dimensions&quot; ## 5 &quot;$var$cos2&quot; &quot;cos2 for the variables&quot; ## 6 &quot;$var$contrib&quot; &quot;contributions of the variables&quot; ## 7 &quot;$ind&quot; &quot;results for the individuals&quot; ## 8 &quot;$ind$coord&quot; &quot;coord. for the individuals&quot; ## 9 &quot;$ind$cos2&quot; &quot;cos2 for the individuals&quot; ## 10 &quot;$ind$contrib&quot; &quot;contributions of the individuals&quot; ## 11 &quot;$call&quot; &quot;summary statistics&quot; ## 12 &quot;$call$centre&quot; &quot;mean of the variables&quot; ## 13 &quot;$call$ecart.type&quot; &quot;standard error of the variables&quot; ## 14 &quot;$call$row.w&quot; &quot;weights for the individuals&quot; ## 15 &quot;$call$col.w&quot; &quot;weights for the variables&quot; x&lt;-res.pca$eig Le premier élément danalyse et le graphe des éboulis ( ou scree plot) qui représentent les variances projetées sur chacune des composantes. Ici deux composantes représentent les deux tiers de la variance expliquée. fviz_screeplot(res.pca, ncp=21) library(&quot;corrplot&quot;) corrplot(res.pca$var$cos2, is.corr=FALSE, tl.cex = 0.8) fviz_pca_var(res.pca, col.var = &quot;cos2&quot;, gradient.cols = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;), repel = TRUE # Évite le chevauchement de texte ) fviz_pca_ind(res.pca, col.ind = &quot;cos2&quot;, gradient.cols = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;), repel = TRUE # Évite le chevauchement de texte ) ### ce merveilleux bi plot fviz_pca_biplot(res.pca, col.ind = &quot;cos2&quot;, labelsize = 3, gradient.cols = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;), repel = TRUE )# Biplot des individus et variables 7.6 Une généralisation de lACP : lAFC LAFC trouve une application remarquable dans lanalyse de tableaux croisés. Elle est une méthode de réprésentation des profils lignes et colonnes: On saperçoit que deux analyses peuvent être menées : lune sur les colonnes, et lautres sur les lignes. Dans les deux cas cette analyse peut se faire en comparant les colonnes (lignes) selon la formule suivante \\[ d_{i,j}= (f_{.i}-f_{.j})^2 \\] Lidée maintenant est claire : on mène deux acp, en ligne et en colonne, et on projettent conjointement ( dans un même espace) library(readr) BDCOM_2020 &lt;- read_csv(&quot;Data//BDCOM/BDCOM_2020.csv&quot;) %&gt;%rename(CODACT=CODE_ACTIVITE) BDCOM_2017_CODACT_OD &lt;- read_delim(&quot;Data/BDCOM/BDCOM_2017_CODACT_OD.csv&quot;, delim = &quot;;&quot;, escape_double = FALSE, trim_ws = TRUE) df&lt;-BDCOM_2020%&gt;%left_join(BDCOM_2017_CODACT_OD, by = &quot;CODACT&quot;)%&gt;%rename(ACT=27) t&lt;-table(df$ARRONDISSEMENT,df$ACT ) res.ca &lt;- CA(t, graph = TRUE) plot(res.ca, autoLab = &quot;yes&quot;) fviz_ca_biplot(res.ca, labelsize = 2, repel=TRUE)+ theme(text = element_text(size =7)) +xlim(-0.75, 1)+ylim(-.75,0.75) règle dinterprétation le point (0,0) représente le baycentre du nuage de point,et donc linvidu moyens les lignes/colonne les plus extcentrée sont les moins présentes, la distance dune modalité dune variable à une autres, indique la correspondance de ces deux modalités qui partagent les mêmes individus. linertie total est chi²/n et donc une véritable méthode : analyse de la décomposition du khi2. Dans notre exemple on note de suite les arrondissement 1 et 2 qui sont les plus proches de la catégorie commerce de gros. On note aussi une disposition linéiaire qui opposent les arrondisssement excentré, aux arrondissement du centre. Un univers commercial résidentiel vs un univers de transit (spectacles et grands magasins) 7.6.1 AFCM multiple Très rapidement la méthode a été appliquée à une généralisation des tableaux croisés : le tableau de burt, ou son équivalent : le tableau disjonctif complet. exemple La mise en oeuvre par factominer permet demployer une techniques de représentation de variables complémentaires : elles ninterviennent pas dans le calcul de la configuration factorielles, mais leurs positions dans lespace sont calculées comme le barycentre des individus qui possède le trait considéré. Leur projection a un rôle illustratif. library(FactoMineR) table(df$ACT) ## ## Agences Alimentaire ## 4240 7663 ## Auto-Moto Autres locaux en boutique ## 823 9215 ## Bricolage-Jardinage Cafés et Restaurants ## 907 15247 ## Commerces de gros Culture et loisirs ## 1009 5289 ## Equipement de la maison Equipement de la personne ## 2666 7324 ## Grands magasins Hôtels et Auberges de jeunesse ## 9 1894 ## Locaux vacants Médical ## 8761 2094 ## Santé-Beauté Services aux entreprises ## 2906 535 ## Services aux particuliers Spectacles ## 12453 237 foo&lt;-df%&gt;% dplyr::select(ACT, SURFACE, SITUATION, LIBACT, ARRONDISSEMENT)%&gt;% as.matrix() res&lt;-MCA(foo,graph = FALSE,quali.sup=5) fviz_mca_var(res, labelsize = 2, repel=TRUE) remarques complémentaires : * pas de signification de linertie globale qui dépend de la structure du tableau ( nombres de variables et de leurs modalités) 7.7 Développements derrière les méthodes il y a un principe mathématique fondamental qui est au fondement de bien dautres méthodes factorielles. Cest celle de la Singular Variance décomposition dont lACP est finalement un cas particulier. 7.7.1 le SVD Le modèle mathématique fondamental décomposer une matrice en plusieurs matrices lacp une application à une matrice de nature particulières : la matrice de covariance ou de corrélation si standardisée de nombreuses autres applications : à des matrices de comptage compression dimage information retrieval dautres méthodes sappuient sur ce principe fondamental, et permettent de traiter des données textuelle . On repporte le lecteur au chapitre X de Booh NLP. LSA NFM 7.7.2 ACM , analyse canonique , analyse discriminante Si ACP, AFC et AFCM ont pris le devant de la scène, bien dautre méthodes analogues ont été développées ACM Analyse canonique Analyse factorielle discriminante qui a perdu du terrain au profit du modèle de régression logistique. "],["en-conclusion.html", "Chapitre 8 En conclusion", " Chapitre 8 En conclusion une idée essentielle : réduire de nombreuses variables à un petit jeu de variables synthétiques des méthode au coeur de lanalyse des données une autre idée essentielle : celle de vectoriser les données quon observe "],["clus.html", "Chapitre 9 Clustering 9.1 Les méthodes hiérarchiques ascendantes 9.2 segmentation simplifiée 9.3 tableaux croisés de la typologie et des critères sociaux démos 9.4 AFCM pour une synthèse 9.5 Les méthodes non-hiérarchiques 9.6 Autres méthodes 9.7 Conclusion", " Chapitre 9 Clustering Lobjectif des méthodes de classification automatique est de regrouper des observations qui se ressemblent sur un ensemble multidimensionnel de caractéristiques. insérer image Dans ce chapitre nous examinons deux familles de méthodes qui le distingue par la procédure de calcul : hierarchique dune part, non hiérarchique de lautre. On garde pour le chapître suivant létude des modèles de décisions qui ont une longue et riche histoire en marketing et ont préparé le développement de certains modèles de machine learning. 9.1 Les méthodes hiérarchiques ascendantes Elles trouvent leur origine en biologie où dès les années 1930 Sokal et Sneath(Sneath and Sokal 1973) ont proposé des méthodes pour analyser lévolution des espèces. Lidée réside dans la comparison de specimens sur la base dun certains nombre de caractéristiques, dabord des caractères phénotypiques, puis dans ce domaine en sappuyant sur les caractéristiques génétiques. Nous nentrerons pas dans une discussion plus approfondis mais signalons que ces choix déterminent des méthodes et des hypothèses très différentes et largement débattues (cladistique etc) Prenons le cas de différences phénotypiques et le tableau suivant. tableau Le but du jeu est de regrouper successivement les spécimens en fonction de leur ressemblance. Lalgorithme consiste simplement à 1) calculer toutes les ressemblances deux à deux et 2) à fondre en une classe les deux éléments qui se ressemble le plus. On réitère lopération jusquà ce quon obtienne plus quune classe. Le résultat est une arborescence dont chaque noeud représente un regrouppement de classe à un certain niveau de distance. figure Leurs variétés dépend de deux paramètres : le choix de la mesure de dissimilarités : Une distance euclidienne ? Son carré ? Une distance binaire comme lindice de Jaccard? le choix de la méthode dagrégation : que choisit-on pour calculer la distance entre deux classes A et B : la plus grande des distances entre les éléments de A et ceux de B ? La plus petite ? La distance moyennes, la médiane ? 9.1.1 Mise en oeuvre On utilise lenquête dhappydemics sur la période de fin mars. df&lt;-readRDS(&quot;./data/last.rds&quot;) %&gt;% filter(date2&gt;=make_datetime(year=2022, month=3, day = 19)) n_t&lt;-nrow(df) period&lt;-&quot; apres le 19 mars&quot; Il y a un trick de traitement des données. La question QCM a été encodée en une colonne, ajoutant les chaines de caractère des 16 thématiques avec un séparateurs $ . foo &lt;-as.data.frame(str_split_fixed(df$themes, &quot;\\\\$&quot;,n=3)) # On splite la colonne thème en autant de thème possibles foo1&lt;-cbind(df,foo)%&gt;% rename(V1=23, V2=24, V3=25) %&gt;% dplyr::select(id,V1,V2,V3)%&gt;% pivot_longer(!id,names_to=&quot;rank&quot;,values_to=&quot;theme&quot;)%&gt;% mutate(rank=ifelse(rank==&quot;V1&quot;, 3,ifelse(rank==&quot;V2&quot;, 2, ifelse(rank==&quot;V3&quot;,1, 0)))) %&gt;% #on recode les rangs par un facteur d&#39;importance de à 0 à 3 filter(theme!=&quot;&quot;)%&gt;% mutate(theme=str_trim(theme))%&gt;% mutate(r=as.numeric(rank))%&gt;% dplyr::select(-rank) n1&lt;-nrow(df) # le nombre d&#39;individus n2&lt;-nrow(foo1) #le nombre de mentions Dans une première étape faisons le bilan global #on calcule la proportion et la pénétration des items foo2 &lt;-foo1%&gt;% mutate(m=1)%&gt;% group_by(theme)%&gt;% summarise(frequence=sum(m), proportion=frequence/n2, penetration=frequence/n1) col&lt;-c(&quot;#F1BB7B&quot;, &quot;#FD6467&quot;, &quot;#FD6467&quot;, &quot;#FD6467&quot;, &quot;#5B1A18&quot;, &quot;#5B1A18&quot;, &quot;#5B1A18&quot;, &quot;#F1BB7B&quot;, &quot;#FD6467&quot;, &quot;#F1BB7B&quot;, &quot;#F1BB7B&quot;, &quot;#F1BB7B&quot;, &quot;#F1BB7B&quot;, &quot;#5B1A18&quot;, &quot;#FD6467&quot;, &quot;#F1BB7B&quot;, &quot;#F1BB7B&quot; ) brks&lt;-c(0.1, 0.2, 0.3,0.4,0.5,0.6) ggplot(foo2,aes(x=reorder(theme, frequence), y=penetration))+ geom_bar(stat=&quot;identity&quot;, aes(fill=theme))+ coord_flip()+ scale_fill_manual(values=col)+ labs(title = &quot;Pénétration des thèmes dans la population&quot;, x=NULL, y= &quot;% de la population&quot;, caption = &quot;data @happydemics dataviz @benavent&quot;)+ theme_minimal()+ theme(legend.position = &quot;none&quot;)+ scale_y_continuous(breaks = brks, labels = scales::percent(brks)) ggsave(paste0(&quot;./plot/theme_&quot;,period,&quot;.jpg&quot;),plot=last_plot(), width = 27, height = 17, units = &quot;cm&quot;) 9.2 segmentation simplifiée On commence va reconstruire un tableaux des individus x les thèmes. On garde les rangs comme indicateurs de limportance . foo3&lt;-foo1%&gt;% pivot_wider(names_from=&quot;theme&quot;, values_from=&quot;r&quot;) %&gt;% replace(is.na(.), 0) head(foo3, 8) ## # A tibble: 8 x 17 ## id `L&#39;immigration` `Le pouvoir d&#39;achat` `L&#39;éducation` `L&#39;égalité Homm~` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 235824277 3 0 0 0 ## 2 235824417 0 3 2 1 ## 3 235825307 0 3 0 0 ## 4 235825503 0 0 0 0 ## 5 235826269 0 1 0 0 ## 6 235828154 0 2 3 0 ## 7 235828507 0 0 3 2 ## 8 235828613 1 2 0 0 ## # ... with 12 more variables: `Les retraites` &lt;dbl&gt;, `L&#39;environnement` &lt;dbl&gt;, ## # `Le service public` &lt;dbl&gt;, `L&#39;insécurité` &lt;dbl&gt;, `La santé` &lt;dbl&gt;, ## # `Le chômage` &lt;dbl&gt;, `L&#39;économie` &lt;dbl&gt;, ## # `La science et la technologie` &lt;dbl&gt;, `L&#39;identité nationale` &lt;dbl&gt;, ## # `La sécurité nationale` &lt;dbl&gt;, `L&#39;union Européenne` &lt;dbl&gt;, ## # `La culture` &lt;dbl&gt; On calcule un tableau de distance et on performe la classification automatique. dans cet essai on tente un modèle à 8 groupes. foo4&lt;-foo3[,2:17] #distance d&lt;-dist(foo4) #clustering h.D &lt;- hclust(d, method=&quot;ward.D&quot;) #dendogramme plot(h.D, hang=-1) #identification des clusters rect.hclust(h.D , k = 8, border = 2:6) #attribution des clusters memb &lt;- cutree(h.D, k = 8) #maj du fichier de données avec l&#39;appartenace des individus aux groupes foo5&lt;-cbind(foo4, memb) Il reste à décrire les différents types sur les 16 variables qui les décrivent. On choisit une méthode de barre ordonnée avec un facetting par groupe. foo6&lt;-foo5 %&gt;% group_by(memb) %&gt;% pivot_longer(-memb,names_to=&quot;Thèmes&quot;,values_to=&quot;Valeurs&quot;)%&gt;% group_by(memb,Thèmes)%&gt;% summarise(Valeurs=mean(Valeurs)) foo6$group[foo6$memb==1]&lt;-&quot;multicritère&quot; foo6$group[foo6$memb==2]&lt;-&quot;Santé/Educ&quot; foo6$group[foo6$memb==3]&lt;-&quot;Pouvoir d&#39;achat/nretraites&quot; foo6$group[foo6$memb==5]&lt;-&quot;Immigration/nInsécurité &quot; foo6$group[foo6$memb==4]&lt;-&quot;égalité h/F&quot; foo6$group[foo6$memb==6]&lt;-&quot;Pouvoir d&#39;achat/nSanté&quot; foo6$group[foo6$memb==7]&lt;-&quot;Economie&quot; foo6$group[foo6$memb==8]&lt;-&quot;Environnement&quot; library(scales) brks&lt;-c(0.5,1,1.5,2, 2.5,3) p2&lt;- ggplot(foo6, aes(x=reorder(Thèmes, Valeurs), y=Valeurs))+ geom_bar(stat=&quot;identity&quot;,aes(fill=as.factor(Thèmes)))+ facet_wrap(vars(memb), ncol=4)+ coord_flip()+ scale_fill_manual(values=col)+ theme_minimal()+ scale_y_continuous(breaks=brks)+ theme(legend.position = &quot;none&quot;, axis.text=element_text(size=7),axis.text.x=element_text(angle = 45, vjust = 0.5, size=2))+ labs(title = &quot;Profils des segments\\npar importance des thématiques&quot;, x=NULL, y=&quot;importance moyenne (de 0 à 3)&quot;) p2 library(wesanderson) seg_col&lt;-wes_palette(&quot;Zissou1&quot;, 8, type = &quot;continuous&quot;) n&lt;-nrow(foo5) foo6&lt;-foo5 %&gt;% mutate(n=1) %&gt;% group_by(memb)%&gt;% summarise(freq=sum(n, na.rm=TRUE))%&gt;% mutate( freq=freq/n) foo6$group[foo6$memb==1]&lt;-&quot;multicritère&quot; foo6$group[foo6$memb==2]&lt;-&quot;Santé/Educ&quot; foo6$group[foo6$memb==3]&lt;-&quot;Pouvoir d&#39;achat/nretraites&quot; foo6$group[foo6$memb==5]&lt;-&quot;Immigration/nInsécurité &quot; foo6$group[foo6$memb==4]&lt;-&quot;égalité h/F&quot; foo6$group[foo6$memb==6]&lt;-&quot;Pouvoir d&#39;achat/nSanté&quot; foo6$group[foo6$memb==7]&lt;-&quot;Economie&quot; foo6$group[foo6$memb==8]&lt;-&quot;Environnement&quot; p1&lt;- ggplot(foo6, aes(x=group, y=freq))+ geom_bar(stat=&quot;identity&quot;, aes(fill=group))+ scale_fill_manual(values=seg_col) + theme_minimal()+ labs(title=&quot;Poids des segments&quot;, x=NULL, y=&quot;Proportion&quot;)+ scale_y_continuous(breaks=brks,labels=percent)+ theme(legend.position = &quot;none&quot;) plot_grid(p1, p2, labels = c(&#39;A&#39;, &#39;B&#39;), label_size = 12, ncol=1,rel_heights = c(1, 2)) ggsave(&quot;./plot/g_segment.jpg&quot;,plot=last_plot(), width = 27, height = 17, units = &quot;cm&quot;) 9.3 tableaux croisés de la typologie et des critères sociaux démos On revient à une approche descriptive, on croisant successivement notre variable typologie avec les critères socio-demo qui ont été mesurés dans lenquête. ( une boucle simplifierait ! ) df&lt;-cbind(df,foo5) df$group[df$memb==1]&lt;-&quot;multicritère&quot; df$group[df$memb==2]&lt;-&quot;Santé/Educ&quot; df$group[df$memb==3]&lt;-&quot;Pouvoir d&#39;achat/nretraites&quot; df$group[df$memb==5]&lt;-&quot;Immigration/nInsécurité &quot; df$group[df$memb==4]&lt;-&quot;égalité h/F&quot; df$group[df$memb==6]&lt;-&quot;Pouvoir d&#39;achat/nSanté&quot; df$group[df$memb==7]&lt;-&quot;Economie&quot; df$group[df$memb==8]&lt;-&quot;Environnement&quot; foo&lt;-df %&gt;% group_by(group, Sensibilité) %&gt;% summarize(n=n())%&gt;% mutate(prop=round(n/sum(n),3), cum=1 - (cumsum(prop)-prop/2)) g01&lt;-ggplot(foo,aes(x=group, y=prop, group=Sensibilité))+ geom_bar(stat=&quot;identity&quot;,aes(y = prop, fill=Sensibilité)) + scale_y_continuous(breaks = brks, labels = scales::percent(brks)) + scale_fill_manual(values=SensiP2) + geom_text(aes(label = prop, y=cum),size=2,color=&quot;white&quot;, vjust = 0.5)+ coord_flip()+ labs(title = &quot;Types d&#39;attentes par sensibilité politique &quot;, x=NULL, y=NULL,)+ theme_bw()+ theme(axis.text.x = element_text(size = 7), legend.text = element_text(size = 7)) ggsave(&quot;./plot/g_segment01.jpg&quot;,plot=last_plot(), width = 27, height = 17, units = &quot;cm&quot;) foo&lt;-df %&gt;% group_by(group, Age) %&gt;% summarize(n=n())%&gt;% mutate(prop=round(n/sum(n),3), cum=1 - (cumsum(prop)-prop/2)) g02&lt;-ggplot(foo,aes(x=group, y=prop, group=Age))+ geom_bar(stat=&quot;identity&quot;,aes(y = prop, fill=Age)) + scale_y_continuous(breaks = brks, labels = scales::percent(brks)) + scale_fill_brewer(palette=&quot;Spectral&quot;) + geom_text(aes(label = prop, y=cum),size=2,color=&quot;white&quot;, vjust = 0.5)+ coord_flip()+ labs(title = &quot;Types d&#39;attentes par classe d&#39;âge &quot;, x=NULL, y=NULL,)+theme_bw() + theme(axis.text.x = element_text(size = 7), legend.text = element_text(size = 7)) ggsave(&quot;./plot/g_segment02.jpg&quot;,plot=last_plot(), width = 27, height = 17, units = &quot;cm&quot;) foo&lt;-df %&gt;% group_by(group, Sexe) %&gt;% summarize(n=n())%&gt;% mutate(prop=round(n/sum(n),3), cum=1 - (cumsum(prop)-prop/2)) g03&lt;-ggplot(foo,aes(x=group, y=prop, group=Sexe))+ geom_bar(stat=&quot;identity&quot;,aes(y = prop, fill=Sexe)) + scale_y_continuous(breaks = brks, labels = scales::percent(brks)) + scale_fill_brewer(palette=&quot;Spectral&quot;) + geom_text(aes(label = prop, y=cum),size=2,color=&quot;white&quot;, vjust = 0.5)+ coord_flip()+ theme_bw()+ labs(title = &quot;Types d&#39;attentes par genre &quot;, x=NULL, y=NULL,)+ theme(axis.text.x = element_text(size = 7), legend.text = element_text(size = 7)) ggsave(&quot;./plot/g_segment03.jpg&quot;,plot=last_plot(), width = 27, height = 17, units = &quot;cm&quot;) foo&lt;-df %&gt;% group_by(group, Education) %&gt;% summarize(n=n())%&gt;% mutate(prop=round(n/sum(n),3), cum=1 - (cumsum(prop)-prop/2)) g04&lt;-ggplot(foo,aes(x=group, y=prop, group=Education))+ geom_bar(stat=&quot;identity&quot;,aes(y = prop, fill=Education)) + scale_y_continuous(breaks = brks, labels = scales::percent(brks)) + scale_fill_brewer(palette=&quot;Spectral&quot;) + geom_text(aes(label = prop, y=cum),size=1.5,color=&quot;white&quot;, vjust = 0.5)+ coord_flip()+theme_bw()+ labs(title = &quot;Types d&#39;attentes par niveau d&#39;éducation &quot;, x=NULL, y=NULL,)+ theme(axis.text.x = element_text(size = 7), legend.text = element_text(size = 7)) ggsave(&quot;./plot/g_segment04.jpg&quot;,plot=last_plot(), width = 27, height = 17, units = &quot;cm&quot;) plot_grid(g01, g02, g04,g03, labels = c(&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;), label_size = 11, ncol=2,rel_widths = c(3, 2)) ggsave(&quot;./plot/g_segment05.jpg&quot;,plot=last_plot(), width = 27, height = 17, units = &quot;cm&quot;) 9.4 AFCM pour une synthèse Cest le bon moment de donner une seconde illustration de lutilité de lAFCM. Pourquoi ne pas synthétiser en une carte lensemble des relations statistiques. library(FactoMineR) library(factoextra) X&lt;-df %&gt;% dplyr::select( group, Age, Sexe, Sensibilité, Situation2) res&lt;-MCA(X, graph =FALSE) foo&lt;-as.data.frame(res$var$coord) %&gt;% rownames_to_column(var=&quot;var&quot;)%&gt;% rename(dim1=2, dim2=3) %&gt;% add_rownames(var = &quot;rowname&quot;) foo$rowname&lt;-as.numeric(foo$rowname) foo&lt;-foo %&gt;% mutate(label=ifelse(rowname&lt;9, &quot;groupe&quot;, &quot;&quot;)) ggplot(foo, aes(x=dim1, y=dim2, label= var) )+ geom_point()+ geom_text(aes(label=var, color=label),size=3)+ theme_bw()+labs( title= &quot;AFCM&quot;) theme(legend.position = &quot;none&quot;) ## List of 1 ## $ legend.position: chr &quot;none&quot; ## - attr(*, &quot;class&quot;)= chr [1:2] &quot;theme&quot; &quot;gg&quot; ## - attr(*, &quot;complete&quot;)= logi FALSE ## - attr(*, &quot;validate&quot;)= logi TRUE 9.4.1 Forces et limites forces : graphiques, complète limites : petite population 9.5 Les méthodes non-hiérarchiques La première dentre elles est la méthode k-means dont le principe est très simple : plutôt que de calculer toutes les distances entre tous les objets, on va se concentrer sur les distances en k group supposés et les n individus. Lhyperparamètre est ici le nombre de groupes 9.5.1 principe 9.5.2 Application 9.5.3 Le problème de la détermination du nombre optimal de groupe méthode du coude méthode silhouette gap statistics 9.6 Autres méthodes de nombreuses variantes sont disponibles mediane kernel les méthodes fuzzy : lappartenance nest pas exclusive mais probabilistique les méthode de classes latentes les méthodes de densités sappuie sur lidée que la continuité dun groupe sexprime en terme s de densités ** paramètriques ** non - paramètriques http://www.sthda.com/english/wiki/wiki.php?id_contents=7940 : https://link.springer.com/referenceworkentry/10.1007%2F978-0-387-30164-8_211 9.7 Conclusion "]]
