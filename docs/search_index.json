[["index.html", "Introduction aux Data Sciences/nAvec r Chapitre 1 Avant propos 1.1 Plan du manuel 1.2 Les jeux de données 1.3 Le cadre technique et les packages utilisés", " Introduction aux Data Sciences/nAvec r Christophe Benavent - Université Paris Dauphine 2022-10-18 Chapitre 1 Avant propos Ce bookdown présent les éléments dun cours de data science avec r. Il est reproductible, on peut en cloner les éléments à partir du repository. Le texte est encore hasardeux mais les codes sont vérifiés. Il sera dynamique, modifié à mesure de nos cours, séminaires et ateliers. Lillustration de couverture représente lévolution de la longueur des films de la base Imbd et raconte en chiffres un aspect de lhistoire du cinéma. Jusquaux années 30, la longueur est hétérogène puis elle se se stabilise : les courts-métrages ont une durée de lordre de 15 mn qui se raccourcit avec les décennies, ce genre menace de disparaître dans les années 80 et reprend du poil de la bête dans les années 2000. Les films longs voient leur longueur saccroître et se stabiliser autour dun peu moins de 100 mn, soit une heure et quarante minutes. On observera enfin quau cours des années 1990 les films de taille intermédiaires réapparaissent. On devinera dans cette évolution lémergence de standards, ou de conventions. Dans ce graphique il y a tous les éléments des data sciences contemporaines : un jeu de données riche et systématique, un modèle statistique fondamental avec la notion de densité de probabilité, une mesure, un critère de comparaison. Les diagrammes ridges, cest ainsi quon les appele, sont inspirés de la pochette de lalbum Unknown Pleasures de Joy division sorti en pleine période New Wave, en 1979. Un article de Vice en rappele lorigine et le destin du graphisme quon connait mieux imprimé sur des t-shirt que dans les cours de statistiques. 1.1 Plan du manuel Cest un projet en cours, Le plan général projeté est le suivant. Certains chapitres sont publiés ( mêmes incomplets) dautres sont dans les limbes. On les ajoutera progressivement. 1 - Lenvironnement r x 2 - Installation et prise en main x 3 - Usage de ggplot - uni et bivarié x 4 - Usage de ggplot - multivarié x 5 - Tables avec flex 6 - Modèles factoriels (Psych) x 7 - AFC x 8 - MDS 9 - Clustering x 10 - Analyse de réseaux 11 - Analyse de variance et régression linéraire x 12 - Modèle linéaire généralisé x 13 - Modèles à décomposition derreur x 14 - Modèle déquations structurelles (Lavaan) 15 - Times series 16 - Analyse spatiale et géographique 17 - Machine learning x 1.2 Les jeux de données Au cours du développement, plusieurs cas pratiques - souvent réduit en volume pour rester exemplaire, seront employés. Les données sont partagées. Voici la présentation des sets de données utilisées dans le syllabus. Ils sont disponibles dans le répertoire ./data/ ESS : cest une très belle base de données de sociologie happydemics : observatoire de la présidentielle2022 NSPools Arpur : commerce de paris Botanic  1.3 Le cadre technique et les packages utilisés Ce syllabus est écrit en Markdown (allaire_rmarkdown_2021?) et avec le package Bookdown (R-bookdown?). Le code sappuie sur tidyverse et emploie largement les ressources de ggplot. Les packages seront introduits au fur et à mesure mais un voici la liste complète. options(tinytex.verbose = TRUE) knitr::opts_chunk$set(echo = TRUE, include=TRUE, cache=TRUE, message=FALSE, warning=FALSE) #boite à outils et dataviz library(tidyverse) # inclut ggplot pour la viz, readr et library(cowplot) #pour créer des graphiques composés library(ggridges) # le joy division touch library(ggmosaic) library(ggcorrplot) library(corrplot) #à supprimer library(ggthemes) library(colorspace) #pour les couleurs library(wesanderson) library(RColorBrewer) #networks library(igraph) library(ggraph) # Accéder aux données library(rtweet) # une interface efficace pour interroger l&#39;api de Twitter # NLP library(tokenizers) library(quanteda) library(quanteda.textstats) library(udpipe) #annotation syntaxique library(tidytext) library(cleanNLP) #annotation syntaxique #sentiment library(syuzhet) #analyse du sentimeent #mise en page des tableaux library(flextable) #statistiques et modèles library(lme4) #pour des modèles plus complexe que les mco library(jtools) #une série d&#39;utiltaire pour bien représenter les résultats library(interactions) #traitement des interactions library(nlme)#pour les hlm library(psych) #pour la psychometrie #ACP et AFCM library(&quot;FactoMineR&quot;) library(&quot;factoextra&quot;) #ML library(caret) #regression library(lme4) library(jtools) library(interactions) library(betareg) library(lavaan) # Utilitaires library(citr) #pour insérer des références dans le markdown #config plot theme_set(theme_minimal()) Lensemble du code est disponible sur github. A ce stade cest encore embryonnaire. Les proches et nos étudiants pourrons cependant y voir lévolution du projet et de la progression. Une version pdf est disponible ici. Quelques conventions décriture du code r On dénomme les data frames de manière générale df, les tableaux intermédiaires sont appelé systématiquement foo Gestion des palettes de couleurs ** une couleur : royalblue ** deux couleurs ** 3 à 7 couleurs On emploie autant que possible le dialecte tidy. Les chunks sont notés en 4 chiffres : 2 pour le chapitre et deux pour le chunck. 0502 est le second chunk du chapitre 5. On commente au maximum les lignes de code pour épargner le corps du texte et le rendre lisible "],["intro.html", "Chapitre 2 Introduction aux data sciences 2.1 Science, art, technique et pratiques 2.2 Une courte histoires des logiciels statistiques 2.3 Le processus de traitement des données 2.4 Les facteurs sociaux du développement des datasciences 2.5 Conclusion", " Chapitre 2 Introduction aux data sciences Lobjet du manuel est de donner un aperçu général des méthodes danalyses de données et de data sciences Mais avant de sengager dans les procédures une réflexion épistémologique et historique peut être utile. Si les méthodes sont puissantes, inventives, ils faut aussi sinterroger sur leur conditions démergences. La discipline fût la statistique, elle alimenta milles champs spécifiques : économétrie, psychométrie, biostatistiques. Derrières les problèmes lavance des mathématiques pour caractériser les modèles proposés. Elle sest laissée aller à dautres terminologies : analyse des données, data mining, Machien learning, deep learning. 2.1 Science, art, technique et pratiques Plutôt que le terme consacré de data sciences, il vaudrait mieux parler de data ingénierie dans la mesure où le data scientiste participe à un processus de production qui va de lacquisition des données à leur propagation dans lorganisation ou la société. La technique domine sur la science et lunité se trouve dans lintégration de ce processus. La révolution des données vient de linteropérabilité croissante de ces techniques et dune intégration qui fluidifie le passage dune étape à une autre. Standards et langages en sont les éléments clés. Du côté des sciences, ce dont bénéficie lunivers des data sciences, cest lhéritage de cultures statistiques foisonnantes qui après sêtre développées dans leur cocon disciplinaire, se retrouvent désormais rassemblées dans un même langage. Bien sur il y a de manière sous-jascente les mathématiques et les statistiques qui construisent les fondements des modèles et des techniques. Mais leur développement sest fait souvent quand le scientifique se retrouve face à un problème où une observation. Prenons le cas des psychologues qui ont inventé lanalyse factorielle dans le but de pouvoir tester certains de leurs concepts : un degré dintelligence, une personnalité, des attitudes. Ou celui des écologues qui souhaitent estimer une population de poisson dans une rivière, problème qui a donné naissance aux modèles de capture/recapture. On pourrait ajouter les géographes avec les modèles danalyse spatiale, les financiers face à la variabilité des cours des places boursières, etc. Celui des économètres est peut-être le plus évident. Les biostatisticiens sont des contributeurs importants. Ce que la technique apporte cest lintégration par un langage et donc un ensemble de conventions, incarnées par r et python, algorithmes, et de programmes qui ne sont plus spécifiques à un domaine, mais peuvent circuler de lun à lautre. Cest ainsi que le catalogues de toutes les techniques psychométriques devient accessible aux autres disciplines par le biais dun package en particulier, psych. De la même manière loutillage des linguiste devient accessible aux autres disciplines, pensons aux économiste qui intégrent dans le indicateurs des sources textuelle telle que lanalyse du sentiment. ( ref) Linteropérabilité apportée par ces langages ne se définit pas que par lalgorithme qui aurait été porté dun autre langage vers celui-ci (des cas de réécriture ?) mais aussi par des programme passerelle qui à partir de r permettent dactivité des algorithme écrit en C, en javascript ou tout autre langage plus informatique et souvent plus efficace. 2.2 Une courte histoires des logiciels statistiques Ce quon observe dans lévolution des logiciels 1980 : stat-itcf 1980 : SAS comme accès à r 1990 : SPSS stata 1997 : s dès 976 puis r, 1996 fre sofsware as r. le CRAN nait en 1997. 2003 création dez r foundation. 1991 - 2001: Python - Guido van Rossum - Python Software Foundation, créée en 2001 keras tensor flow http://www.deenov.com/blog-deenov/histoire-du-logiciel-spad.aspx Un des grand mouvement du domaine est lhésitation entre le programmatique et le no-code. La pression commerciale conduit certains acteurs à encourager lusage de sur-couche logiciel qui débarrasse lutilisateur de lexigence techniques, il peut se laisser guider par lintuition, mais laliène en dissimulant la mécanique profonde des processus de traitement des données. Le succès de python et de r réside dans La modularisation : langage de base /fonction/ package et notion de dépendance Linteropérabilité pas toujours parfaite ( versions, classes de données) La cumulativité : les fonction sajoutent aux fonctions, se sédimentent Laccès 2.3 Le processus de traitement des données Les data sciences ne sont finalement que lintégration dun flot de traitement des données qui va de lacquisition à la divulgation. Acquisition Codification , filtrage et correction derreur Structuration des données : api, open data Exploration Modélisation : validation : tests versus AB testing Simulation et décision Vizualisation et sensemaking Déploiement : Contrôle : Publication : dash board, pdf , slide etc, webb site 2.4 Les facteurs sociaux du développement des datasciences Ces développements sont favorisés par un environnement fertile dont quatre facteurs se renforcent mutuellement. La constitution dun système de communication commun organisé autour de peu de langage, et dun ensemble de normes de données mais aussi la vitalité dune communauté. La multiplicité des sources de données et lévolution des technologies de la mesure et du nombre constitue un second groupe de facteurs. 2.4.1 Une lingua franca La lingua franca est la langue des ports et du commerce de la méditerranée au XIVème siècle, un mélange de langage qui sert léchange, un commun pourrait-on dire aujourdhui. Cest ce que sont devenus python et r parmi dautres, la seconde langue après langlais qui sest imposé comme la langue décriture. Le langage des scientifiques est sans doute désormais un pidgin, un créole danglais, et de r ou de python, sans compter les sparc, les C+++ ou javascript. Les langues de la donnée se mêlent volontiers, elle sont de plus en plus agnostique. Lenvironnement r par exemple devient de plus en plus ouverts à python, à la fois de manière directe en permettant de coder dans un même document des calculs en r pluis en python, mais aussi de manière indirecte parce que prolifèrent des packages passerelles permettant daller chercher des ressources écrites dans un autre langage. 2.4.2 Une communauté Le second facteur, intimement lié au premier, est la constitution dune large communauté de développeurs et utilisateurs qui se retrouvent aujourdhui dans des plateformes de dépôts (Github, Gitlab), de plateformes de type quora (StalkOverFlow), de tutoriaux, de blogs (BloggeR), de journaux (Journal of Statistical Software) et de bookdown. Des ressources abondantes sont disponibles et facilitent la formation des chercheurs et des data scientists. Toutes les conditions sont réunies pour engendrer une effervescence créative. Cette communauté se reproduit à petite échelles dans les procédures de laboratoire et les conventions de travail en commun des chercheurs. Elle peut se développer autant verticalement quhorizontalement : des hubs qui concentrent lensemble des acteurs et des ressources, quun grand nombre de micro communauté focalisés sur des problèmes très locaux. 2.4.3 La multiplication des sources de données. Le troisième est la multiplication des sources de données et leur facilité daccès. Les données privées, et en particulier celles des réseaux sociaux, même si un péage doit être payé pour accéder aux APIs, popularisent le traitement de données massives. Le mouvement des données ouvertes (open data) proposent et facilitent accès à des milliers de corps de données : retards de la SNCF, grand débat, le formidable travail de lInsee, european survey etc. 2.4.4 de la statistique à lIA Le retour au boites noires dans les années 2000. Ce qui distingue les statistiques traditionnelles de lapproche machine learning réside dabord par une approche de la modélisation différente. Les modèles statistiques et économétriques considèrent une structure de relation, la spécification du modèle (ex : le modèle linéaire), mais aussi des modèles de distribution des erreurs qui définissent le cadre destimation. Lévaluation passe par le test des hypothèses sur les paramètres et par la qualité dajustement. Le machine learning, se concentre sur la valeur prédictive, et considère nimporte quelle spécification même si elle est peu intelligible et comprend de grandes quantités de paramètres sur lesquels aucun test nest produit. Les deux approches ont plutôt tendance à ce compléter, les première testant des théories, les secondes procurant aux première de nouvelles hypothèse par de nouvelle mesure. Pour en donner un exemple simple, lanalyse de sentiment emploie des modèles complexes pour le prédire avec le seul texte, lIA permet denrichir des données empiriques par exemple en testant en finance la relation de cet indicateurs aux prix de marché. Un autre exemple en marketing. Les méthodes disponibles se sont accumulées depuis ces dernières 20 années. faisons-en une courte liste. 1956 :perceptron 1963 : arbre de décision 2005 : CNN 2008 : lda topic 2013 : word2vec 2018 : transformers . KNN, SVM, rf et le retour des réseaux de neurones. 2.5 Conclusion Il ne reste plus quà soulever le capot et de mettre les mains dans le cambouis. Et à se rappeler que si la nécessité de se faire remarquer à conduit les acteurs du domaine à envisager des data sciences, que cest dabord un art décriture, et une pratique qui permet à leurs artisans de séchanger des secrets de fabrique. On remerciera tous ceux qui développent des Packages, nous aurons le point de vue de ceux qui les utilisent. Ce cours est aussi un livre de recette, celui dun chercheur en sciences sociales qui picore dans limmensité de la production pour trouver des procédures reproductibles par ses étudiants. "],["prise-en-main-de-r.html", "Chapitre 3 Prise en main de r 3.1 La convention du Rmarkdown 3.2 Lire les données 3.3 Dplyr pour manipuler les données", " Chapitre 3 Prise en main de r Pour démarrer : 1 - Télécharger et installer r sur le site du Comprehensive r Archive Network 2 - Télécharger et installer Rstudio.(version free) 3 - Dans le cadre de cet atelier, on adopte la méthode du rmarkdown. On recommande fortement de lire louvrage de référence, même si la prise en main est très rapide. 4 - Il est désormais indispensable dutiliser le package tidyverse et en particulier les fonctions de manipulation et de pipe (%&gt;%) fournies par dplyr. Ce sera donc le premier package à installer (attention, il appele de nombreuses dépendences, linstallation peut prendre plusieurs minutes ) 3.1 La convention du Rmarkdown Différentes manières dinteragir avec r sont possibles : la première est le mode console, pour de petite opérations et un utilisateur chevronné, celà peut être commode car rapide mais très rapidement on sera amené à enregistrer les opérations dans des scripts. Une idée novatrice a été dintégrer lensemble des élements dans un seul document : le script découpé en petits éléments : des chunks, le commentaire et lanalyse verbabe dans un format texte, et le résultat. Dans lunivers python il sagit des carnets Jupiter, pour r cest le rmarkdown. Cest un dialecte du markdown générique adapté au langage r. On recommande au lecteur den lire le manuel et de le garder dans ses onglets. Quelques éléments de base : un document markdown est composé de plusieurs éléments Yalm : dans cet entête les éléments essentiels sont définis et paramétrés Texte : il suit les conventions de mise en forme du html : des # pour les niveau de titres une syntaxe (x)[.xxx] pour des liens vers les URLS ou des images. Les chunks sont isolés par 3 tiks au début et à la fin. Résultats apparaissent sous les chunks après avoir été exécutés Ce document peut être publié sous différents formats : html, pdf ou même word. Il comprend les éléments suivants : Plan Texte Code Résultats Bibliographie Références Liens Images 3.2 Lire les données La première étape cest la lecture des données. On commence par lecture de fichiers locaux, dont les formats sont multiples : csv, tsv, xlsx, Spss, etc Pour chacun deux existe une fonction dédiée. Le package readr contribue à cette tâche pour les fichiers *.csv. df &lt;- read_csv(&quot;./Data/BXL_listings.csv&quot;) head(df,5) ## # A tibble: 5 x 16 ## id name host_id host_~1 neigh~2 neigh~3 latit~4 longi~5 room_~6 price ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2352 Triplex-2~ 2582 Oda NA Molenb~ 50.9 4.31 Entire~ 91 ## 2 2354 COURT/Lon~ 2582 Oda NA Molenb~ 50.9 4.31 Entire~ 74 ## 3 45145 B&amp;B Welco~ 199370 &lt;NA&gt; NA Bruxel~ 50.9 4.37 Hotel ~ 120 ## 4 48180 Top Apart~ 219560 Ahmet NA Woluwe~ 50.8 4.41 Entire~ 200 ## 5 52796 Bright ap~ 244722 Pierre NA Ixelles 50.8 4.36 Entire~ 74 ## # ... with 6 more variables: minimum_nights &lt;dbl&gt;, number_of_reviews &lt;dbl&gt;, ## # last_review &lt;date&gt;, reviews_per_month &lt;dbl&gt;, ## # calculated_host_listings_count &lt;dbl&gt;, availability_365 &lt;dbl&gt;, and ## # abbreviated variable names 1: host_name, 2: neighbourhood_group, ## # 3: neighbourhood, 4: latitude, 5: longitude, 6: room_type Il est possible aussi daccéder en direct aux données du web, cest bien utile pour sassurer que les données sont bien fraîches. Par exemple une connexion à Nsppolls qui propose une compilation de tous les sondages dintention de vote de la présidentielle 2022. df_pol &lt;- read_delim(&quot;https://raw.githubusercontent.com/nsppolls/nsppolls/master/presidentielle.csv&quot;, delim = &quot;,&quot;, escape_double = FALSE, trim_ws = TRUE) Bien dautre possibilités sont offertes, on pourra utiliser des API, des programmes de scrapping., ire en bouche des fichiers dans un répertoire, interroger des bases SQL des SGBD) ou dautres systèmes. 3.2.1 La diversité des formats Peu de formats échappent à r, ils peuvent faire appel à des packages spécifiques excell Json shape et autre données géographiques. les formats bibliographiques sont plus exotique : bib et ris les xml pourront donner des maux de têtes. 3.3 Dplyr pour manipuler les données Dès lors que les données sont chargées en mémoire il va souvent être nécessaire den travailler, laspect et la structure. Laspect concerne les formats et les significations, les recodages. La structure est relative à la forme des tableaux. Il faudra souvent traiter les données brutes pour proposer à nos modèles des structures appropriées. Dplyr est un des packages essentiels de la suite tidyverse. Il permet de manipuler aisément les données et mérite une étude approfondie. Un point de départ ou en français : dplyr. Deux idées sont au coeur de Dplyr dabord celle du pipe, ensuite celle du verbe. Dplyr encourage une approche processus et performative. 3.3.1 Des pipes %&gt;% Une grand part de lintérêt de dplyr est de reprendre un opérateur de magritr très utile : le pipe noté data %&gt;% f() %&gt;% g()... Celui ci permet de passer le résultat de lopération à gauche f() sur les données data, dans la fonction g() à droite. Un exemple simple : dans la ligne de code suivante, une première fonction lit le fichier CSV, et envoie le résultat de cette lecture dans une fonction graphique élémentaire: compter les occurrences des modalités de la variable room_type. g &lt;- read_csv(&quot;./Data/BXL_listings.csv&quot;) %&gt;% ggplot(aes(x=room_type))+ geom_bar()+ coord_flip()+ labs(x=NULL, y= &quot;Fréquence&quot;, title=&quot; Distribution des types de logement à Bruxelles en 2020&quot;) g 3.3.2 Des verbes Loriginalité de dplyrest de définir les fonctions comme des verbes. Chaque verbe désigne une action particulière. On va les examiner progressivement. * transformer une variable, * filtrer les observations selon un critère, * isoler des variables, * les grouper pour en calculer des résultats statistiques (somme, moyenne, variance, max min etc), * les déployer selon un format long ou les distribuer en différents critères, * les fusionner enfin. 3.3.2.1 Mutate En Français cest transformer. On modifie la valeur dune variable par une fonction plus ou moins complexe, éventuellement en ajoutant des conditions. Dans notre exemple, faisant au plus simple, puisque la distribution est asymétrique, une transformation du prix par les log10 peut donner des résultats intéressants. Et cest le cas, on retrouve une distribution qui semble être gaussienne. g &lt;- read_csv(&quot;./Data/BXL_listings.csv&quot;) %&gt;% mutate(price=log10(price))%&gt;% ggplot(aes(x=price))+ geom_histogram() g 3.3.2.2 Filter On peut vouloir se concentrer sur une sous population. Par exemple les chambres privées. g &lt;- read_csv(&quot;./Data/BXL_listings.csv&quot;) %&gt;% filter(room_type==&quot;Private room&quot; ) %&gt;% mutate(price=price)%&gt;% ggplot(aes(x=log10(price)))+ geom_histogram() g 3.3.2.3 select On peut sélectionner des colonnes pour créer un tableau spécifique. On en profite pour introduire flextable , une solution élégante pour éditer des tableaux en html. foo &lt;- read_csv(&quot;./Data/BXL_listings.csv&quot;) %&gt;% dplyr::select(room_type,price) ft &lt;- flextable(foo[ sample.int(10),])%&gt;% set_header_labels(room_type=&quot;Type de logement&quot;, price = &quot;Prix en euros&quot;)%&gt;% theme_vanilla()%&gt;% fontsize(size = 9)%&gt;% autofit() ft .tabwid table{ border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-spacing: 0; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-bad59688{border-collapse:collapse;}.cl-bac83236{font-family:'Arial';font-size:11pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-bac83237{font-family:'Arial';font-size:9pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-bac85932{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-bac85933{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-bac8ce08{width:90.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bac8ce09{width:114pt;background-color:transparent;vertical-align: middle;border-bottom: 0.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bac8ce0a{width:90.2pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bac8ce0b{width:114pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bac8ce0c{width:90.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bac8ce0d{width:114pt;background-color:transparent;vertical-align: middle;border-bottom: 0.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bac8ce0e{width:90.2pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bac8ce0f{width:114pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Type de logementPrix en eurosHotel room120Entire home/apt65Entire home/apt80Entire home/apt74Entire home/apt80Entire home/apt91Entire home/apt85Entire home/apt200Entire home/apt95Entire home/apt74 3.3.2.4 Group_by et summarize cest une opération clé, en groupant les observations selon les modalités dune variables, on peut construire des tableaux agrégés avec summarise qui permet de calculer de nombreuses statistiques : somme, moyenne, variance, max, min .. à travers les groupes. foo &lt;- read_csv(&quot;./Data/BXL_listings.csv&quot;)%&gt;% dplyr::select(neighbourhood, price)%&gt;% group_by(neighbourhood ) %&gt;% summarise(averageprice=round(mean(price),1), nombreoffre=n()) #mise en forme flextable ft &lt;- flextable(foo)%&gt;% set_header_labels(neighbourhood=&quot;Quartier&quot;, averageprice = &quot;Prix en euros&quot;, nombreoffre=&quot;Nombre d&#39;offre&quot;, size=9)%&gt;% fontsize(size = 9)%&gt;% theme_vanilla() ft .tabwid table{ border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-spacing: 0; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-bb493eee{border-collapse:collapse;}.cl-bb3ebed8{font-family:'Arial';font-size:11pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-bb3ebed9{font-family:'Arial';font-size:9pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-bb3ebeda{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-bb3ebedb{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-bb3f33ae{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bb3f33af{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bb3f33b0{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bb3f33b1{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bb3f33b2{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bb3f33b3{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bb3f33b4{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bb3f33b5{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} QuartierPrix en eurosNombre d'offreAnderlecht71.9232Auderghem66.377Berchem-Sainte-Agathe65.931Bruxelles91.01,759Etterbeek75.8296Evere70.041Forest64.9226Ganshoren50.521Ixelles81.5849Jette70.375Koekelberg70.737Molenbeek-Saint-Jean67.4179Saint-Gilles76.1589Saint-Josse-ten-Noode55.2136Schaerbeek61.9364Uccle75.5274Watermael-Boitsfort74.865Woluwe-Saint-Lambert62.5121Woluwe-Saint-Pierre111.081 3.3.2.5 Pivot_wider et pivot_longer Si pour lhabitué des feuilles de calculs, les données croisent des observations avec des variables, ce format nest pas le seul moyen de représenter des données, et pas forcément le meilleur. Une théorie des tidy data a été proposé par wickham : Un ensemble de données est une collection de valeurs, généralement des nombres (si elles sont quantitatives) ou des chaînes de caractères (si elles sont qualitatives). Les valeurs sont organisées de deux manières. Chaque valeur appartient à une variable et à une observation. Une variable contient toutes les valeurs qui mesurent le même attribut sous-jacent (comme la hauteur, la température, la durée) dans différentes unités. Une observation contient toutes les valeurs mesurées sur la même unité (comme une personne, ou un jour, ou une course) à travers les attributs. merge Pour passer dun tableau individu x variable à une structure ordonnée, la fonction pivot_longer est particulièrement appropriée. En voici lanatomie. Et un exemple numérique : foo &lt;- foo %&gt;% pivot_longer(-neighbourhood,names_to= &quot;Variables&quot;,values_to = &quot;Values&quot;) ggplot(foo, aes(x=neighbourhood, y=Values, group=Variables))+ geom_line()+facet_wrap(vars(Variables),scales=&quot;free&quot;)+ coord_flip() Lopération inverste est de partir dun tableau long vers un tableau large. On remarquera que lusage de cette fonction est nécessaire dans lemploi de ggplot qui suit la logique des tidy data, ou données ordonnées 3.3.3 Fusionner les données On sera souvent amené à construire des tableaux de données en les enrichissant par dautres tableaux et à fusionner les données. Le cas le plus simple est dajouter dautres observations à un fichier de données. On distingue deux cas : les deux tableaux concernent les mêmes individus classé dans le même ordre, seules les colonnes diffèrent. On utilisera la fonction cbind() si les variables sont identiques mais que les individus sont différents on peut concatène des données avec rbind() (Léquivalent de DPLYR est row_bind et column_bind) x1&lt;-as.data.frame(c(1,2,3,4,5)) %&gt;%rename(x=1) y&lt;-as.data.frame(c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;,&quot;e&quot;)) %&gt;%rename(y=1) z&lt;-cbind(x1,y) ft&lt;-flextable(z) ft .tabwid table{ border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-spacing: 0; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-bc1f973c{border-collapse:collapse;}.cl-bbcf8b3e{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-bbcfb230{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-bbcfb231{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-bbcfd922{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bbcfd923{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bbcfd924{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bbcfd925{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bbcfd926{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bbcfd927{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} xy1a2b3c4d5e x2&lt;-as.data.frame(c(9,8,7,6)) %&gt;%rename(x=1) w&lt;-rbind(x1,x2) ft&lt;-flextable(w) ft .tabwid table{ border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-spacing: 0; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-bc441d3c{border-collapse:collapse;}.cl-bc3b978e{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-bc3bbe80{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-bc3be57c{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bc3be57d{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bc3be57e{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} x123459876 mais très souvent on sera dans des cas différents et la fusion des données devra suivre des index merge Plusieurs types de fusion sont proposées. Mode de fusions genérale fusion à gauche fusion à droite (Voir ce cours)[https://coletl.github.io/tidy_intro/lessons/dplyr_join/dplyr_join.html] "],["analyses-univariées.html", "Chapitre 4 Analyses univariées 4.1 La grammaire des graphiques 4.2 Une étude de cas", " Chapitre 4 Analyses univariées Nous avons appris à lire des données, à les manipuler, nous avons le droit dêtre pressé de les représenter de manière immédiatement lisible, par des dataviz. On présente dabord rapidement le concept de grammaire des graphiques On se concentre ensuite sur un cas détude On décline. 4.1 La grammaire des graphiques Cest sans doute une des percées conceptuelles laplus intéressante des datasciences. La représentation graphhiques des données fait lobjet à la fois dune explosion créative mais aussi dune synthèse théorique. Cest lapport de la grammaire des graphiques. Ces outils sappuient sur lidée de grammaire des graphiques. En voici un clair résumé.En français il y a toujours le larmarange 4.1.1 Un modèle en couche Celle-ci met un ordre dans les éléments qui composent un graphique et les superpose. layers laesthetic definit les éléments que lon veut représenter : ce quon met en abscisse, ce quon met en ordonnné, les groupes que lon veut distinguer. la geométrie (geom_x)qui définit la forme de représentation les échelles (scale_x) Labelisation (labs) les templates le facetting ggplot est construit selon cette structure. Voici le book de référence, qui est au centre de ce cours. On aura besoin de manière assez systématique de manipuler les données avant de les représenter, dplyr nous permet de le faire aisément. 4.1.2 Une typologie des représentations Un point de départ fondamental est la gallery de ggplot,, elle présente de manière synthétique la plupart les types de figures qui peuvent être représentées, avec du code facilement reproductible. Une classification simple Analyse univariée Analyse bi variée Analyse multivariée ** les variables sont quantitatives : on analyse des matrices de corrélations ** les variables sont qualitatives : on analyse des tableaux croisés Analyse geospatiale Analyse de réseaux analyse darbres Diagramme de flux 4.1.3 Lesthétique Lart des couleurs tient dans les palettes on aimera celles de Wes Anderson, on peut adorer fishualize. on trouvera 4.2 Une étude de cas Les données sont extraites de lESS, une sélection est disponible ici. Elle couvre les 9 vagues et concernent la France et LAllemagne. Les variables dépendantes (celles que lon veut étudier et expliquer) sont les 9 items de la confiance, les variable considérées comme indépendantes (ou explicatives) sont une sélection de variables socio-démographiques : âge, genre, perception du pouvoir dachat, orientation politique, type dhabitat. On fait quelques opérations de recodage et on renomme les variables avoir une lecture plus aisée des variables et de leurs catégories. Le plan de recodage dun jeu de données quon va employer dans les chapitres suivants. Il sappuie sur le langage de base. Lanalyse univarié, comme son nom lindique, ne sintéresse quà une seule variable. Celle-ci peut être quantitative ou qualitative etne comporter quun nombre limité de modalités entre lesquels aucune comparaison de grandeur ne peut être faite. Les premières ont le plus souvent dans r un format numeric, les autres correspondent au format factor. (Un exercice peut être de le réécrire avec dplyr.) df&lt;-readRDS(&quot;./data/trustFrAll.rds&quot;) #quelques recodages #on renomme pour plus de clarte names(df)[names(df)==&quot;trstun&quot;] &lt;- &quot;NationsUnies&quot; names(df)[names(df)==&quot;trstep&quot;] &lt;- &quot;ParlementEurop&quot; names(df)[names(df)==&quot;trstlgl&quot;] &lt;- &quot;Justice&quot; names(df)[names(df)==&quot;trstplc&quot;] &lt;- &quot;Police&quot; names(df)[names(df)==&quot;trstplt&quot;] &lt;- &quot;Politiques&quot; names(df)[names(df)==&quot;trstprl&quot;] &lt;-&quot;Parlement&quot; names(df)[names(df)==&quot;trstprt&quot;] &lt;- &quot;Partis&quot; names(df)[names(df)==&quot;pplhlp&quot;] &lt;- &quot;help&quot; names(df)[names(df)==&quot;pplfair&quot;] &lt;- &quot;fair&quot; names(df)[names(df)==&quot;ppltrst&quot;] &lt;- &quot;trust&quot; #on construit les scores de confiance df&lt;-df %&gt;% mutate(trust_institut=(Partis+Parlement+Politiques+Police+Justice+NationsUnies+ParlementEurop)*10/7,trust_interpersonnel=(help+fair+trust)*10/3) df$Year&lt;-2000 #recodage des variables independantes df$Year[df$essround==1]&lt;-2002 df$Year[df$essround==2]&lt;-2004 df$Year[df$essround==3]&lt;-2006 df$Year[df$essround==4]&lt;-2008 df$Year[df$essround==5]&lt;-2010 df$Year[df$essround==6]&lt;-2012 df$Year[df$essround==7]&lt;-2014 df$Year[df$essround==8]&lt;-2016 df$Year[df$essround==9]&lt;-2018 df$Year&lt;-as.factor(df$Year) df$OP&lt;-&quot; &quot; #ggplot(df,aes(x=lrscale))+geom_histogram() df$OP[df$lrscale==0] &lt;- &quot;Extrême gauche&quot; df$OP[df$lrscale==1] &lt;- &quot;Gauche&quot; df$OP[df$lrscale==2] &lt;- &quot;Gauche&quot; df$OP[df$lrscale==3] &lt;- &quot;Centre Gauche&quot; df$OP[df$lrscale==4] &lt;- &quot;Centre Gauche&quot; df$OP[df$lrscale==5] &lt;- &quot;Ni G ni D&quot; df$OP[df$lrscale==6] &lt;- &quot;Centre Droit&quot; df$OP[df$lrscale==7] &lt;- &quot;Centre Droit&quot; df$OP[df$lrscale==8] &lt;- &quot;Droite&quot; df$OP[df$lrscale==9] &lt;- &quot;Droite&quot; df$OP[df$lrscale==10] &lt;- &quot;Extrême droite&quot; #la ligne suivante est pour ordonner les modalités de la variables df$OP&lt;-factor(df$OP,levels=c(&quot;Extrême droite&quot;,&quot;Droite&quot;,&quot;Centre Droit&quot;,&quot;Ni G ni D&quot;,&quot;Centre Gauche&quot;,&quot;Gauche&quot;,&quot;Extrême gauche&quot;)) df$revenu&lt;-&quot; &quot; df$revenu[df$hincfel&gt;4] &lt;- NA df$revenu[df$hincfel==1] &lt;- &quot;Vie confortable&quot; df$revenu[df$hincfel==2] &lt;- &quot;Se débrouille avec son revenu&quot; df$revenu[df$hincfel==3] &lt;- &quot;Revenu insuffisant&quot; df$revenu[df$hincfel==4] &lt;- &quot;Revenu très insuffisant&quot; df$revenu&lt;-factor(df$revenu,levels=c(&quot;Vie confortable&quot;,&quot;Se débrouille avec son revenu&quot;,&quot;Revenu insuffisant&quot;,&quot;Revenu très insuffisant&quot;)) df$habitat&lt;-&quot; &quot; df$habitat[df$domicil==1]&lt;- &quot;Big city&quot; df$habitat[df$domicil==2]&lt;-&quot;Suburbs&quot; df$habitat[df$domicil==3]&lt;-&quot;Town&quot; df$habitat[df$domicil==4]&lt;-&quot;Village&quot; df$habitat[df$domicil==5]&lt;-&quot;Countryside&quot; df$habitat&lt;-factor(df$habitat,levels=c(&quot;Big city&quot;,&quot;Suburbs&quot;,&quot;Town&quot;,&quot;Village&quot;,&quot;Countryside&quot;)) df$genre&lt;-&quot; &quot; df$genre[df$gndr==1]&lt;-&quot;H&quot; df$genre[df$gndr==2]&lt;-&quot;F&quot; df$age&lt;-&quot; &quot; df$age[df$agea&lt;26]&lt;-&quot;25&lt;&quot; df$age[df$agea&gt;25 &amp; df$agea&lt;36]&lt;-&quot;26-35&quot; df$age[df$agea&gt;35 &amp; df$agea&lt;46]&lt;-&quot;36-45&quot; df$age[df$agea&gt;45 &amp; df$agea&lt;66]&lt;-&quot;46-65&quot; df$age[df$agea&gt;65 &amp; df$agea&lt;76]&lt;-&quot;66-75&quot; df$age[df$agea&gt;75]&lt;-&quot;75&gt;&quot; df$age&lt;-factor(df$age,levels=c(&quot;25&lt;&quot;,&quot;26-35&quot;,&quot;36-45&quot;,&quot;46-65&quot;,&quot;66-75&quot;, &quot;75&gt;&quot;)) saveRDS(df, &quot;./data/dfTrust.rds)&quot;) foo&lt;-df%&gt;% dplyr::select(Year,cntry, trust_institut, trust_interpersonnel)%&gt;% group_by(Year,cntry)%&gt;% summarise(trust_institut=mean(trust_institut, na.rm=TRUE), trust_interpersonnel=mean(trust_interpersonnel, na.rm=TRUE)) foo$Year&lt;- as.character(foo$Year) foo$cntry&lt;- as.character(foo$cntry) foo&lt;-foo%&gt;%pivot_longer(!c(Year,cntry),names_to=&quot;Trust&quot;, values_to=&quot;value&quot; ) ggplot(foo,aes(x=Year, y=value, group=Trust))+ geom_line(stat=&quot;identity&quot;,aes(color=Trust), size=1.2)+ facet_wrap(vars(cntry))+ scale_color_manual(values = c(&quot;Cyan3&quot;,&quot;Orange2&quot;))+ theme( legend.position = &quot;bottom&quot;, legend.justification = c(&quot;right&quot;, &quot;top&quot;), legend.box.just = &quot;right&quot;, legend.margin = margin(6, 6, 6, 6) )+ labs(x=NULL, y=&quot;Niveau&quot;)+ylim(40,60) 4.2.1 Le cas des variables quantitatives Les variables quantitatives décrivent une variable dont les valeurs décrivent les quantités dune grandeur. Elle peuvent être discrètes (dénombrement du dun nombre dunités) - le nombre dhabitant), ou continue (le nombre de km parcourus). lhistogramme est loutil de base pour représenter la distribution dune telle variable. Il représente pour des intervalles de valeurs donnés, la fréquence des observations. Sa syntaxe simple comporte dabord la définition de la variable et de la source de données, puis une des géométrie de ggplot : la fonction geom_histogram. Dans notre exemple, on va représenter le score de confiance institutionnelle pour la France en se concentrant sur la dernière vague denquête. #On charge le fichier recodé à la fin du chapitre précédent df&lt;-readRDS(&quot;./data/dfTrust.rds)&quot;) #filtrage sur 2018 et la France. foo&lt;-df%&gt;% filter(Year==&quot;2018&quot; &amp; cntry==&quot;FR&quot; &amp; !is.na(trust_institut)) # on stocke le diagramme dans l&#39;objet g00, pour le réutiliser ultérieurement et pouvoir le compléter. g00&lt;-ggplot(foo,aes(x=trust_institut))+ geom_histogram() g00 g00+labs(title=&quot;Distribution de la confiance institutionnelle en France et en 2018&quot;, x=&quot;Confiance institutionnelle&quot;) On va améliorer laspect en modifiant la couleur et la largeur des barres, ajoutant un thème, en précisant les éléments textuels (titres, label) en calculant et en représentant la valeur moyenne et lécart-type . Pour ces statistiques, on emploie les fonction de base : mean, sd et round. On notera que le titre est défini par la concaténation de plusieurs chaines de caractères avec la fonction paste0. On peut ainsi injecter dans le graphique des éléments externes au jeu de données. #on calcule la moyenne moy=mean(foo$trust_institut, na.rm=TRUE) sd=sd(foo$trust_institut, na.rm=TRUE) #avec tous les éléments g01 &lt;-ggplot(foo,aes(x=trust_institut))+ geom_histogram(binwidth=5,fill=&quot;pink&quot;)+ labs(title= &quot;Distribution de la confiance institutionnelle&quot;, subtitle= paste0(&quot;moyenne = &quot;,round(moy,2), &quot; ecart-type = &quot;, round(sd,2)), caption=&quot;ESS2002-2018&quot;, y= &quot;frequence&quot;, x=&quot;confiance (index de 0 à 100)&quot;)+ geom_vline(xintercept=moy, color=&quot;red&quot;,size=1.5)+ geom_segment(y = 0, yend=0,x=moy-sd,xend=moy+sd, color=&quot;orange&quot;,size=1.5) g01 Diagramme de densité : Au lieu de représenter les effectifs, on ramène leffectif total à 1. g04&lt;-ggplot(foo,aes(x=trust_institut))+ geom_density(fill=&quot;pink2&quot;) + labs(title= &quot;Fonction de densité de probabilité&quot;, caption=&quot;ESS2002-2018&quot;,y= &quot;frequence&quot;,x=&quot;Confiance (index de 1 à 100)&quot;) g04 enfin on peut examiner par rapport à une distribution théorique, en loccurrence une distribution gaussienne, ou normale, de paramètres égaux à la moyenne et la variance empirique de la distribution. Lajustement est convenable même si on observe une déviation sur la droite. Cest pourquoi on calcule aussi la Kurtosis et le skewness de la distribution. #On a déjà calculé la moyenne : mean #il nous manque l&#39;écart-type et sd&lt;-sd(foo$trust_institut, na.rm=TRUE) library(moments) sk&lt;-skewness(foo$trust_institut) ks&lt;-kurtosis(foo$trust_institut) g05&lt;-ggplot(foo,aes(x=trust_institut))+ labs(title= &quot;Distribution de la confiance institutionnelle&quot;, caption=&quot;ESS2002-2018&quot;,y= &quot;frequence&quot;,x=&quot;confiance (index de 0 à 100)&quot;) + geom_density(fill=&quot;pink2&quot;)+ stat_function(fun = dnorm,color=&quot;red&quot;,size=1.2, args = list(mean =moy, sd=sd)) g05 Un grand classique du test de normalité dune distribution est le diagramme QQ g06 &lt;- ggplot(foo, aes(sample = trust_institut)) + stat_qq() + stat_qq_line()+ labs(title= &quot;QQplot confiance interpersonnelle&quot;, caption=&quot;ESS2002-2018&quot;,y= &quot;Echantillon&quot;,x=&quot;Théorique&quot;) g06 On fini cette étude détaillée par lajustement dabord dun modèle (loi normale) aux données. Ensuite dun modèle de mélange ( Mixture model) par lequel on défiit la loi de distribution sous jascente, comme un mélange entre deux populations normale de paramètres distincts. https://tinyheero.github.io/2015/10/13/mixture-model.html df0&lt;-df %&gt;% na.omit() library(MASS) fit&lt;-fitdistr(df0$trust_interpersonnel,&quot;normal&quot;) fit ## mean sd ## 52.48548790 16.57617220 ## ( 0.09344363) ( 0.06607462) g07&lt;- g05+stat_function(fun = dnorm ,color=&quot;orange&quot;,size=1.2, args = list( mean=52.48, sd=16.57)) g07 library(mixtools) trust = foo$trust_institut mixmdl = normalmixEM(trust, k=2) ## number of iterations= 552 mixmdl$mu ## [1] 39.23072 56.04167 mixmdl$sigma ## [1] 17.90246 10.56092 mixmdl$lambda ## [1] 0.6719504 0.3280496 plot(mixmdl,which=2) lines(density(trust), lty=2, lwd=2) Finalement si notre distribution est univariée, car nétudiant quune variable, on peut quand distinguer deux population distinctes. 4.2.1.1 Dautres méthodes Il ny a pas que lhistogram ou le diagramme de densité, dautres méthodes sont utiles, surtout quand on veut comparer des groupes ( ce sera lobjet du prochain chapitre). Il sagit du diagramme à moustache et du diagramme en violon. g0306 &lt;- ggplot(foo, aes(y = trust_institut, x=1)) + geom_boxplot(fill=&quot;Grey&quot;) g0307 &lt;- ggplot(foo, aes(x=1,y = trust_institut)) + geom_violin(fill=&quot;Gold&quot;) + labs(x=&quot;density&quot;) plot_grid(g0306, g0307, labels = c(&quot;Boxplot&quot;,&quot;Violin plot&quot;), label_size = 12 ) 4.2.2 Quand la variable est qualitative Quand la variable est qualitative, que ses variables sont discrètes, la manière de représenter la plus commune est le fameux camembert que les experts écartent. Un diagramme en barre représente mieux les proportions. Un premier exemple pour représenter les vagues denquêtes g08&lt;-ggplot(df,aes(x=age))+ geom_bar(fill=&quot;skyblue&quot;)+ labs(title= &quot;Distribution par classe d&#39;âge&quot;, caption=&quot;ESS2002-2018&quot;,y= &quot;frequence&quot;,x=&quot;Vague d&#39;enquête&quot;) g08 Avec quelques améliorations : contôle de la couleurs des barres, ajout des % et pivot pour une meilleure lecture. foo&lt;-df %&gt;% filter(!is.na(age)) g10&lt;-ggplot(foo,aes(x=age, y = prop.table(stat(count)),label = scales::percent(prop.table(stat(count)))))+ geom_bar(aes(fill = age)) + coord_flip()+ labs(title= &quot;Répartition de la population par classe d&#39;âge&quot;, caption=&quot;ESS2002-2018&quot;,y= &quot;%&quot;,x=&quot;classes d&#39;age&quot;) + scale_y_continuous(labels = scales::percent)+ #contrôle de l&#39;échelle des % et du format scale_fill_brewer()+ geom_text(stat = &#39;count&#39;,position = position_dodge(.9), hjust = 1, size = 3) g10 si on tient au diagramme en secteur foo&lt;-df %&gt;%filter(!is.na(age)) g10&lt;-ggplot(foo,aes(x=&quot;&quot;, y = prop.table(stat(count)),label = scales::percent(prop.table(stat(count)))))+ geom_bar(aes(fill = age)) + labs(title= &quot;Répartition de la population par classe d&#39;âge&quot;, caption=&quot;ESS2002-2018&quot;,y= &quot;%&quot;,x=&quot;classes d&#39;age&quot;) + geom_text(stat = &#39;count&#39;,position = position_dodge(.9), hjust = 1, size = 3) + coord_polar(&quot;y&quot;, start=0) g10 https://cran.r-project.org/web/packages/treemapify/vignettes/introduction-to-treemapify.html si on tient au diagramme en cercle, autant opter pour un treemap avec la bibliothèque treemapifi library(treemapify) tree1&lt;-df %&gt;% mutate(n=1)%&gt;%group_by(age) %&gt;% summarize(n=sum(n)) %&gt;% filter(!is.na(age)) g11 &lt;- ggplot(tree1, aes(area = n, fill=n),label=age) + geom_treemap() + geom_treemap_text(aes(label=age),colour = &quot;white&quot;, place = &quot;centre&quot;,grow = FALSE)+ labs(title= &quot;Répartition de la population par classe d&#39;âge&quot;, caption=&quot;ESS2002-2018&quot;,y= NULL,x=NULL) g11 "],["analyse-bi-variée.html", "Chapitre 5 Analyse bi variée 5.1 Diagrammes xy - la magie des corrélations 5.2 Comparer les distributions et des moyennes", " Chapitre 5 Analyse bi variée Comme son nom lindique, il sagit dexaminer la relation entre deux variables et détudier leur distribution conjointe. On distinguera 3 situations et on examinera pour chaune les modes de représentations graphiques ainsi que les tests associés qui permette de sassurer que la relation apparente est effective. Deux variables quantitatives : scatterplot et corélations deux variable qualitatives : tableau croisé et test du chi2 une variable quanti et une variable quali. Compariaons de moyennes et ANOVA par comparer des distribution de plusieurs groupes (variables catégorielles) par comparer des moyennes dune variable dépendante en fonction de plusieurs variables indépendantes catégorielle mesurer lassociation entre deux variables qualitatives 5.1 Diagrammes xy - la magie des corrélations Venons en à analyser les relations entre deux variables quantitatives. foo&lt;-df %&gt;% filter(cntry==&quot;FR&quot; &amp; Year==&quot;2018&quot;) #selection de l&#39;echantillon g31&lt;- ggplot(foo, aes(x= trust_interpersonnel,y=trust_institut)) + geom_point( size=0.1) g31 Ce graphe est peu clair, il y a trop de points qui prennent des valeurs discrètes. Une astuce est de donner une position aléatoire pour sur disperser, on fait mieux apparaitre la densité de points. On ajoute la représentation de deux courbe dajustement, lune linéraire et lautre non linéaires. Mais en attendant en voici un calcul élémentaire. le calcul de la variance \\[{SS}_{xx} = \\sum (x - \\bar{x})^2 = \\sum x^2 - \\frac {(\\sum x)^2}{n}\\] le calcul de la covariance \\[{SS}_{xy} = \\sum (x - \\bar{x})(y - \\bar{y}) = \\sum xy - \\frac {(\\sum x)(\\sum y)}{n}\\] et la corrélation qui est le rapport de la covariance sur la racine carrée du produit des variances de x et y. \\[r = \\frac {{SS}_{xy}}{\\sqrt {{SS}_{xx}{SS}_{yy}}}\\] La corrélation est de lordre dun peu plus de 0,40 ce qui est assez élevé mais laisse une certaine indépendance des variables. Elle désignent des objets liés mais distinct. On peut tester lhypothèse quen réalité cette corrélation est nulle. Le test conduit au rejet de lhypothèse nulle de manière très nette, compte-tenu de léchantillon lintervalle de confiance est compris entre 0.36 et 0.44. #psych r&lt;-cor.test(foo$trust_interpersonnel, foo$trust_institut) #le test vient du package psych r ## ## Pearson&#39;s product-moment correlation ## ## data: foo$trust_interpersonnel and foo$trust_institut ## t = 18.861, df = 1821, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.3651235 0.4419644 ## sample estimates: ## cor ## 0.404257 rp&lt;-round(r$estimate,3) rp ## cor ## 0.404 Améliorons le graphe On peut souhaiter ajouter une droite des moindre carrés (calculée pour chaque vague denquête pour évaluer la stabilité de la relation dans le temps). Les lignes sont parallèles, la corrélation ne change pas dans le temps, cest une relation stable. Les deux formes de confiance vont dans le meme sens. On verra dans un autre chapitre comment calculer ces droites de corrélations. library(ggExtra) g32&lt;-ggplot(foo, aes(x= trust_interpersonnel,y=trust_institut)) + geom_point(position = &quot;jitter&quot;, size=0.1, color=&quot;grey&quot;)+ geom_smooth(method=&quot;lm&quot;, se=TRUE) + geom_smooth(method=&quot;gam&quot;,color=&quot;red&quot;) + labs(title = &quot;Relation entre confiance \\ninstitutionnelle et interpersonnelle&quot;, subtitle = paste(&quot;r de pearson: &quot;,rp ), x= &quot;Confiance interpersonnelle&quot;, y=&quot; Confiance institutionnelle&quot;) ggMarginal(g32 ,type = &quot;density&quot;, fill = &quot;Royalblue1&quot;, alpha=.5) Une autre façon de représenter est celle de carte de densité de probabilité. g32&lt;-ggplot(foo, aes(x= trust_interpersonnel,y=trust_institut)) + geom_point(position = &quot;jitter&quot;, size=0.1, color=&quot;grey&quot;)+geom_density2d()+ labs(title = &quot;Relation entre confiance institutionnelle et interpersonnelles&quot;, subtitle = paste(&quot;r de pearson: &quot;,rp )) g33&lt;-ggplot(foo, aes(x= trust_interpersonnel,y=trust_institut)) + geom_density2d_filled(aes(fill = ..level.., color = ..level..), contour_var = &quot;density&quot;)+ labs(title = &quot;Relation entre confiance institutionnelle et interpersonnelles&quot;, subtitle = paste(&quot;r de pearson: &quot;,rp ))+theme(legend.position = &quot;none&quot;) plot_grid(g32, g33, labels = c(&#39;A&#39;, &#39;B&#39;), label_size = 12) 5.2 Comparer les distributions et des moyennes Dans notre base on a pris les données de lAllemagne et de la France. On va comparer leur distribution. Et tant quà faire, puisque quon a deux variables, on va faire deux comparaisons : par pays et par type de confiance. A cette fin, nous construisons un tableau de données spécifique. #on recode en facteur la variable foo &lt;- df %&gt;% dplyr::select(cntry,trust_institut, Year,trust_interpersonnel) %&gt;% filter( Year==&quot;2018&quot;) %&gt;% dplyr::select(-Year)%&gt;% drop_na() %&gt;% gather(variable, value, -cntry) #attention plutôt utiliser pivot_longer head(foo) ## # A tibble: 6 x 3 ## cntry variable value ## &lt;chr+lbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 DE [Germany] trust_institut 58.6 ## 2 DE [Germany] trust_institut 65.7 ## 3 DE [Germany] trust_institut 58.6 ## 4 DE [Germany] trust_institut 65.7 ## 5 DE [Germany] trust_institut 48.6 ## 6 DE [Germany] trust_institut 37.1 Pour la représentation, en plus de la représentation en terme de densité, on va choisir une méthode de violon et de boxplot. On utilise une couche de facetting pour éclater ainsi la distribution des deux variables selon un critère de pays. #on peut utiliser &quot;facet&quot; g20&lt;-ggplot(foo,aes(x=value))+ geom_density(binwidth=10, fill=&quot;pink&quot;)+ facet_grid(cntry~variable)+ labs(title= &quot;Confiance institutionnnelle&quot;, caption=&quot;ESS2002-2018&quot;,y= &quot;frequence&quot;,x=&quot;Confiance&quot;) g20 g21&lt;-ggplot(foo,aes(x=variable, y=value))+ geom_violin( fill=&quot;pink&quot;) + geom_boxplot(width=0.1)+ facet_grid(cntry~.)+ labs(title= &quot;Confiance institutionnnelle&quot;, caption=&quot;ESS2002-2018&quot;,y= &quot;frequence&quot;,x=&quot;Confiance&quot;) g21 5.2.1 Comparaison de moyennes Comparer des distributions est une étape initiale nécesséaire, mais en général on sera plutôt intéresser de comparer des moyennes. Par exemple, on souhaiterais savoir si les degrés de confiances institutionnnelle et interpersonnelles varient en France selon les situations de revenu. Calculons dabord ces moyennes avec la fonction group_by et summarise. df_wave&lt;-df %&gt;% filter(cntry==&quot;FR&quot; &amp; Year==&quot;2018&quot;) %&gt;% group_by(revenu) %&gt;% summarise(trust_interpersonnel=mean(trust_interpersonnel, na.rm=TRUE), trust_institut =mean(trust_institut, na.rm=TRUE)) %&gt;% filter(!is.na(revenu)) %&gt;% #filtrer les valeurs manquantes gather(variable, value, -revenu) #fichier long ( pivot longer is better) head(df_wave) ## # A tibble: 6 x 3 ## revenu variable value ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Vie confortable trust_interpersonnel 55.6 ## 2 Se débrouille avec son revenu trust_interpersonnel 51.7 ## 3 Revenu insuffisant trust_interpersonnel 46.7 ## 4 Revenu très insuffisant trust_interpersonnel 41.4 ## 5 Vie confortable trust_institut 50.2 ## 6 Se débrouille avec son revenu trust_institut 44.1 Représentons ces moyennes graphiquement avec un geom_bar. g06a&lt;-ggplot(df_wave,aes(x=revenu,y=value, group=variable))+ geom_bar(stat=&quot;identity&quot;,aes(fill=variable), position =position_dodge())+ #dodge pour mettre les barres l&#39;une à côté de l&#39;autre labs(title= &quot;Confiance institutionnnelle&quot;, caption=&quot;ESS2002-2018&quot;,y= &quot;frequence&quot;,x=&quot;Niveau de Confiance&quot;)+ coord_flip() g06a On a une solution mais pas la meilleure, on perd lidée de variance et ce serait bien dajouter des barres dintervalle de confiances , un diagramme en lignes serait plus élégant. On en profite pour corriger laspect des labels peu lisibles en les inclinants, et à choisir une échelle qui omettent les valeur supérieur à 70 et inférieure à 30 pour donner une vision plus respectueuses de la totalité de léchelle qui va de 0 à 100. Au passage on emploie à nouveau cowplot pour combiner les graphes, et ici plus précisément partager la légende des deux graphiques. On observera que si le niveau de confiance diminue avec le revenu, la confiance interpersonnelle est plus forte, et de manière parallèle, à la confiance institutionnelle. On remarquera enfin que cest pour les revenu les plus faibles que lestimation est la plus imprécise ou la variance la plus grande. df_wave2&lt;-df %&gt;% filter(cntry==&quot;FR&quot; &amp; Year==&quot;2018&quot;)%&gt;% group_by(revenu) %&gt;% mutate(n=1) %&gt;% summarise(trust_interpersonnel_se=sd(trust_interpersonnel, na.rm=TRUE), #on calcule l&#39;écartype des deux variables trust_institut_se =sd(trust_institut, na.rm=TRUE), n=sum(n), trust_interpersonnel_se= 2*trust_interpersonnel_se/sqrt(n), # on calcule l&#39;erreur type d&#39;échantillonnage trust_institut_se=2*trust_institut_se/sqrt(n) ) %&gt;% dplyr::select(-n) %&gt;% filter(!is.na(revenu)) %&gt;% gather(variable, value, -revenu) %&gt;% #on passe en format long dplyr::select(-revenu,-variable)%&gt;% rename(se=value) df_wave3&lt;-cbind(df_wave,df_wave2) #on concatène les moyennes et les erreurs types #on peut enfin produire le graphique g06a&lt;-ggplot(df_wave3,aes(x=revenu,y=value, group=variable))+ geom_line(stat=&quot;identity&quot;,aes(color=variable), size=1.5)+ geom_errorbar(aes(ymin=value-se, ymax=value+se, color=variable), width=.2,position=position_dodge(0))+ labs(title= &quot;Confiance et revenu&quot;,y= &quot;Moyenne&quot;,x=NULL)+ theme(axis.text.x = element_text( angle=45, hjust =1)) #on controle l&#39;angle et la position horizontale du label g06b&lt;-ggplot(df_wave3,aes(x=revenu,y=value, group=variable))+ geom_line(stat=&quot;identity&quot;,aes(color=variable), size=1.5)+ geom_errorbar(aes(ymin=value-se, ymax=value+se, color=variable), width=.2,position=position_dodge(0))+ ylim(0,100)+ labs(title= &quot;&quot;,y= &quot;Moyenne&quot;,x=NULL)+ theme(axis.text.x = element_text( angle=45, hjust =1)) #on controle l&#39;angle et la position horizontale du label prow &lt;- plot_grid( g06a + theme(legend.position=&quot;none&quot;), g06b + theme(legend.position=&quot;none&quot;), align = &#39;vh&#39;, labels = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), hjust = -1, nrow = 1 ) # extract a legend that is laid out horizontally legend_b &lt;- get_legend( g06a + guides(color = guide_legend(nrow = 1)) + theme(legend.position = &quot;bottom&quot;) ) # add the legend underneath the row we made earlier. Give it 10% # of the height of one plot (via rel_heights). plot_grid(prow, legend_b, ncol = 1, rel_heights = c(1, .1)) La visualisation est utile, encore faut-il quon soit bien certain que les variations ne soit pas le produit du hasard, des fluctuations déchantillonnage. Si en moyenne la perception du pouvoir dachat est associée à des moyennes de confiance décroissantes, les différences observées sont-elle significatives? Dans les représentations précédentes cest le choix de léchelle qui oriente lanalyse. On a un besoin dun test plus objectif. Celui est le très classique test danalyse de variance (ANOVA). Celui-çi est le test danalyse de variance qui consiste à comparer la variance à lintérieur des groupes ( intra), et la variance entre les moyennes des groupes (inter ou between). On note quici on introduit la méthode flextable pour présenter des tableaux au formats scientifique. Lastuce ici est dutiliser aussi xtable. foo&lt;-df %&gt;% filter(cntry==&quot;FR&quot; &amp; Year==&quot;2018&quot;) %&gt;% drop_na() #selection des données fit&lt;-lm(trust_institut~revenu, foo) #calcul du modèle linéaire anova(fit) #test d&#39;analyse de variance ## Analysis of Variance Table ## ## Response: trust_institut ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## revenu 3 27651 9217.1 32.052 &lt; 2.2e-16 *** ## Residuals 1686 484846 287.6 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 library(xtable) #xtable transforme en table certains type d&#39;objet dont les résultats de l&#39;anova ft &lt;- xtable_to_flextable(xtable(anova(fit)), hline.after = c(0,2)) #la fonction permet d&#39;exploiter flextable. ft .tabwid table{ border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-spacing: 0; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-d710345c{border-collapse:collapse;}.cl-d7073776{font-family:'Arial';font-size:11pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-d7073777{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-d7075e68{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d7075e69{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d707ac4c{width:48.1pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d707ac4d{width:52.6pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d707ac4e{width:58.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d707ac4f{width:72.5pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d707ac50{width:65.8pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d707ac51{width:69.5pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d707ac52{width:48.1pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d707ac53{width:58.4pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d707ac54{width:72.5pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d707ac55{width:52.6pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d707ac56{width:65.8pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d707d348{width:69.5pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d707d349{width:48.1pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d707d34a{width:52.6pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d707d34b{width:65.8pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d707d34c{width:58.4pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d707d34d{width:72.5pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d707d34e{width:69.5pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} DfSum SqMean SqF valuePr(&gt;F)revenu327,651.49,217.132.10.0Residuals1,686484,845.7287.6 5.2.2 Deux variables qualitatives Létude de la relation éventuelle entre deux variables qualitative sapprécie traditionnellement par une méthode de tableau croisé. 5.2.2.1 Tableau croisé Pour calculer le tableau croisé on utilise la fonction très simple table et la fonction prop.table t&lt;-table(foo$revenu,foo$habitat) t ## ## Big city Suburbs Town Village Countryside ## Vie confortable 118 82 161 142 31 ## Se débrouille avec son revenu 120 109 275 227 58 ## Revenu insuffisant 48 38 129 88 22 ## Revenu très insuffisant 9 5 18 10 0 prop.table(t,2) ## ## Big city Suburbs Town Village ## Vie confortable 0.40000000 0.35042735 0.27615780 0.30406852 ## Se débrouille avec son revenu 0.40677966 0.46581197 0.47169811 0.48608137 ## Revenu insuffisant 0.16271186 0.16239316 0.22126930 0.18843683 ## Revenu très insuffisant 0.03050847 0.02136752 0.03087479 0.02141328 ## ## Countryside ## Vie confortable 0.27927928 ## Se débrouille avec son revenu 0.52252252 ## Revenu insuffisant 0.19819820 ## Revenu très insuffisant 0.00000000 Mais ce nest pas esthétique, avec la fonction proc_freq de flextable on obtient une meilleure présentation. Elle nous donne en peu de mots les effectif par cellule, les pourcentages en lignes, et en colonnes. ft1&lt;- proc_freq(foo, &quot;revenu&quot;, &quot;habitat&quot;, include.table_percent = FALSE, include.row_percent = FALSE, include.column_total = FALSE, include.column_percent = TRUE) ft1 .tabwid table{ border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-spacing: 0; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-d777d4a4{border-collapse:collapse;}.cl-d76d7b6c{font-family:'Arial';font-size:11pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-d76d7b6d{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-d76d7b6e{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d76d7b6f{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d76dc95a{width:90.9pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d76dc95b{width:45pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d76dc95c{width:72.5pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d76dc95d{width:79.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d76dc95e{width:168.5pt;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d76dc95f{width:90.9pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d76dc960{width:45pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d76dc961{width:168.5pt;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d76dc962{width:72.5pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d76dc963{width:79.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d76dc964{width:168.5pt;background-color:transparent;vertical-align: top;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d76df04c{width:72.5pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d76df04d{width:90.9pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d76df04e{width:79.2pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d76df04f{width:168.5pt;background-color:transparent;vertical-align: top;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d76df050{width:45pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d76df051{width:90.9pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d76df052{width:45pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d76df053{width:79.2pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d76df054{width:168.5pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d76df055{width:72.5pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d76df056{width:79.2pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d76e173e{width:90.9pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d76e173f{width:45pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d76e1740{width:72.5pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d76e1741{width:168.5pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} habitatrevenulabelBig citySuburbsTownVillageCountrysideTotalVie confortableFrequency1188216114231534Col Pct40%35.04%27.62%30.41%27.93%Se débrouille avec son revenuFrequency12010927522758789Col Pct40.68%46.58%47.17%48.61%52.25%Revenu insuffisantFrequency48381298822325Col Pct16.27%16.24%22.13%18.84%19.82%Revenu très insuffisantFrequency951810042Col Pct3.05%2.14%3.09%2.14%0% ft2&lt;- proc_freq(foo, &quot;revenu&quot;, &quot;habitat&quot;, include.table_percent = FALSE, include.row_percent = TRUE, include.column_percent = FALSE) ft2 .tabwid table{ border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-spacing: 0; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-d7c2183e{border-collapse:collapse;}.cl-d7aec4a0{font-family:'Arial';font-size:11pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-d7aec4a1{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-d7aeeb92{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d7aeeb93{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d7af3980{width:72.5pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d7af3981{width:48.1pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d7af3982{width:168.5pt;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d7af3983{width:90.9pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d7af3984{width:84.8pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d7af3985{width:48.1pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d7af3986{width:90.9pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d7af3987{width:84.8pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d7af3988{width:72.5pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d7af3989{width:168.5pt;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d7af398a{width:84.8pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d7af6072{width:72.5pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d7af6073{width:168.5pt;background-color:transparent;vertical-align: top;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d7af6074{width:90.9pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d7af6075{width:48.1pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d7af6076{width:168.5pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d7af6077{width:84.8pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d7af6078{width:72.5pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d7af6079{width:48.1pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d7af607a{width:90.9pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d7af607b{width:84.8pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d7af607c{width:90.9pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d7af8764{width:72.5pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d7af8765{width:168.5pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d7af8766{width:48.1pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} habitatrevenulabelBig citySuburbsTownVillageCountrysideTotalVie confortableFrequency1188216114231534Row Pct22.1%15.36%30.15%26.59%5.81%Se débrouille avec son revenuFrequency12010927522758789Row Pct15.21%13.81%34.85%28.77%7.35%Revenu insuffisantFrequency48381298822325Row Pct14.77%11.69%39.69%27.08%6.77%Revenu très insuffisantFrequency951810042Row Pct21.43%11.9%42.86%23.81%0%TotalFrequency2952345834671111690 5.2.2.2 le valeureux chi² Le test du chi2 sappuie sur une idée très simple qui de fait est un théorème : Si deux variables X et Y sont indépendantes, la fréquence de leur combinaison est le produit des fréquences marginales. On peut donc sur cette base, calculer leffectif attendu (expected frequency) puis le comparer à ce quon a observé pour chacune des cellules du tableau. On somme enfin ces écarts. \\[\\chi^2 = \\sum \\frac {(O_{ij} - E_{ij})^2}{E_{ij}}\\] Naturellement , une même valeur de cette quantité pour un petit tableau( 2x2) na pas la même signification que si le tableau est grand( par ex 20x 10). On lappréciera donc en fonction des degrés de liberté (n-1 x m-1). Le test proprement dit consiste à examiner quelles sont les chances quon obtienne la valeur du chi2 calculé, pour un nombre de degré de liberté donné. Si cette probabilité est faible on rejetera lhypothèse dindépendance des deux variables. Avec r la fonction chsq.test nous simplifie chi2&lt;-chisq.test(t) chi2 ## ## Pearson&#39;s Chi-squared test ## ## data: t ## X-squared = 23.853, df = 12, p-value = 0.0213 Lobjet chi2 est une liste # On isole les éléments qui nous intéresse #library() chi&lt;-round(chi2$statistic,2) p&lt;-round(chi2$p.value,3) #V&lt;-cramerV(t, digit=3) ( le package n&#39;est plus maintenu) 5.2.2.3 diagramme en mosaique Le diagramme en mosaïque détermine la largeur des barres en fonction de leffectif de la variable en abscisse et leur hauteur en fonction de la variable en ordonnée. Les couleurs permettent de mieux comparer. On saperçoit ici que les plus à laise avec leur revenu sont proportionnellement plus nombreux dans les grandes villes, et que ceux qui se débrouille sont plus fréquents dans les campagnes. library(ggmosaic) g1 &lt;- ggplot(data = foo) + geom_mosaic(aes(x=product( revenu ,habitat), fill = revenu))+ theme(axis.text.x = element_text(angle = 45, hjust = -0.1, vjust = -0.2))+ theme(legend.position = &quot;none&quot;)+ labs(title=&quot;Statut vaccinal \\npar genre&quot;, subtitle=paste0(&quot;chi2 =&quot;,chi, &quot; p = &quot;, p, &quot; - V : &quot;, V))+ scale_fill_brewer(palette = &quot;RdYlGn&quot;, direction = -1) g1 5.2.2.4 les chi2s partiels et des cartes de chaleur. Une carte de chaleur représente une grandeur par un gradient de couleur pour chaque cellule définie par des variable x et y. Faisons un premier essai pour représenter les effectifs, plutôt quavoir un tableau de nombres on va obtenir un tableau de couleurs. Larbre qui apparait en ligne et en colonne correspond au résultat dune classification hiérarchique que nous développons dans le chapitre X. library(pheatmap) library(viridis) table2&lt;-as.data.frame(t) %&gt;% pivot_wider(names_from = Var1, values_from = Freq) %&gt;% column_to_rownames( var = &quot;Var2&quot;) pheatmap(table2 , color = rocket(10,direction =-1)) On utilise la même technique mais en représenant une grandeur différentes : les tests du chi2 partiels, pour apprécier les sous ou les sur-représentation. chi2df&lt;- as.data.frame(chi2$stdres) table2&lt;-chi2df %&gt;% pivot_wider(names_from = Var1, values_from = Freq) %&gt;% column_to_rownames( var = &quot;Var2&quot;) pheatmap(table2 , color = brewer.pal(n = 9, name = &quot;RdBu&quot;)) 5.2.2.5 Les treemaps, cest merveilleux Dautre graphiques et des emboîtements library(treemapify) tree1&lt;-df %&gt;% mutate(n=1)%&gt;%group_by(cntry,genre,habitat) %&gt;% summarize(n=sum(n),mean=mean(trust_interpersonnel, na.rm=TRUE)) g10 &lt;- ggplot(tree1, aes(area = n, fill=genre,subgroup=cntry)) + geom_treemap() + geom_treemap_text(aes(label=habitat),colour = &quot;white&quot;, place = &quot;centre&quot;,grow = FALSE)+ geom_treemap_subgroup_text(color=&quot;white&quot;,grow = FALSE)+ geom_treemap_subgroup_border() g10 "],["analyse-graphique-multivariée.html", "Chapitre 6 Analyse graphique multivariée 6.1 La confiance institutionnelle, en détail 6.2 Table de corrélation 6.3 Un cas plus complexe : présidentielle2020 6.4 une boucle pour produire de multiple graphe en variant un paramètre 6.5 Modéliser le biais du sondeur", " Chapitre 6 Analyse graphique multivariée Dans ce chapitre, on généralise à des ensembles de variables. 6.1 La confiance institutionnelle, en détail On veut reprénter 6 variables, correspondant à 5 types dhabitats et 2 pays. df&lt;-readRDS(&quot;./data/dfTrust.rds)&quot;) rad&lt;-df %&gt;% group_by (habitat,cntry) %&gt;% summarize(Partis=mean(Partis, na.rm=TRUE), Parlement=mean(Parlement, na.rm=TRUE), Politiques=mean(Politiques, na.rm=TRUE), Police=mean(Police, na.rm=TRUE), Justice=mean(Justice, na.rm=TRUE), NationsUnies=mean(NationsUnies, na.rm=TRUE), ParlementEurop=mean(ParlementEurop, na.rm=TRUE)) %&gt;% filter(!is.na(habitat)) %&gt;% gather(variable, value, -habitat, -cntry) ggplot(rad, aes(x=reorder(variable, value),y=value, group=habitat))+ geom_line(aes(color=habitat), size=2)+ facet_grid(.~cntry) +coord_flip()+ scale_color_brewer(type=&quot;div&quot;,palette=3)+labs(title= &quot;Les éléments de la confiance institutionnelle&quot;, caption=&quot;ESS2002-2018&quot;,y= &quot;confiance (de 1 à 10)&quot;,x=&quot;institutions&quot;) Une autre variante qui donne lévolution de lévolution de les éléments de la confiance institutionnelle rad&lt;-df %&gt;% group_by (Year,cntry) %&gt;% summarize(Partis=mean(Partis, na.rm=TRUE), Parlement=mean(Parlement, na.rm=TRUE), Politiques=mean(Politiques, na.rm=TRUE), Police=mean(Police, na.rm=TRUE), Justice=mean(Justice, na.rm=TRUE), NationsUnies=mean(NationsUnies, na.rm=TRUE), ParlementEurop=mean(ParlementEurop, na.rm=TRUE)) %&gt;% gather(variable, value, -Year, -cntry) ggplot(rad, aes(x=Year,y=value, group=variable))+ geom_line(aes(color=variable), size=1.2)+ facet_wrap(.~cntry, nrow=1) + scale_color_brewer(palette=&quot;Spectral&quot;)+labs(title= &quot;Les éléments de la confiance institutionnelle&quot;, caption=&quot;ESS2002-2018&quot;,y= &quot;confiance (de 1 à 10)&quot;,x=&quot;institutions&quot;) La différence entre les deux pays est claire, la rupture est accusée plus fortement en France quen Allemagne. Lexplication nest sans doute pas culturelle mais démographique, un coup doeil à la carte des densité permet de comprendre mieux : https://www.populationdata.net/cartes/allemagne-france-densite-de-population-2011/. On pourra tenté un graphe en radar. Mais il nest pas si convaincant. library(fmsb) rad&lt;-df %&gt;% filter(cntry==&quot;FR&quot;) %&gt;% group_by (habitat) %&gt;% summarize(Partis=mean(Partis, na.rm=TRUE), Parlement=mean(Parlement, na.rm=TRUE), Politiques=mean(Politiques, na.rm=TRUE), Police=mean(Police, na.rm=TRUE), Justice=mean(Justice, na.rm=TRUE), NationsUnies=mean(NationsUnies, na.rm=TRUE), ParlementEurop=mean(ParlementEurop, na.rm=TRUE)) %&gt;% filter(!is.na(habitat)) %&gt;% dplyr::select(-habitat) #on doit indiquer les valeurs minimale et maximale - la fonction rep permet de repeter (ici 7 fois pour les 7 variables/col) data &lt;- rbind(rep(7,7) , rep(3,7) , rad) #l&#39;autre method c&#39;est ce choisir maxmin=FALSE #rownames(rad) &lt;- c(&quot;big city&quot;, &quot;suburbs&quot; ,&quot;town&quot;,&quot;village&quot;, &quot;countryside&quot;) radarchart(rad, axistype=0, seg=4, title=&quot;Moyenne par institution&quot;, maxmin=FALSE) legend(x=0.7, y=1, legend = rownames(rad), bty = &quot;n&quot;,text.col = &quot;grey&quot;, cex=1.2, pt.cex=3) 6.2 Table de corrélation Comparer les moyennes est une chose, on souhaiter en plus savoir quelle structure de corrélation les caractérisent. Rien de plus simple library(ggcorrplot) df&lt;-readRDS(&quot;./data/dfTrust.rds)&quot;)%&gt;%filter(Year==2018) foo&lt;-df %&gt;% dplyr::select(NationsUnies,ParlementEurop, Parlement, Justice, Police, Politiques, Partis) %&gt;% drop_na() r&lt;-cor(foo) ggcorrplot(r, hc.order = TRUE, type = &quot;lower&quot;, lab = TRUE) g&lt;-paste0(&quot;./plot/g1&quot;,&quot;.jpg&quot;) ggsave(g,plot=last_plot(), width = 27, height = 19, units = &quot;cm&quot;) 6.3 Un cas plus complexe : présidentielle2020 Nsppolls cumulent les sondages publiés des grands instituts. On utilise ces données , ainsi quune boucle, pour explorer différents paramètre dun modèle de lissage. Le but : mieux percevoir les tendance par une sorte de méta-analyse des différents sondages : 6.4 une boucle pour produire de multiple graphe en variant un paramètre library(lubridate) alph&lt;-.5 for (alph in seq(from=0, to= 1, by=.05)){ df_pol &lt;- read_delim(&quot;https://raw.githubusercontent.com/nsppolls/nsppolls/master/presidentielle.csv&quot;, delim = &quot;,&quot;, escape_double = FALSE, trim_ws = TRUE)%&gt;% filter(tour==&quot;Premier tour&quot;) %&gt;%filter(candidat==&quot;Eric Zemmour&quot;| candidat== &quot;Marine Le Pen&quot;| candidat== &quot;Emmanuel Macron&quot;| candidat== &quot;Jean-Luc Mélenchon&quot;| candidat== &quot;Yannick Jadot&quot;| candidat== &quot;Valérie Pécresse&quot;| candidat==&quot;Fabien Roussel&quot;| candidat==&quot;Anne Hidalgo&quot;) %&gt;% filter(fin_enquete&gt;ymd(&quot;2022-01-09&quot;)) # on commence en septembre , octobre est-il meilleur ? table(df_pol$candidat) SensiP1&lt;-c(&quot;pink&quot;, &quot;orange&quot;, &quot;gray20&quot;, &quot;red&quot;,&quot;firebrick&quot;, &quot;Royalblue&quot;, &quot; skyblue&quot;, &quot;Chartreuse&quot;) ggplot(df_pol, aes(y=intentions, x=fin_enquete))+ geom_point(aes(color=candidat), size=.5, alpha=1-alph)+ geom_smooth(span = alph, aes(col=candidat,fill=candidat), alpha=0.2)+ scale_color_manual(values=SensiP1)+ scale_fill_manual(values=SensiP1)+ labs(title= &quot;Evolution des intentions de vote #présidentielle2022 1er tour&quot;, subtitle =paste(&quot;Lissage méthode loess. alpha=&quot;,alph, &quot; - ci=95%&quot;), caption = &quot;data @nsppolls viz @benavent&quot;, x=NULL)+theme_minimal()+scale_x_date(date_breaks = &quot;1 month&quot;, date_minor_breaks = &quot;1 week&quot;, date_labels = &quot;%B&quot;) sondage_nsppolls&lt;-paste0(&quot;./nsppolls/sondage_nsppolls&quot;, alph*20, &quot;.jpg&quot;) ggsave(sondage_nsppolls,plot=last_plot(), width = 27, height = 19, units = &quot;cm&quot;) } n&lt;-df_pol%&gt;% mutate(n=1)%&gt;% group_by(id)%&gt;%summarise(n=sum(n)) #nombre de sondage n&lt;-nrow(n) Pour créer le gif on emplie magick. On a pris soin de sauvegarder les graphes dans un répertoire propre, ça facilite la lecture en boucle et la fabrication du gif. library(magick) #gif #on constitue une liste des noms des fichier *.jpg que l&#39;on veut associer frames &lt;- paste0(&quot;./nsppolls/&quot;,&quot;sondage_nsppolls&quot;, 0:20,&quot;.jpg&quot;) #on lit et on stoke dans m les images m &lt;- image_read(frames) #on fabrique et on sauvergarde le gif m &lt;- image_animate(m, fps=1) image_write(m, &quot;./plot/sondages_lissage.gif&quot;) 6.4.1 effet sondeur pour anticiper sur le chapitre suivant foo&lt;-df_pol%&gt;% dplyr::select(candidat, intentions, fin_enquete, echantillon,nom_institut)%&gt;% group_by(nom_institut, candidat)%&gt;% summarise(moy=mean(intentions, na.rm=TRUE), std=sd(intentions, na.rm=TRUE)) SensiP2&lt;-c(&quot;gray90&quot;,&quot;gray20&quot;, &quot;Royalblue&quot;, &quot;skyblue&quot;, &quot;orange&quot;, &quot;yellow&quot;, &quot;pink&quot;, &quot;firebrick&quot;, &quot;green&quot;, &quot;gold1&quot;, &quot;gold2&quot;) g&lt;-ggplot(foo,aes(x=candidat,y=moy))+ geom_segment(aes(x = candidat, y = -std+moy, xend = candidat, yend = std+moy, color = nom_institut), size=1.2)+ geom_point(aes(color=nom_institut), size=2)+ scale_color_manual(values = SensiP2)+ theme_minimal()+ coord_flip() g 6.5 Modéliser le biais du sondeur http://www.stat.yale.edu/Courses/1997-98/101/anovareg.htm df_pol$tps&lt;-2 df_pol$tps[df_pol$fin_enquete &lt; ymd(&quot;2022-01-31&quot;)]&lt;-1 df_pol$tps[df_pol$fin_enquete &gt; ymd(&quot;2022-03-01&quot;)]&lt;-3 df_pol$tps&lt;- as.factor(df_pol$tps) fit1&lt;- lm(intentions~candidat*tps,data=df_pol) anova(fit1) ## Analysis of Variance Table ## ## Response: intentions ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## candidat 7 122735 17533.6 10705.9435 &lt; 2.2e-16 *** ## tps 2 25 12.7 7.7658 0.0004363 *** ## candidat:tps 14 4534 323.9 197.7554 &lt; 2.2e-16 *** ## Residuals 2096 3433 1.6 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 fit2&lt;- lm(intentions~candidat*tps+candidat*nom_institut,data=df_pol) anova(fit2) ## Analysis of Variance Table ## ## Response: intentions ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## candidat 7 122735 17533.6 12742.2074 &lt; 2.2e-16 *** ## tps 2 25 12.7 9.2428 0.000101 *** ## nom_institut 10 25 2.5 1.8283 0.051096 . ## candidat:tps 14 4534 323.9 235.3684 &lt; 2.2e-16 *** ## candidat:nom_institut 70 633 9.0 6.5768 &lt; 2.2e-16 *** ## Residuals 2016 2774 1.4 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(fit1,fit2) ## Analysis of Variance Table ## ## Model 1: intentions ~ candidat * tps ## Model 2: intentions ~ candidat * tps + candidat * nom_institut ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 2096 3432.7 ## 2 2016 2774.1 80 658.65 5.9832 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(fit1) ## ## Call: ## lm(formula = intentions ~ candidat * tps, data = df_pol) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.2585 -0.6350 -0.0278 0.6021 5.6493 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.1765 0.1792 17.726 &lt; 2e-16 *** ## candidatEmmanuel Macron 21.5392 0.2534 84.992 &lt; 2e-16 *** ## candidatEric Zemmour 9.7549 0.2534 38.492 &lt; 2e-16 *** ## candidatFabien Roussel -0.8039 0.2534 -3.172 0.001535 ** ## candidatJean-Luc Mélenchon 6.4118 0.2534 25.300 &lt; 2e-16 *** ## candidatMarine Le Pen 13.7353 0.2534 54.198 &lt; 2e-16 *** ## candidatValérie Pécresse 13.2255 0.2534 52.187 &lt; 2e-16 *** ## candidatYannick Jadot 2.7255 0.2534 10.755 &lt; 2e-16 *** ## tps2 -0.6765 0.2342 -2.888 0.003915 ** ## tps3 -0.9765 0.2089 -4.674 3.14e-06 *** ## candidatEmmanuel Macron:tps2 0.6844 0.3312 2.066 0.038934 * ## candidatEric Zemmour:tps2 2.0645 0.3312 6.233 5.52e-10 *** ## candidatFabien Roussel:tps2 2.1511 0.3312 6.494 1.04e-10 *** ## candidatJean-Luc Mélenchon:tps2 1.6438 0.3312 4.963 7.52e-07 *** ## candidatMarine Le Pen:tps2 0.5842 0.3312 1.764 0.077955 . ## candidatValérie Pécresse:tps2 -0.9616 0.3312 -2.903 0.003734 ** ## candidatYannick Jadot:tps2 -0.1630 0.3312 -0.492 0.622725 ## candidatEmmanuel Macron:tps3 4.6819 0.2955 15.847 &lt; 2e-16 *** ## candidatEric Zemmour:tps3 -1.0570 0.2955 -3.578 0.000355 *** ## candidatFabien Roussel:tps3 2.0919 0.2955 7.080 1.95e-12 *** ## candidatJean-Luc Mélenchon:tps3 5.1319 0.2955 17.370 &lt; 2e-16 *** ## candidatMarine Le Pen:tps3 3.4154 0.2955 11.560 &lt; 2e-16 *** ## candidatValérie Pécresse:tps3 -4.8670 0.2955 -16.473 &lt; 2e-16 *** ## candidatYannick Jadot:tps3 0.4724 0.2955 1.599 0.109995 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.28 on 2096 degrees of freedom ## Multiple R-squared: 0.9737, Adjusted R-squared: 0.9735 ## F-statistic: 3379 on 23 and 2096 DF, p-value: &lt; 2.2e-16 summary(fit2) ## ## Call: ## lm(formula = intentions ~ candidat * tps + candidat * nom_institut, ## data = df_pol) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.7981 -0.5456 -0.0272 0.5924 5.2019 ## ## Coefficients: ## Estimate Std. Error ## (Intercept) 3.39409 1.18971 ## candidatEmmanuel Macron 20.60082 1.68251 ## candidatEric Zemmour 10.59844 1.68251 ## candidatFabien Roussel -2.19508 1.68251 ## candidatJean-Luc Mélenchon 11.04593 1.68251 ## candidatMarine Le Pen 15.30968 1.68251 ## candidatValérie Pécresse 7.55835 1.68251 ## candidatYannick Jadot 2.30826 1.68251 ## tps2 -0.68106 0.21565 ## tps3 -0.99409 0.19847 ## nom_institutBVA -0.30224 1.23753 ## nom_institutCluster17 -0.84531 1.21657 ## nom_institutElabe -0.71509 1.21215 ## nom_institutHarris Interactive -0.18347 1.21291 ## nom_institutIfop -0.18249 1.18474 ## nom_institutIpsos -0.08904 1.19038 ## nom_institutKantar Public -0.47826 1.31223 ## nom_institutOdoxa -0.67101 1.35576 ## nom_institutOpinion Way -0.04925 1.18126 ## nom_institutYouGov -1.40000 1.65893 ## candidatEmmanuel Macron:tps2 0.73921 0.30498 ## candidatEric Zemmour:tps2 2.12294 0.30498 ## candidatFabien Roussel:tps2 2.12788 0.30498 ## candidatJean-Luc Mélenchon:tps2 1.71555 0.30498 ## candidatMarine Le Pen:tps2 0.60025 0.30498 ## candidatValérie Pécresse:tps2 -0.97779 0.30498 ## candidatYannick Jadot:tps2 -0.15932 0.30498 ## candidatEmmanuel Macron:tps3 4.79918 0.28068 ## candidatEric Zemmour:tps3 -0.99844 0.28068 ## candidatFabien Roussel:tps3 2.09508 0.28068 ## candidatJean-Luc Mélenchon:tps3 5.15407 0.28068 ## candidatMarine Le Pen:tps3 3.59032 0.28068 ## candidatValérie Pécresse:tps3 -4.65835 0.28068 ## candidatYannick Jadot:tps3 0.29174 0.28068 ## candidatEmmanuel Macron:nom_institutBVA 0.75768 1.75014 ## candidatEric Zemmour:nom_institutBVA -0.46013 1.75014 ## candidatFabien Roussel:nom_institutBVA 1.43661 1.75014 ## candidatJean-Luc Mélenchon:nom_institutBVA -4.47432 1.75014 ## candidatMarine Le Pen:nom_institutBVA -1.61439 1.75014 ## candidatValérie Pécresse:nom_institutBVA 5.43117 1.75014 ## candidatYannick Jadot:nom_institutBVA 0.47709 1.75014 ## candidatEmmanuel Macron:nom_institutCluster17 0.21696 1.72049 ## candidatEric Zemmour:nom_institutCluster17 0.93708 1.72049 ## candidatFabien Roussel:nom_institutCluster17 1.75386 1.72049 ## candidatJean-Luc Mélenchon:nom_institutCluster17 -1.72026 1.72049 ## candidatMarine Le Pen:nom_institutCluster17 -2.63348 1.72049 ## candidatValérie Pécresse:nom_institutCluster17 5.22876 1.72049 ## candidatYannick Jadot:nom_institutCluster17 0.59139 1.72049 ## candidatEmmanuel Macron:nom_institutElabe 1.38103 1.71424 ## candidatEric Zemmour:nom_institutElabe -1.35629 1.71424 ## candidatFabien Roussel:nom_institutElabe 1.64477 1.71424 ## candidatJean-Luc Mélenchon:nom_institutElabe -3.52444 1.71424 ## candidatMarine Le Pen:nom_institutElabe -0.78677 1.71424 ## candidatValérie Pécresse:nom_institutElabe 5.34773 1.71424 ## candidatYannick Jadot:nom_institutElabe 0.80139 1.71424 ## candidatEmmanuel Macron:nom_institutHarris Interactive 1.05598 1.71531 ## candidatEric Zemmour:nom_institutHarris Interactive -0.57494 1.71531 ## candidatFabien Roussel:nom_institutHarris Interactive 0.83821 1.71531 ## candidatJean-Luc Mélenchon:nom_institutHarris Interactive -3.70231 1.71531 ## candidatMarine Le Pen:nom_institutHarris Interactive -0.96863 1.71531 ## candidatValérie Pécresse:nom_institutHarris Interactive 4.65034 1.71531 ## candidatYannick Jadot:nom_institutHarris Interactive 0.56962 1.71531 ## candidatEmmanuel Macron:nom_institutIfop 1.34434 1.67547 ## candidatEric Zemmour:nom_institutIfop -0.55201 1.67547 ## candidatFabien Roussel:nom_institutIfop 1.38693 1.67547 ## candidatJean-Luc Mélenchon:nom_institutIfop -4.84893 1.67547 ## candidatMarine Le Pen:nom_institutIfop -1.37146 1.67547 ## candidatValérie Pécresse:nom_institutIfop 5.78598 1.67547 ## candidatYannick Jadot:nom_institutIfop 0.22809 1.67547 ## candidatEmmanuel Macron:nom_institutIpsos 0.79936 1.68345 ## candidatEric Zemmour:nom_institutIpsos -0.87532 1.68345 ## candidatFabien Roussel:nom_institutIpsos 1.26446 1.68345 ## candidatJean-Luc Mélenchon:nom_institutIpsos -4.62283 1.68345 ## candidatMarine Le Pen:nom_institutIpsos -2.71909 1.68345 ## candidatValérie Pécresse:nom_institutIpsos 4.63358 1.68345 ## candidatYannick Jadot:nom_institutIpsos 1.44225 1.68345 ## candidatEmmanuel Macron:nom_institutKantar Public 1.11499 1.85577 ## candidatEric Zemmour:nom_institutKantar Public 0.49465 1.85577 ## candidatFabien Roussel:nom_institutKantar Public 1.34180 1.85577 ## candidatJean-Luc Mélenchon:nom_institutKantar Public -4.09037 1.85577 ## candidatMarine Le Pen:nom_institutKantar Public -1.02748 1.85577 ## candidatValérie Pécresse:nom_institutKantar Public 4.67986 1.85577 ## candidatYannick Jadot:nom_institutKantar Public -0.11224 1.85577 ## candidatEmmanuel Macron:nom_institutOdoxa 1.45332 1.91734 ## candidatEric Zemmour:nom_institutOdoxa -1.47379 1.91734 ## candidatFabien Roussel:nom_institutOdoxa 1.92240 1.91734 ## candidatJean-Luc Mélenchon:nom_institutOdoxa -4.05383 1.91734 ## candidatMarine Le Pen:nom_institutOdoxa 0.76336 1.91734 ## candidatValérie Pécresse:nom_institutOdoxa 4.03981 1.91734 ## candidatYannick Jadot:nom_institutOdoxa 0.55035 1.91734 ## candidatEmmanuel Macron:nom_institutOpinion Way 0.50026 1.67056 ## candidatEric Zemmour:nom_institutOpinion Way -1.46076 1.67056 ## candidatFabien Roussel:nom_institutOpinion Way 1.44706 1.67056 ## candidatJean-Luc Mélenchon:nom_institutOpinion Way -5.45426 1.67056 ## candidatMarine Le Pen:nom_institutOpinion Way -1.84075 1.67056 ## candidatValérie Pécresse:nom_institutOpinion Way 6.12136 1.67056 ## candidatYannick Jadot:nom_institutOpinion Way 0.33856 1.67056 ## candidatEmmanuel Macron:nom_institutYouGov 0.60000 2.34608 ## candidatEric Zemmour:nom_institutYouGov -1.60000 2.34608 ## candidatFabien Roussel:nom_institutYouGov 1.10000 2.34608 ## candidatJean-Luc Mélenchon:nom_institutYouGov -2.20000 2.34608 ## candidatMarine Le Pen:nom_institutYouGov 5.10000 2.34608 ## candidatValérie Pécresse:nom_institutYouGov 4.10000 2.34608 ## candidatYannick Jadot:nom_institutYouGov 2.40000 2.34608 ## t value Pr(&gt;|t|) ## (Intercept) 2.853 0.004377 ** ## candidatEmmanuel Macron 12.244 &lt; 2e-16 *** ## candidatEric Zemmour 6.299 3.66e-10 *** ## candidatFabien Roussel -1.305 0.192163 ## candidatJean-Luc Mélenchon 6.565 6.59e-11 *** ## candidatMarine Le Pen 9.099 &lt; 2e-16 *** ## candidatValérie Pécresse 4.492 7.44e-06 *** ## candidatYannick Jadot 1.372 0.170242 ## tps2 -3.158 0.001611 ** ## tps3 -5.009 5.96e-07 *** ## nom_institutBVA -0.244 0.807081 ## nom_institutCluster17 -0.695 0.487241 ## nom_institutElabe -0.590 0.555298 ## nom_institutHarris Interactive -0.151 0.879779 ## nom_institutIfop -0.154 0.877598 ## nom_institutIpsos -0.075 0.940383 ## nom_institutKantar Public -0.364 0.715551 ## nom_institutOdoxa -0.495 0.620703 ## nom_institutOpinion Way -0.042 0.966747 ## nom_institutYouGov -0.844 0.398816 ## candidatEmmanuel Macron:tps2 2.424 0.015445 * ## candidatEric Zemmour:tps2 6.961 4.55e-12 *** ## candidatFabien Roussel:tps2 6.977 4.07e-12 *** ## candidatJean-Luc Mélenchon:tps2 5.625 2.11e-08 *** ## candidatMarine Le Pen:tps2 1.968 0.049184 * ## candidatValérie Pécresse:tps2 -3.206 0.001367 ** ## candidatYannick Jadot:tps2 -0.522 0.601458 ## candidatEmmanuel Macron:tps3 17.098 &lt; 2e-16 *** ## candidatEric Zemmour:tps3 -3.557 0.000383 *** ## candidatFabien Roussel:tps3 7.464 1.24e-13 *** ## candidatJean-Luc Mélenchon:tps3 18.363 &lt; 2e-16 *** ## candidatMarine Le Pen:tps3 12.791 &lt; 2e-16 *** ## candidatValérie Pécresse:tps3 -16.597 &lt; 2e-16 *** ## candidatYannick Jadot:tps3 1.039 0.298749 ## candidatEmmanuel Macron:nom_institutBVA 0.433 0.665115 ## candidatEric Zemmour:nom_institutBVA -0.263 0.792645 ## candidatFabien Roussel:nom_institutBVA 0.821 0.411826 ## candidatJean-Luc Mélenchon:nom_institutBVA -2.557 0.010644 * ## candidatMarine Le Pen:nom_institutBVA -0.922 0.356411 ## candidatValérie Pécresse:nom_institutBVA 3.103 0.001940 ** ## candidatYannick Jadot:nom_institutBVA 0.273 0.785186 ## candidatEmmanuel Macron:nom_institutCluster17 0.126 0.899664 ## candidatEric Zemmour:nom_institutCluster17 0.545 0.586049 ## candidatFabien Roussel:nom_institutCluster17 1.019 0.308139 ## candidatJean-Luc Mélenchon:nom_institutCluster17 -1.000 0.317496 ## candidatMarine Le Pen:nom_institutCluster17 -1.531 0.126012 ## candidatValérie Pécresse:nom_institutCluster17 3.039 0.002403 ** ## candidatYannick Jadot:nom_institutCluster17 0.344 0.731085 ## candidatEmmanuel Macron:nom_institutElabe 0.806 0.420553 ## candidatEric Zemmour:nom_institutElabe -0.791 0.428924 ## candidatFabien Roussel:nom_institutElabe 0.959 0.337434 ## candidatJean-Luc Mélenchon:nom_institutElabe -2.056 0.039913 * ## candidatMarine Le Pen:nom_institutElabe -0.459 0.646309 ## candidatValérie Pécresse:nom_institutElabe 3.120 0.001837 ** ## candidatYannick Jadot:nom_institutElabe 0.467 0.640200 ## candidatEmmanuel Macron:nom_institutHarris Interactive 0.616 0.538214 ## candidatEric Zemmour:nom_institutHarris Interactive -0.335 0.737521 ## candidatFabien Roussel:nom_institutHarris Interactive 0.489 0.625133 ## candidatJean-Luc Mélenchon:nom_institutHarris Interactive -2.158 0.031015 * ## candidatMarine Le Pen:nom_institutHarris Interactive -0.565 0.572343 ## candidatValérie Pécresse:nom_institutHarris Interactive 2.711 0.006763 ** ## candidatYannick Jadot:nom_institutHarris Interactive 0.332 0.739863 ## candidatEmmanuel Macron:nom_institutIfop 0.802 0.422437 ## candidatEric Zemmour:nom_institutIfop -0.329 0.741837 ## candidatFabien Roussel:nom_institutIfop 0.828 0.407892 ## candidatJean-Luc Mélenchon:nom_institutIfop -2.894 0.003844 ** ## candidatMarine Le Pen:nom_institutIfop -0.819 0.413138 ## candidatValérie Pécresse:nom_institutIfop 3.453 0.000565 *** ## candidatYannick Jadot:nom_institutIfop 0.136 0.891729 ## candidatEmmanuel Macron:nom_institutIpsos 0.475 0.634955 ## candidatEric Zemmour:nom_institutIpsos -0.520 0.603149 ## candidatFabien Roussel:nom_institutIpsos 0.751 0.452670 ## candidatJean-Luc Mélenchon:nom_institutIpsos -2.746 0.006085 ** ## candidatMarine Le Pen:nom_institutIpsos -1.615 0.106425 ## candidatValérie Pécresse:nom_institutIpsos 2.752 0.005968 ** ## candidatYannick Jadot:nom_institutIpsos 0.857 0.391697 ## candidatEmmanuel Macron:nom_institutKantar Public 0.601 0.548025 ## candidatEric Zemmour:nom_institutKantar Public 0.267 0.789843 ## candidatFabien Roussel:nom_institutKantar Public 0.723 0.469738 ## candidatJean-Luc Mélenchon:nom_institutKantar Public -2.204 0.027628 * ## candidatMarine Le Pen:nom_institutKantar Public -0.554 0.579867 ## candidatValérie Pécresse:nom_institutKantar Public 2.522 0.011753 * ## candidatYannick Jadot:nom_institutKantar Public -0.060 0.951779 ## candidatEmmanuel Macron:nom_institutOdoxa 0.758 0.448546 ## candidatEric Zemmour:nom_institutOdoxa -0.769 0.442182 ## candidatFabien Roussel:nom_institutOdoxa 1.003 0.316155 ## candidatJean-Luc Mélenchon:nom_institutOdoxa -2.114 0.034612 * ## candidatMarine Le Pen:nom_institutOdoxa 0.398 0.690574 ## candidatValérie Pécresse:nom_institutOdoxa 2.107 0.035242 * ## candidatYannick Jadot:nom_institutOdoxa 0.287 0.774112 ## candidatEmmanuel Macron:nom_institutOpinion Way 0.299 0.764623 ## candidatEric Zemmour:nom_institutOpinion Way -0.874 0.381997 ## candidatFabien Roussel:nom_institutOpinion Way 0.866 0.386475 ## candidatJean-Luc Mélenchon:nom_institutOpinion Way -3.265 0.001113 ** ## candidatMarine Le Pen:nom_institutOpinion Way -1.102 0.270647 ## candidatValérie Pécresse:nom_institutOpinion Way 3.664 0.000254 *** ## candidatYannick Jadot:nom_institutOpinion Way 0.203 0.839420 ## candidatEmmanuel Macron:nom_institutYouGov 0.256 0.798174 ## candidatEric Zemmour:nom_institutYouGov -0.682 0.495325 ## candidatFabien Roussel:nom_institutYouGov 0.469 0.639216 ## candidatJean-Luc Mélenchon:nom_institutYouGov -0.938 0.348494 ## candidatMarine Le Pen:nom_institutYouGov 2.174 0.029834 * ## candidatValérie Pécresse:nom_institutYouGov 1.748 0.080687 . ## candidatYannick Jadot:nom_institutYouGov 1.023 0.306439 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.173 on 2016 degrees of freedom ## Multiple R-squared: 0.9788, Adjusted R-squared: 0.9777 ## F-statistic: 902.8 on 103 and 2016 DF, p-value: &lt; 2.2e-16 library(jtools) library(interactions) cat_plot(fit2, pred=candidat,modx= nom_institut, color.class=&quot;Spectral&quot;)+ scale_color_manual(values = SensiP2)+coord_flip() cat_plot(fit2, pred= tps,modx=candidat, color.class=&quot;Spectral&quot;, dodge.width=0)+ scale_color_manual(values = SensiP2)+geom_line(aes(color=candidat)) "],["données-géographique.html", "Chapitre 7 Données géographique", " Chapitre 7 Données géographique voir étude de cas Airbnb avec le package sf "],["analyses-factorielles.html", "Chapitre 8 Analyses factorielles 8.1 Origine et histoire 8.2 Le modèle factoriel des tests psychologiques 8.3 Examen de la matrice de corrélation 8.4 Modèle factoriel 8.5 Une généralisation de lACP : lAFC 8.6 Développements 8.7 En conclusion", " Chapitre 8 Analyses factorielles 8.1 Origine et histoire Par analyse factorielle, on entend finalement un ensemble de méthodes dont lobjectifs est dextraire dun ensemble multivariée de données, un petit nombre de dimensions, les facteurs, qui rendent compte lessentiel des variations. Elles partagent aussi une même structure mathématique qui permet de décomposer et de réduire une matrice de données en un ensemble de matrice de dimensions réduite. On peut en distinguer deux écoles, lune alimentée par des questions de psychométrie a nourrit plusieurs decennies de recherche en traitant les tests psychométriques. Lautre française sintéressent aux variables qualitatives, et a une perspective plus descriptive. 8.1.1 Une petite histoire de la psychométrie Lanalyse factorielle trouve son origine, en psychologie, dans lintuition que dans des épreuves multiples un facteur principal contrôle les variation des items (les performance à différents tests). Mais cest avec Thurstone Thurstone (1931) que lidée prend toute son ampleur en permettant que plusieurs facteurs traduisent la structure de la matrice de corrélations entre les tests. Spearman, hotelling,. Dans le monde de la gestion et en particulier de la GRH et du marketing, largement inspirés par la psychologie et la psychologie sociale, ces méthodes se sont propagées et ont formalisé un processus détude largement fondé sur ces techniques. Il est bien connu par de processus de Churchill qui a synthésisé une manière de construire et de développé des instruments de mesure par questionnaire. Cest l article de historique de Churchill ( ref). 8.1.2 Lécole française de lanalyse des données appliquée aux sciences sociales Un personnage : Emile Benzekri Boudieu en premier applicateurs Une école Française : pagès, escoffier, morisseau, Husson a repris le flambleau en développant FactoMiner. Une série de logiciels : Alceste, Statitcf 8.2 Le modèle factoriel des tests psychologiques 8.2.1 Un peu de théorie La première historiquement est celle des psychologues et en particulier le modèle en terme de facteurs communs et spécifiques. Elle vise à partir de lanalyse dune matrice de corrélation à identifier des éléments de structures sous-jascents. La structure du modèle factoriel peut être présentée de manière simple. On supposera que chaque variables observées peut être décrites comme composées de facteurs généraux (\\(F_{ik}\\) ) et de facteurs spécifiques $ _{i}$. Le modèle suppose ainsi que la valeur de lindividu i pour la variable j, dépend de k facteurs sousjacents, les facteurs communs, et dun terme spécifique à litem et à lindividu. \\[x_{ij}= a_{1j}F_{i1} + a_{2j}F_{i2} + \\cdots + a_{jk}F_{ik}+\\varepsilon_{ij}\\] On peut représenter celà de manière plus graphique, en utilisant les conventions symboliques des modèles structurels quon examine dans le chapitre 9. On y verra dailleurs comment ce modèle peut être spécifié de manière confirmatoire. On remarquera dans cette structure que les facteurs peuvent être corrélés. Modèle Factoriel exploratoire - EFA 8.2.2 Lestimation Certains lecteurs seront surpris de cette présentation, ils sont sans doute plus habitués à factoriser en employant une méthode de l ACP. Effectivement cette méthode sur laquelle on va revenir avec plus de détail dans la seconde section de ce chapitre, est une des techniques qui permettent dapprocher le modèle théorique que lon vient de présenter. Elle nest pas la seule. Lestimation du modèle requierts deux décisions : lune sur la méthode dextraction des facteurs, et lautres sur la méthode de rotation. les méthodes dextraction ACP ML analyse en facteur principaux et spécifiques Les méthodes de rotation. Varimax Promax Oblimin  Lobjectif des méthodes danalyses factorielles est de réduire un ensemble de variables à un petit nombre de dimensions qui résument lessentiel de linformation. 8.2.3 Ressources On utilise principalement le package psych développé par Revelle et dédié à la psychométrie. Il couvre le plus complétement le champs de lanalyse factorielle et de la psychométrie. Sy ajoutent deux fonctions très utiles pour représenter les résultats des analyses sous une forme lisible et au standard des publications scientifiques. Elles utilisent les ressources de flextable. #library(psych) #library(flextable) # Une fonction utile pour créer flex &lt;- function(data, title=NULL) { # this grabs the data and converts it to a flextbale flextable(data) %&gt;% # this makes the table fill the page width set_table_properties(layout = &quot;autofit&quot;, width = 1) %&gt;% # font size fontsize(size=10, part=&quot;all&quot;) %&gt;% #this adds a ttitlecreates an automatic table number set_caption(title, autonum = officer::run_autonum(seq_id = &quot;tab&quot;, pre_label = &quot;Table &quot;, post_label = &quot;\\n&quot;, bkm = &quot;anytable&quot;)) %&gt;% # font type font(fontname=&quot;Times New Roman&quot;, part=&quot;all&quot;) } # et une seconde fonction pour le tableaux des loadings fa_table &lt;- function(x, cut) { #get sorted loadings loadings &lt;- fa.sort(x)$loadings %&gt;% round(3) #supress loadings loadings[loadings &lt; cut] &lt;- &quot;&quot; #get additional info add_info &lt;- cbind(x$communality, x$uniquenesses, x$complexity) %&gt;% # make it a data frame as.data.frame() %&gt;% # column names rename(&quot;Communality&quot; = V1, &quot;Uniqueness&quot; = V2, &quot;Complexity&quot; = V3) %&gt;% #get the item names from the vector rownames_to_column(&quot;item&quot;) #build table loadings %&gt;% unclass() %&gt;% as.data.frame() %&gt;% rownames_to_column(&quot;item&quot;) %&gt;% left_join(add_info) %&gt;% mutate(across(where(is.numeric), round, 3)) } 8.2.4 Cas dapplication Pour appliquer la méthode on va sinteresser à léchelle des valeurs de kahle qui sont mesurée dan différents pays au cours des différentes vagues de lenquête ESS? Les variables mesurées sont un ensemble de 21 questions qui proposent des niveaux dimportances accordées à 21 questions, ou items, dont voici les formulation en anglais. Les répondants ont le choix sur une échelle de 0 à 10 qui va de pas du tout important à très important. On se concentre sur les observations de la dernière vague. Cette échelle a été développée par Schwartz En voici les items dans leur formulation anglaise. IPCRTIV Important to think new ideas and being creative IMPRICH Important to be rich, have money and expensive things IPEQOPT Important that people are treated equally and have equal opportunities IPSHABT Important to show abilities and be admired IMPSAFE Important to live in secure and safe surroundings IMPDIFF Important to try new and different things in life IPFRULE Important to do what is told and follow rules IPUDRST Important to understand different people IPMODST Important to be humble and modest, not draw attention IPGDTIM Important to have a good time IMPFREE Important to make own decisions and be free IPHLPPL Important to help people and care for others well-being IPSUCES Important to be successful and that people recognise achievements IPSTRGV Important that government is strong and ensures safety IPADVNT Important to seek adventures and have an exciting life IPBHPRP Important to behave properly IPRSPOT Important to get respect from others IPLYLFR Important to be loyal to friends and devote to people close IMPENV Important to care for nature and environment IMPTRAD Important to follow traditions and customs IMPFUN Important to seek fun and things that give pleasure # On renomme les variables pour une meilleure lecture et on selectionne le tableau de données utile à l&#39;analyse. df &lt;- read_csv(&quot;./Data/ESS1-9e01_1.csv&quot;) %&gt;% rename( V_creative=ipcrtiv, V_richness= imprich, V_justice =ipeqopt, V_admiration=ipshabt, V_security=impsafe, V_novelty=impdiff, V_conformism=ipfrule, V_openmindedness=ipudrst, V_modesty=ipmodst, V_fun=ipgdtim, V_autonomy=impfree, V_Care=iphlppl, V_Success=ipsuces, V_Autority =ipstrgv, V_Adventures=ipadvnt, V_wellbehavior=ipbhprp, V_respect=iprspot, V_loyalty=iplylfr, V_environnement=impenv, V_tradition=imptrad, V_pleasure=impfun) foo1&lt;-df %&gt;% filter(essround==9)%&gt;% dplyr::select(matches(&quot;V_.*&quot;), cntry) %&gt;% #notons la selection fondée sur des regex drop_na() 8.3 Examen de la matrice de corrélation Calculons la matrice de corrélation, et présentons là en organisant lordre des variables selon leur corrélation. A ce stade indiquons quil sagit de mettre un ordre dans les variables, tel que des variables fortement corrélées soient adjascentes (on revient sur la méthode utilisée dans le chapitre suivant). On saperçoit quune structure émerge. Quatre groupes de variables peuvent être discernés: * la jouissance * le succès social * louverture aux autres * la sécurité Dans le filigrane de la matrice de corrélation, on devine une structure factorielle. foo&lt;- foo1 %&gt;% dplyr::select(matches(&quot;V_.*&quot;)) M = cor(foo) ggcorrplot(M, hc.order = TRUE, type = &quot;lower&quot;, outline.col = &quot;white&quot;, ggtheme = ggplot2::theme_gray, colors = c(&quot;#6D9EC1&quot;, &quot;white&quot;, &quot;#E46726&quot;), lab = TRUE, lab_size = 4) 8.4 Modèle factoriel Testons un modèle danalyse factorielle à 4 dimensions. Nous laugmentons dun procédure de rotation oblimin pour un meilleur ajustement. fa &lt;- fa(foo,4, rotate=&quot;oblimin&quot;) #principal axis fa_table(fa, .30)%&gt;% flex(&quot;A Pretty Factor Analysis Table&quot;) .tabwid table{ border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-spacing: 0; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-1f79a976{table-layout:auto;border-collapse:collapse;width:100%;}.cl-1f6bd044{font-family:'Times New Roman';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-1f6bf736{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-1f6bf737{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-1f6c4524{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1f6c4525{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1f6c4526{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1f6c4527{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1f6c4528{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1f6c4529{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 8.1: A Pretty Factor Analysis Table itemMR1MR4MR3MR2CommunalityUniquenessComplexityV_openmindedness0.6920.4720.5281.002V_justice0.6650.3990.6011.044V_Care0.6210.5160.4841.145V_environnement0.5610.4270.5731.171V_loyalty0.5180.4930.5071.570V_autonomy0.4380.3640.6361.603V_creative0.4290.3410.6592.225V_modesty0.4210.3010.3390.6611.967V_admiration0.6970.5050.4951.049V_Success0.6440.5390.4611.094V_richness0.5260.3360.6641.402V_respect0.4190.3790.3830.6172.157V_wellbehavior0.6040.4610.5391.094V_tradition0.5310.3230.6771.111V_conformism0.4640.2650.7351.166V_security0.4450.3860.6142.047V_Autority0.3950.3690.6311.921V_pleasure0.7580.5610.4391.035V_fun0.5310.4130.5871.145V_Adventures0.5060.4650.5351.829V_novelty0.3920.4310.5692.553 fa[[&quot;Vaccounted&quot;]] %&gt;% as.data.frame() %&gt;% rownames_to_column(&quot;Property&quot;) %&gt;% mutate(across(where(is.numeric), round, 3)) %&gt;% flex(&quot;Eigenvalues and Variance Explained for Oblimin Factor Solution&quot;) .tabwid table{ border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-spacing: 0; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-1f9de17e{table-layout:auto;border-collapse:collapse;width:100%;}.cl-1f9166f6{font-family:'Times New Roman';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-1f918de8{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-1f918de9{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-1f9229c4{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1f9229c5{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1f9229c6{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1f9229c7{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1f9229c8{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1f9229c9{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 8.1: Eigenvalues and Variance Explained for Oblimin Factor Solution PropertyMR1MR4MR3MR2SS loadings3.1221.9581.8971.813Proportion Var0.1490.0930.0900.086Cumulative Var0.1490.2420.3320.419Proportion Explained0.3550.2230.2160.206Cumulative Proportion0.3550.5780.7941.000 Le set de données que nous avons traité est composé de 15 échantillons venant dautant de pays. Puisque nous avons réduits les 22 mesures initiales à 4 grands facteurs, il est temps danalyser les différences entre les pays. On va dabord récupérer les scores de chaque observation sur les quatre dimensions obtenues quon ajoute à notre fichier de travail pour récupérer la variable pays. #récupérer les scores scores&lt;-fa$scores scores&lt;-as.data.frame(unclass(scores)) #matcher pour récupérer la variable pays et renommer pour plus de lisibilité df_typo&lt;-cbind(foo1, scores) %&gt;% rename(F_Altruisme = MR1, F_Conservatisme=MR2, F_Performance=MR4, F_Hedonisme=MR3) # On calcule les scores moyens par pays et les erreurs d&#39;échantillonnage df_g &lt;- df_typo %&gt;% dplyr::select(matches(&quot;F_.*&quot;), cntry)%&gt;% gather(variable, value,-cntry)%&gt;% mutate(n=1)%&gt;% group_by(variable,cntry)%&gt;% summarize(mean=mean(value), n=sum(n), se=sd(value)/sqrt(n)) #on représente les résultats ggplot(df_g,aes(x=variable, y=mean))+ geom_bar(stat=&quot;identity&quot;,aes(fill=cntry), size=1.5)+ coord_flip()+ geom_errorbar(aes(ymin=mean-se, ymax=mean+se), width=.2, position=position_dodge(.9)) + facet_wrap(vars(cntry ),ncol=3) ## Analyse en composante principale LACP, dont loptique est différente dans le sens où lon cherche moins à rendre compte dune structure sous-jascente à la matrice de corrélation , quà réduire linformation dans un espace limité. 8.4.1 le problème théorique De manière intuitive lACP est la technique qui permet de représenter un poisson, une structure, sous son jour le plus intelligible, cest à dire celui qui magnifie ses variations. Examinons un poisson sous différentes projections. La première image rend mieux compte de la forme du poisson que la seconde, elle ne diffère que par la projection. De lune à lautre il ny a quy rotation à 90°C vers la droite. Cest la même image, le même phénomène mais représenté selon deux perspectives, deux bases en terme de mathématiques. On comprend que pour représenter un objet au mieux dans un faible nombre de dimensions, il faut trouver la base vectorielle qui maximise les variations de taille. résoudre ce problème est ce que fait lACP Modèle Factoriel exploratoire - EFA 8.4.2 Une représentation symbolique Lidée va donc être de décomposer une matrice de variance-covariance (ou de corrélation) en respectant une contraintes : faire en sorte que le maximum de variance soit capturée par la première dimension, puis par les suivantes successivement. La solution à ce problème se trouve dans la résolution dun problème matriciel. Il faut procéder à un changement de base, autrement dit à un changement de référentiel. La matrice de variance-covariance, ou de corrélation, si on a, au préalable, centré et standardisé les valeurs des variables, est obtenue simplement en multipliant la matrice de données (individus x variable) par sa transposée. \\[\\Sigma = XX^t\\] Comme \\[\\Sigma\\] est symétrique, elle est diagonalisable et peut-être représentée par une matrice de score W et une matrice diagonale D. \\[\\Sigma_{e} =WDW^T\\] où D est la matrice diagonale des valeurs propres et W la matrice des composantes comprenant les j variables ( en ligne) et les k dimensions (en colonne). Léquivalence suppose que le nombre de composantes est égal au nombre de variables initiales, Cependant lusage conduit à ne retenir quun petit nombre de dimensions de telles sorte à ce que la différence entre \\(\\Sigma\\) et \\(\\Sigma_{e}\\) soit relativement petite. La matrice de score comprend autant de lignes que dindividus et de colonnes que de dimensions-sous-jascentes. On remarquera que dans ce modèles on a autant de composantes que de variables, mais que ces dernières représentent une part décroissante de la variance. Certaines composantes nont pas de sens on se concentrera sur les premières rejoignant lidée de lanalyse factorielle : peu de composantes, de facteurs, rendent compte des variations des données. On restera cependant conscient que lACP nest au fond quune manière de représenter les données, juste une projection. Ne retenir que les premières composantes va au-delà du modèle, cest une démarche qui consiste à considérer que seules les premières composantes sont significatives, en apportant du sens, et les dernières peuvent être négligée. Cest une manière approximative de rejoindre le modèle factoriel, une solution simple pour en obtenir une solution. 8.4.3 Application En guise dapplication on va utiliser un tout petit jeu de données issu de lanalyse précédente : le tableau des profils pays, sur les 21 valeurs de @{schwartz_les_2006, Avec cette procédure dagrégation on réduit fortement la variance individuelle, pour ne garder que des différences en moyenne dun pays à lautre. Le plus ici ne va plus être de comprendre la structure profonde des données, mais simplement de représenter ces différences dans un espace de dimension réduite. foo&lt;-foo1%&gt;% group_by(cntry)%&gt;% summarise(across(V_creative:V_pleasure, ~ mean(.x, na.rm = TRUE))) #on note la fonction qui permet de résumer plusieurs variables à la fois X&lt;- foo%&gt;% dplyr::select(-cntry)%&gt;% as.data.frame() rownames(X) &lt;- foo$cntry Plusieurs bibliothèque, en plus de la fonction de base princomp, propose une solution d ACP. On choisit dutiliser celle du package Factominer quon accompagne de la bibliothèque factoextra pour ses ressources graphiques. Les résultats portent sur 3 éléments : les valeurs propres de chacune des dimensions retenues, les coordonnées des vecteurs variables, et celles des points individus. #ACP res.pca&lt;-PCA(X, scale.unit = TRUE, ncp = 2, graph = FALSE) fviz_screeplot(res.pca, ncp=21) x&lt;-res.pca$eig Le premier élément danalyse et le graphe des éboulis (ou scree plot) qui représente les variances projetées sur chacune des composantes. Ici deux composantes représentent les deux tiers. On représente les corrélations des valeurs aux deux composantes par un corrélogramme qui distingue nettement une composante douverture aux autres, et une composante plus autoritaire et égocentrée. Le biplot permet en plus de représenter les individus qui sont dans ce cas les différents pays. On laisse au lecteur de comprendre les différences entre les pays. library(&quot;corrplot&quot;) corrplot(t(res.pca$var$cos2), is.corr=FALSE, tl.cex = 0.8) ### ce merveilleux bi plot fviz_pca_biplot(res.pca, col.ind = &quot;cos2&quot;, labelsize = 3, gradient.cols = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;), repel = TRUE )# Biplot des individus et variables 8.5 Une généralisation de lACP : lAFC LAFC trouve une application remarquable dans lanalyse de tableaux croisés. Elle est une méthode de réprésentation des profils lignes et colonnes: on saperçoit que deux analyses peuvent être menées : lune sur les colonnes, et lautre sur les lignes. Dans les deux cas cette analyse peut se faire en comparant les colonnes (lignes) selon la formule suivante \\[d_{i,j}= (f_{.i}-f_{.j})^2\\] Lidée maintenant est claire : on mène deux acp, en ligne et en colonne, et on projettent conjointement ( dans un même espace) library(readr) BDCOM_2020 &lt;- read_csv(&quot;Data//BDCOM/BDCOM_2020.csv&quot;) %&gt;%rename(CODACT=CODE_ACTIVITE) BDCOM_2017_CODACT_OD &lt;- read_delim(&quot;Data/BDCOM/BDCOM_2017_CODACT_OD.csv&quot;, delim = &quot;;&quot;, escape_double = FALSE, trim_ws = TRUE) df&lt;-BDCOM_2020%&gt;%left_join(BDCOM_2017_CODACT_OD, by = &quot;CODACT&quot;)%&gt;%rename(ACT=27) t&lt;-table(df$ARRONDISSEMENT,df$ACT ) res.ca &lt;- CA(t, graph = FALSE) fviz_ca_biplot(res.ca, labelsize = 2, repel=TRUE)+ theme(text = element_text(size =7)) +xlim(-0.75, 1)+ylim(-.75,0.75) règles dinterprétation le point (0,0) représente le barycentre du nuage de point,et donc linvidu moyens les lignes/colonne les plus excentrée sont les moins fréquentes, la distance dune modalité dune variable à une autres, indique la correspondance des deux modalités qui partagent les mêmes individus. linertie totale est chi²/n et donc une véritable méthode : analyse de la décomposition du khi2. Dans notre exemple on note de suite les arrondissement 2 et 3 qui sont les plus proches de la catégorie commerce de gros. On note aussi une disposition linéaire qui opposent les arrondissements excentrés, aux arrondissements du centre. Un univers commercial résidentiel vs un univers de transit (spectacles et grands magasins) 8.5.1 AFCM multiple Très rapidement la méthode a été appliquée à une généralisation des tableaux croisés : le tableau de burt, ou son équivalent : le tableau disjonctif complet. exemple La mise en oeuvre par factominer permet demployer une techniques de représentation de variables complémentaires : elles ninterviennent pas dans le calcul de la configuration factorielles, mais leurs positions dans lespace sont calculées comme le barycentre des individus qui possède le trait considéré. Leur projection a un rôle illustratif. library(FactoMineR) foo&lt;-df%&gt;% dplyr::select(ACT, SURFACE, SITUATION, LIBACT, ARRONDISSEMENT)%&gt;% as.matrix() res&lt;-MCA(foo,graph = FALSE,quali.sup=5) fviz_mca_var(res, labelsize = 2, repel=TRUE) remarques complémentaires : * pas de signification de linertie globale qui dépend de la structure du tableau ( nombres de variables et de leurs modalités) 8.6 Développements derrière les méthodes il y a un principe mathématique fondamental qui est au fondement de bien dautres méthodes factorielles. Cest celle de la Singular Variance décomposition dont lACP est finalement un cas particulier. 8.6.1 le SVD Le modèle mathématique fondamental décomposer une matrice en plusieurs matrices lacp une application à une matrice de nature particulières : la matrice de covariance ou de corrélation si standardisée de nombreuses autres applications : à des matrices de comptage compression dimage information retrieval dautres méthodes sappuient sur ce principe fondamental, et permettent de traiter des données textuelle . On reporte le lecteur au chapitre X de du book NLP. 8.6.2 ACM , analyse canonique , analyse discriminante Si ACP, AFC et AFCM ont pris le devant de la scène, bien dautre méthodes analogues ont été développées ACM Analyse canonique Analyse factorielle discriminante qui a perdu du terrain au profit du modèle de régression logistique. Un regain avec le machine learning et LSA, NFM etc.. 8.7 En conclusion une idée essentielle : réduire de nombreuses variables à un petit jeu de variables synthétiques des méthodes au cur de lanalyse des données une autre idée essentielle : celle de vectoriser les données quon observe. References "],["clus.html", "Chapitre 9 Clustering 9.1 Les méthodes hiérarchiques ascendantes 9.2 segmentation simplifiée 9.3 tableaux croisés de la typologie et des critères sociaux démos 9.4 AFCM pour une synthèse 9.5 Les méthodes non-hiérarchiques 9.6 Autres méthodes 9.7 Conclusion", " Chapitre 9 Clustering Lobjectif des méthodes de classification automatique est de regrouper des observations qui se ressemblent sur un ensemble multidimensionnel de caractéristiques. insérer image Dans ce chapitre nous examinons deux familles de méthodes qui le distingue par la procédure de calcul : hierarchique dune part, non hiérarchique de lautre. On garde pour le chapître suivant létude des modèles de décisions qui ont une longue et riche histoire en marketing et ont préparé le développement de certains modèles de machine learning. 9.1 Les méthodes hiérarchiques ascendantes Elles trouvent leur origine en biologie où dès les années 1930 Sokal et Sneath(Sneath and Sokal 1973) ont proposé des méthodes pour analyser lévolution des espèces. Lidée réside dans la comparison de specimens sur la base dun certains nombre de caractéristiques, dabord des caractères phénotypiques, puis dans ce domaine en sappuyant sur les caractéristiques génétiques. Nous nentrerons pas dans une discussion plus approfondis mais signalons que ces choix déterminent des méthodes et des hypothèses très différentes et largement débattues (cladistique etc) Prenons le cas de différences phénotypiques et le tableau suivant. tableau Le but du jeu est de regrouper successivement les spécimens en fonction de leur ressemblance. Lalgorithme consiste simplement à 1) calculer toutes les ressemblances deux à deux et 2) à fondre en une classe les deux éléments qui se ressemble le plus. On réitère lopération jusquà ce quon obtienne plus quune classe. Le résultat est une arborescence dont chaque noeud représente un regrouppement de classe à un certain niveau de distance. figure Leurs variétés dépend de deux paramètres : le choix de la mesure de dissimilarités : Une distance euclidienne ? Son carré ? Une distance binaire comme lindice de Jaccard? le choix de la méthode dagrégation : que choisit-on pour calculer la distance entre deux classes A et B : la plus grande des distances entre les éléments de A et ceux de B ? La plus petite ? La distance moyennes, la médiane ? 9.1.1 Mise en oeuvre On utilise lenquête dhappydemics sur la période de fin mars. library(lubridate) df&lt;-readRDS(&quot;./data/last.rds&quot;) %&gt;% filter(date2&gt;=make_datetime(year=2022, month=3, day = 19)) n_t&lt;-nrow(df) period&lt;-&quot; apres le 19 mars&quot; Il y a un trick de traitement des données. La question QCM a été encodée en une colonne, ajoutant les chaines de caractère des 16 thématiques avec un séparateurs $ . foo &lt;-as.data.frame(str_split_fixed(df$themes, &quot;\\\\$&quot;,n=3)) # On splite la colonne thème en autant de thème possibles foo1&lt;-cbind(df,foo)%&gt;% rename(V1=23, V2=24, V3=25) %&gt;% dplyr::select(id,V1,V2,V3)%&gt;% pivot_longer(!id,names_to=&quot;rank&quot;,values_to=&quot;theme&quot;)%&gt;% mutate(rank=ifelse(rank==&quot;V1&quot;, 3,ifelse(rank==&quot;V2&quot;, 2, ifelse(rank==&quot;V3&quot;,1, 0)))) %&gt;% #on recode les rangs par un facteur d&#39;importance de à 0 à 3 filter(theme!=&quot;&quot;)%&gt;% mutate(theme=str_trim(theme))%&gt;% mutate(r=as.numeric(rank))%&gt;% dplyr::select(-rank) n1&lt;-nrow(df) # le nombre d&#39;individus n2&lt;-nrow(foo1) #le nombre de mentions Dans une première étape faisons le bilan global #on calcule la proportion et la pénétration des items foo2 &lt;-foo1%&gt;% mutate(m=1)%&gt;% group_by(theme)%&gt;% summarise(frequence=sum(m), proportion=frequence/n2, penetration=frequence/n1) col&lt;-c(&quot;#F1BB7B&quot;, &quot;#FD6467&quot;, &quot;#FD6467&quot;, &quot;#FD6467&quot;, &quot;#5B1A18&quot;, &quot;#5B1A18&quot;, &quot;#5B1A18&quot;, &quot;#F1BB7B&quot;, &quot;#FD6467&quot;, &quot;#F1BB7B&quot;, &quot;#F1BB7B&quot;, &quot;#F1BB7B&quot;, &quot;#F1BB7B&quot;, &quot;#5B1A18&quot;, &quot;#FD6467&quot;, &quot;#F1BB7B&quot;, &quot;#F1BB7B&quot; ) brks&lt;-c(0.1, 0.2, 0.3,0.4,0.5,0.6) ggplot(foo2,aes(x=reorder(theme, frequence), y=penetration))+ geom_bar(stat=&quot;identity&quot;, aes(fill=theme))+ coord_flip()+ scale_fill_manual(values=col)+ labs(title = &quot;Pénétration des thèmes dans la population&quot;, x=NULL, y= &quot;% de la population&quot;, caption = &quot;data @happydemics dataviz @benavent&quot;)+ theme_minimal()+ theme(legend.position = &quot;none&quot;)+ scale_y_continuous(breaks = brks, labels = scales::percent(brks)) ggsave(paste0(&quot;./plot/theme_&quot;,period,&quot;.jpg&quot;),plot=last_plot(), width = 27, height = 17, units = &quot;cm&quot;) 9.2 segmentation simplifiée On commence va reconstruire un tableaux des individus x les thèmes. On garde les rangs comme indicateurs de limportance . foo3&lt;-foo1%&gt;% pivot_wider(names_from=&quot;theme&quot;, values_from=&quot;r&quot;) %&gt;% replace(is.na(.), 0) head(foo3, 8) ## # A tibble: 8 x 17 ## id L&#39;imm~1 Le po~2 L&#39;édu~3 L&#39;éga~4 Les r~5 L&#39;env~6 Le se~7 L&#39;ins~8 La sa~9 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2.36e8 3 0 0 0 0 0 0 0 0 ## 2 2.36e8 0 3 2 1 0 0 0 0 0 ## 3 2.36e8 0 3 0 0 2 0 0 0 0 ## 4 2.36e8 0 0 0 0 3 2 1 0 0 ## 5 2.36e8 0 1 0 0 3 0 2 0 0 ## 6 2.36e8 0 2 3 0 0 1 0 0 0 ## 7 2.36e8 0 0 3 2 0 1 0 0 0 ## 8 2.36e8 1 2 0 0 0 0 0 3 0 ## # ... with 7 more variables: `Le chômage` &lt;dbl&gt;, `L&#39;économie` &lt;dbl&gt;, ## # `La science et la technologie` &lt;dbl&gt;, `L&#39;identité nationale` &lt;dbl&gt;, ## # `La sécurité nationale` &lt;dbl&gt;, `L&#39;union Européenne` &lt;dbl&gt;, ## # `La culture` &lt;dbl&gt;, and abbreviated variable names 1: `L&#39;immigration`, ## # 2: `Le pouvoir d&#39;achat`, 3: `L&#39;éducation`, 4: `L&#39;égalité Homme/Femme`, ## # 5: `Les retraites`, 6: `L&#39;environnement`, 7: `Le service public`, ## # 8: `L&#39;insécurité`, 9: `La santé` On calcule un tableau de distance et on performe la classification automatique. dans cet essai on tente un modèle à 8 groupes. foo4&lt;-foo3[,2:17] #distance d&lt;-dist(foo4) #clustering h.D &lt;- hclust(d, method=&quot;ward.D&quot;) #dendogramme plot(h.D, hang=-1) #identification des clusters rect.hclust(h.D , k = 8, border = 2:6) #attribution des clusters memb &lt;- cutree(h.D, k = 8) #maj du fichier de données avec l&#39;appartenace des individus aux groupes foo5&lt;-cbind(foo4, memb) Il reste à décrire les différents types sur les 16 variables qui les décrivent. On choisit une méthode de barre ordonnée avec un facetting par groupe. foo6&lt;-foo5 %&gt;% group_by(memb) %&gt;% pivot_longer(-memb,names_to=&quot;Thèmes&quot;,values_to=&quot;Valeurs&quot;)%&gt;% group_by(memb,Thèmes)%&gt;% summarise(Valeurs=mean(Valeurs)) foo6$group[foo6$memb==1]&lt;-&quot;multicritère&quot; foo6$group[foo6$memb==2]&lt;-&quot;Santé/Educ&quot; foo6$group[foo6$memb==3]&lt;-&quot;Pouvoir d&#39;achat/nretraites&quot; foo6$group[foo6$memb==5]&lt;-&quot;Immigration/nInsécurité &quot; foo6$group[foo6$memb==4]&lt;-&quot;égalité h/F&quot; foo6$group[foo6$memb==6]&lt;-&quot;Pouvoir d&#39;achat/nSanté&quot; foo6$group[foo6$memb==7]&lt;-&quot;Economie&quot; foo6$group[foo6$memb==8]&lt;-&quot;Environnement&quot; library(scales) brks&lt;-c(0.5,1,1.5,2, 2.5,3) p2&lt;- ggplot(foo6, aes(x=reorder(Thèmes, Valeurs), y=Valeurs))+ geom_bar(stat=&quot;identity&quot;,aes(fill=as.factor(Thèmes)))+ facet_wrap(vars(group), ncol=4)+ coord_flip()+ scale_fill_manual(values=col)+ theme_minimal()+ scale_y_continuous(breaks=brks)+ theme(legend.position = &quot;none&quot;, axis.text=element_text(size=7),axis.text.x=element_text(angle = 45, vjust = 0.5, size=2))+ labs(title = &quot;Profils des segments\\npar importance des thématiques&quot;, x=NULL, y=&quot;importance moyenne (de 0 à 3)&quot;) p2 ggsave(&quot;./plot/g_segment_p2.jpg&quot;,plot=last_plot(), width = 27, height = 17, units = &quot;cm&quot;) library(wesanderson) seg_col&lt;-wes_palette(&quot;Zissou1&quot;, 8, type = &quot;continuous&quot;) n&lt;-nrow(foo5) foo6&lt;-foo5 %&gt;% mutate(n=1) %&gt;% group_by(memb)%&gt;% summarise(freq=sum(n, na.rm=TRUE))%&gt;% mutate( freq=freq/n) foo6$group[foo6$memb==1]&lt;-&quot;multicritère&quot; foo6$group[foo6$memb==2]&lt;-&quot;Santé/Educ&quot; foo6$group[foo6$memb==3]&lt;-&quot;Pouvoir d&#39;achat/nretraites&quot; foo6$group[foo6$memb==5]&lt;-&quot;Immigration/nInsécurité &quot; foo6$group[foo6$memb==4]&lt;-&quot;égalité h/F&quot; foo6$group[foo6$memb==6]&lt;-&quot;Pouvoir d&#39;achat/nSanté&quot; foo6$group[foo6$memb==7]&lt;-&quot;Economie&quot; foo6$group[foo6$memb==8]&lt;-&quot;Environnement&quot; p1&lt;- ggplot(foo6, aes(x=group, y=freq))+ geom_bar(stat=&quot;identity&quot;, aes(fill=group))+ scale_fill_manual(values=seg_col) + theme_minimal()+ labs(title=&quot;Poids des segments&quot;, x=NULL, y=&quot;Proportion&quot;)+ scale_y_continuous(breaks=brks,labels=percent)+ theme(legend.position = &quot;none&quot;) ggsave(&quot;./plot/g_segment_p1.jpg&quot;,plot=last_plot(), width = 27, height = 17, units = &quot;cm&quot;) plot_grid(p1, p2, labels = c(&#39;A&#39;, &#39;B&#39;), label_size = 12, ncol=1,rel_heights = c(1, 2)) ggsave(&quot;./plot/g_segment.jpg&quot;,plot=last_plot(), width = 27, height = 17, units = &quot;cm&quot;) 9.3 tableaux croisés de la typologie et des critères sociaux démos On revient à une approche descriptive, on croisant successivement notre variable typologie avec les critères socio-demo qui ont été mesurés dans lenquête. ( une boucle simplifierait ! ) df&lt;-cbind(df,foo5) df$group[df$memb==1]&lt;-&quot;multicritère&quot; df$group[df$memb==2]&lt;-&quot;Santé/Educ&quot; df$group[df$memb==3]&lt;-&quot;Pouvoir d&#39;achat/nretraites&quot; df$group[df$memb==5]&lt;-&quot;Immigration/nInsécurité &quot; df$group[df$memb==4]&lt;-&quot;égalité h/F&quot; df$group[df$memb==6]&lt;-&quot;Pouvoir d&#39;achat/nSanté&quot; df$group[df$memb==7]&lt;-&quot;Economie&quot; df$group[df$memb==8]&lt;-&quot;Environnement&quot; foo&lt;-df %&gt;% group_by(group, Sensibilité) %&gt;% summarize(n=n())%&gt;% mutate(prop=round(n/sum(n),3), cum=1 - (cumsum(prop)-prop/2)) g01&lt;-ggplot(foo,aes(x=group, y=prop, group=Sensibilité))+ geom_bar(stat=&quot;identity&quot;,aes(y = prop, fill=Sensibilité)) + scale_y_continuous(breaks = brks, labels = scales::percent(brks)) + scale_fill_manual(values=SensiP2) + geom_text(aes(label = prop, y=cum),size=2,color=&quot;white&quot;, vjust = 0.5)+ coord_flip()+ labs(title = &quot;Types d&#39;attentes par sensibilité politique &quot;, x=NULL, y=NULL,)+ theme_bw()+ theme(axis.text.x = element_text(size = 7), legend.text = element_text(size = 7)) ggsave(&quot;./plot/g_segment01.jpg&quot;,plot=last_plot(), width = 27, height = 17, units = &quot;cm&quot;) foo&lt;-df %&gt;% group_by(group, Age) %&gt;% summarize(n=n())%&gt;% mutate(prop=round(n/sum(n),3), cum=1 - (cumsum(prop)-prop/2)) g02&lt;-ggplot(foo,aes(x=group, y=prop, group=Age))+ geom_bar(stat=&quot;identity&quot;,aes(y = prop, fill=Age)) + scale_y_continuous(breaks = brks, labels = scales::percent(brks)) + scale_fill_brewer(palette=&quot;Spectral&quot;) + geom_text(aes(label = prop, y=cum),size=2,color=&quot;white&quot;, vjust = 0.5)+ coord_flip()+ labs(title = &quot;Types d&#39;attentes par classe d&#39;âge &quot;, x=NULL, y=NULL,)+theme_bw() + theme(axis.text.x = element_text(size = 7), legend.text = element_text(size = 7)) ggsave(&quot;./plot/g_segment02.jpg&quot;,plot=last_plot(), width = 27, height = 17, units = &quot;cm&quot;) foo&lt;-df %&gt;% group_by(group, Sexe) %&gt;% summarize(n=n())%&gt;% mutate(prop=round(n/sum(n),3), cum=1 - (cumsum(prop)-prop/2)) g03&lt;-ggplot(foo,aes(x=group, y=prop, group=Sexe))+ geom_bar(stat=&quot;identity&quot;,aes(y = prop, fill=Sexe)) + scale_y_continuous(breaks = brks, labels = scales::percent(brks)) + scale_fill_brewer(palette=&quot;Spectral&quot;) + geom_text(aes(label = prop, y=cum),size=2,color=&quot;white&quot;, vjust = 0.5)+ coord_flip()+ theme_bw()+ labs(title = &quot;Types d&#39;attentes par genre &quot;, x=NULL, y=NULL,)+ theme(axis.text.x = element_text(size = 7), legend.text = element_text(size = 7)) ggsave(&quot;./plot/g_segment03.jpg&quot;,plot=last_plot(), width = 27, height = 17, units = &quot;cm&quot;) foo&lt;-df %&gt;% group_by(group, Education) %&gt;% summarize(n=n())%&gt;% mutate(prop=round(n/sum(n),3), cum=1 - (cumsum(prop)-prop/2)) g04&lt;-ggplot(foo,aes(x=group, y=prop, group=Education))+ geom_bar(stat=&quot;identity&quot;,aes(y = prop, fill=Education)) + scale_y_continuous(breaks = brks, labels = scales::percent(brks)) + scale_fill_brewer(palette=&quot;Spectral&quot;) + geom_text(aes(label = prop, y=cum),size=1.5,color=&quot;white&quot;, vjust = 0.5)+ coord_flip()+theme_bw()+ labs(title = &quot;Types d&#39;attentes par niveau d&#39;éducation &quot;, x=NULL, y=NULL,)+ theme(axis.text.x = element_text(size = 7), legend.text = element_text(size = 7)) ggsave(&quot;./plot/g_segment04.jpg&quot;,plot=last_plot(), width = 27, height = 17, units = &quot;cm&quot;) plot_grid(g01, g02, g04,g03, labels = c(&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;), label_size = 11, ncol=2,rel_widths = c(3, 2)) ggsave(&quot;./plot/g_segment05.jpg&quot;,plot=last_plot(), width = 27, height = 17, units = &quot;cm&quot;) 9.4 AFCM pour une synthèse Cest le bon moment de donner une seconde illustration de lutilité de lAFCM. Pourquoi ne pas synthétiser en une carte lensemble des relations statistiques. library(FactoMineR) library(factoextra) X&lt;-df %&gt;% dplyr::select( group, Age, Sexe, Sensibilité, Situation2) res&lt;-MCA(X, graph =FALSE) foo&lt;-as.data.frame(res$var$coord) %&gt;% rownames_to_column(var=&quot;var&quot;)%&gt;% rename(dim1=2, dim2=3) %&gt;% add_rownames(var = &quot;rowname&quot;) foo$rowname&lt;-as.numeric(foo$rowname) foo&lt;-foo %&gt;% mutate(label=ifelse(rowname&lt;9, &quot;groupe&quot;, &quot;&quot;)) ggplot(foo, aes(x=dim1, y=dim2, label= var) )+ geom_point()+ geom_text(aes(label=var, color=label),size=3)+ theme_bw()+labs( title= &quot;AFCM&quot;) theme(legend.position = &quot;none&quot;) ## List of 1 ## $ legend.position: chr &quot;none&quot; ## - attr(*, &quot;class&quot;)= chr [1:2] &quot;theme&quot; &quot;gg&quot; ## - attr(*, &quot;complete&quot;)= logi FALSE ## - attr(*, &quot;validate&quot;)= logi TRUE 9.4.1 Forces et limites forces : graphiques, complète limites : petite population 9.5 Les méthodes non-hiérarchiques La première dentre elles est la méthode k-means dont le principe est très simple : plutôt que de calculer toutes les distances entre tous les objets, on va se concentrer sur les distances en k group supposés et les n individus. Lhyperparamètre est ici le nombre de groupes 9.5.1 principe 9.5.2 Application 9.5.3 Le problème de la détermination du nombre optimal de groupe méthode du coude méthode silhouette gap statistics 9.6 Autres méthodes de nombreuses variantes sont disponibles mediane kernel les méthodes fuzzy : lappartenance nest pas exclusive mais probabilistique les méthode de classes latentes les méthodes de densités sappuie sur lidée que la continuité dun groupe sexprime en terme s de densités ** paramètriques ** non - paramètriques http://www.sthda.com/english/wiki/wiki.php?id_contents=7940 : https://link.springer.com/referenceworkentry/10.1007%2F978-0-387-30164-8_211 9.7 Conclusion References "],["régression.html", "Chapitre 10 9 - Régression 10.1 Une étude de cas : les offres Blablacar 10.2 Notes, prix et taux doccupations 10.3 Analyse des prix 10.4 Analyser la demande : quest ce qui détermine le taux doccupation ? 10.5 Autres modèles", " Chapitre 10 9 - Régression 10.1 Une étude de cas : les offres Blablacar Cest un jeu de données scrappées sur un site de covoiturage constitué de 13000 offres proposées sur différents types de trajet à un moment donné (au cours de 2016). Le but va être de tester leffet des signaux de qualité sur les taux de réservation, et de mieux comprendre la nature dune plateforme qui étant de fait une place de marché se propose dêtre collaborative. Si elles ne sont plus dactualité, elle permettent cependant une étude intéressante des facteurs qui encourage la demande dans un moment où le modèle DREAMS de Blablacar était mis en avant pour résoudre le problème de la confiance dans le monde digital : comment échanger avec des inconnus. df &lt;- read_delim(&quot;./data/covoit.csv&quot;, &quot;;&quot;, escape_double = FALSE, trim_ws = TRUE) df&lt;- df%&gt;% dplyr::rename(Places=`Places Restantes`, Depart=`Ville Depart Capture`, Arrivee=`Ville Arrivee Capture`, Flex=`Flexibilite Horaire` ) df&lt;-df %&gt;% filter( Distance&lt;1200) #on recode le nombre de places restantes par une approximation du taux de réservation df$Occup&lt;- NA df$Occup[df$Places==0]&lt;-1 df$Occup[df$Places==1]&lt;-.75 df$Occup[df$Places==2]&lt;-.50 df$Occup[df$Places==3]&lt;-.25 df$Occup[df$Places==4]&lt;-0 df$Occup[df$Places&gt;=4]&lt;-0 #l&#39;expérience peut se capter par le nombre de voyages df$Nombre[is.na(df$Nombre)]&lt;-0 Voici la liste des requêtes et le nombre doffres obtenues pour chacune delle. On observe la domination des trajet inter-régionaux avec Nantes-Rennes, Toulouse-Montpellier et Bordeaux-Toulouse. Les trajets ont été échantillonnés pour représenter différents niveaux déchelle de distance, les courts trajets, les trajets de 100 à 200 km, ceux à 300-400 et une minorité de distances plus longues, ce qui explique la concentration à certains niveaux. La distribution ne représente pas la distribution de la demande des trajets ou de loffre, mais un pool doffres obtenus pour une séries de requête relatives à un trajet donné, un jour donné. trajet&lt;-table(df$Depart, df$Arrivee) df$trajet&lt;-paste0(df$Depart,&quot;-&quot;,df$Arrivee) foo&lt;-df %&gt;% mutate(n=1)%&gt;% group_by(trajet)%&gt;% summarise(n=sum(n))%&gt;%filter(n&gt;0) g01&lt;-ggplot(foo,aes(x=reorder(trajet,n), y=n))+ geom_bar(stat=&quot;identity&quot;, fill=&quot;coral1&quot;)+ coord_flip()+ labs(title=&quot;Trajets les plus fréquents&quot;, x=NULL, y=&quot;Fréquence&quot;) g02&lt;-ggplot(df, aes(x=Distance))+ geom_histogram(fill=&quot;Chartreuse4&quot;,binwidth = 25) + labs(title = &quot;Distribution de la distance des trajets&quot;, y=&quot;Fréquence&quot;, x= &quot;Distance (en km)&quot;) plot_grid( g01, g02, labels = &quot;AUTO&quot;, ncol = 2 ) 10.1.1 Les caractérisques de loffre loffre se détermine par plusieurs éléments le véhicule le conducteur les conditions du trajets 10.1.1.1 Le véhicule Rôle de la marque et du standing du véhicule mérite plus de recodage. Il est très subjectif, ici on privilégie les origines nationales qui expriment un style, un esprit dautomobile. df$Marque[df$Marque==&quot;Alfa-romeo&quot;]&lt;-&quot;Alfa Romeo, Lancia&quot; df$Marque[df$Marque==&quot;Alfa Romeo&quot;]&lt;-&quot;Alfa Romeo, Lancia&quot; df$Marque[df$Marque==&quot;Lancia&quot;]&lt;-&quot;Alfa Romeo, Lancia&quot; df$Marque[df$Marque==&quot;Volvo&quot;]&lt;-&quot;Volvo, Saab&quot; df$Marque[df$Marque==&quot;Zx&quot;]&lt;-&quot;Citroen&quot; df$Marque[df$Marque==&quot;Saab&quot;]&lt;-&quot;Volvo, Saab&quot; df$Marque[df$Marque==&quot;Audi-quattro&quot;]&lt;-&quot;Audi&quot; df$Marque[df$Marque==&quot;Mercedes&quot;]&lt;-&quot;Mercedes-benz&quot; df$Marque[df$Marque==&quot;Vw&quot;]&lt;-&quot;Volkswagen&quot; df$Marque[df$Marque==&quot;Kia&quot;]&lt;-&quot;Autres Asie&quot; df$Marque[df$Marque==&quot;Honda&quot;]&lt;-&quot;Autres Asie&quot; df$Marque[df$Marque==&quot;Mazda&quot;]&lt;-&quot;Autres Asie&quot; df$Marque[df$Marque==&quot;Suzuki&quot;]&lt;-&quot;Autres Asie&quot; df$Marque[df$Marque==&quot;Isuzu&quot;]&lt;-&quot;Autres Asie&quot; df$Marque[df$Marque==&quot;Mitsubishi&quot;]&lt;-&quot;Autres Asie&quot; df$Marque[df$Marque==&quot;Lexus&quot;]&lt;-&quot;Autres Asie&quot; df$Marque[df$Marque==&quot;Subaru&quot;]&lt;-&quot;Autres Asie&quot; df$Marque[df$Marque==&quot;Daewoo&quot;]&lt;-&quot;Autres Asie&quot; df$Marque[df$Marque==&quot;Huanghai&quot;]&lt;-&quot;Autres Asie&quot; df$Marque[df$Marque==&quot;Land&quot;]&lt;-&quot;Rover, jaguar, mini&quot; df$Marque[df$Marque==&quot;Rover&quot;]&lt;-&quot;Rover, jaguar, mini&quot; df$Marque[df$Marque==&quot;Jaguar&quot;]&lt;-&quot;Rover, jaguar, mini&quot; df$Marque[df$Marque==&quot;Abarth&quot;]&lt;-&quot;Rover, jaguar, mini&quot; df$Marque[df$Marque==&quot;Ldv&quot;]&lt;-&quot;Rover, jaguar, mini&quot; df$Marque[df$Marque==&quot;Austin&quot;]&lt;-&quot;Rover, jaguar, mini&quot; df$Marque[df$Marque==&quot;Mini&quot;]&lt;-&quot;Rover, jaguar, mini&quot; df$Marque[df$Marque==&quot;Chevrolet&quot;]&lt;-&quot;Autres US&quot; df$Marque[df$Marque==&quot;Chrysler&quot;]&lt;-&quot;Autres US&quot; df$Marque[df$Marque==&quot;Jeep&quot;]&lt;-&quot;Autres US&quot; df$Marque[df$Marque==&quot;Dodge&quot;]&lt;-&quot;Autres US&quot; df$Marque[df$Marque==&quot;Lamborghini&quot;]&lt;-&quot;Sport&quot; df$Marque[df$Marque==&quot;Maserati&quot;]&lt;-&quot;Sport&quot; df$Marque[df$Marque==&quot;Porsche&quot;]&lt;-&quot;Sport&quot; df$Marque[df$Marque==&quot;Ac&quot;]&lt;-&quot;Autres&quot; df$Marque[df$Marque==&quot;Acura&quot;]&lt;-&quot;Autres&quot; df$Marque[df$Marque==&quot;eacute&quot;]&lt;-&quot;Autres&quot; df$Marque[df$Marque==&quot;Iveco&quot;]&lt;-&quot;Autres&quot; df$Marque[df$Marque==&quot;Camping-car&quot;]&lt;-&quot;Autres&quot; df$Marque[df$Marque==&quot;Infiniti&quot;]&lt;-&quot;Autres&quot; df$Marque[df$Marque==&quot;Admiral&quot;]&lt;-&quot;Autres&quot; df$Marque[df$Marque==&quot;Sport&quot;]&lt;-&quot;Autres&quot; foo&lt;-df %&gt;% group_by(Marque)%&gt;% summarise(n=n())%&gt;% drop_na() ggplot(foo, aes(x=reorder(Marque, n), y=n))+ geom_bar(stat=&quot;identity&quot;,fill=&quot;Chartreuse4&quot;) + coord_flip() + scale_y_log10()+ labs(title= &quot;Distribution des offres par marque&quot;, x=NULL) 10.1.1.2 Lâge et lexpérience du capitaine On distingue deux populations en terme dâge, une en dessous de la trentaine, lautre de 40 à 50 ans. On Jette un coup doeil ensuite sur la relation entre lâge et la note qui culmine à 30 ans et baisse avec les décades. Est-ce leffet dune inadéquation des âges? La demande est-elle plus jeune que loffre? Cela crée-t-il un biais systématique dévaluation? Le conducteur se manifeste au travers de 3 critères : le statut attribué par blablacar, son expérience traduite par le nombre de voyages quil a fait ( et pour lesquels il a été évalué). La note moyenne obtenue des passagers. g03&lt;-ggplot(df, aes(x=Age))+ geom_histogram(fill=&quot;Chartreuse4&quot;, binwidth = 2)+ labs(title=&quot;Distribution par \\nâge&quot;, y=&quot;Fréquence des offres&quot;)+xlim (18, 80) df$Statut&lt;-as.factor(df$Statut) df$Statut &lt;- factor(df$Statut, ordered = TRUE, levels = c(&quot;Ambassadeur&quot;, &quot;Expert&quot;, &quot;Habitue&quot;, &quot;Confirme&quot;,&quot;Pas de Statut&quot;)) g04&lt;-ggplot(df, aes(x=Statut))+ geom_bar(fill=&quot;Chartreuse4&quot;) + coord_flip()+ labs(title = &quot;Statut des \\nconducteurs&quot;, y=&quot;Fréquence des offres&quot;) mean&lt;-round(mean(df$Nombre,na.rm=TRUE),2) g05&lt;-ggplot(df, aes(x=Nombre))+ geom_histogram(fill=&quot;Chartreuse4&quot;, binwidth = 5)+ labs(title = &quot;Distribution du nombre \\nde trajets&quot;, y=&quot; Fréquence des offres&quot;, subtitle =paste0(&quot;Moyenne=&quot;, mean), x=&quot;nombre de voyages&quot;)+ xlim(0,250) plot_grid( g03, g05,g04, labels = &quot;AUTO&quot;, ncol = 3 ) Lexpérience se distribue de manière très inégale, une minorité de conducteurs ayant réalisé plus de 50 voyages, une très grande majorité en ayant fait moins dune vingtaine de voyages. Examinons le statut et notamment en comparant lexpérience et lévaluation. foo&lt;-df %&gt;% group_by(Statut)%&gt;% summarise(Note=mean(Note,na.rm=TRUE), Nombre=mean(Nombre,na.rm=TRUE), Age=round(mean(Age,na.rm=TRUE),0))%&gt;% pivot_longer(-Statut,names_to = &quot;variable&quot;,values_to = &quot;Moyenne&quot; ) ggplot(foo, aes(x=Statut, y=Moyenne,group=variable))+ geom_line(aes(color=variable), size=2)+ coord_flip()+ facet_wrap(vars(variable),scales =&quot;free&quot;, ncol=3)+ labs(title = &quot;Statuts des conducteur&quot;, x=NULL, y=NULL)+ scale_color_manual(values=c(&quot;skyblue4&quot;, &quot;coral2&quot;, &quot;Gold2&quot;)) 10.1.1.3 Le trajet Le conducteur peut être flexible dans lhoraire de départ ou pas. Un recodage est cependant nécessaire.Le trajet peut être strict ou comporter des détours. Là aussi besoin dun peu de recodage. df$Flex[df$Flex!=&quot;Depart pile a l&#39;heure&quot; &amp; df$Flex!=&quot;Pas d&#39;indication&quot;]&lt;-&quot;Voir avec le conducteur&quot; df$Flex[is.na(df$Flex)]&lt;-&quot;Pas d&#39;indication&quot; g07&lt;-ggplot(df, aes(x=Flex))+ geom_bar(fill=&quot;Chartreuse4&quot;)+ coord_flip()+ labs(title=&quot;Flexibilité de l&#39;horaire de départ&quot;, x=NULL, y=&quot;Fréquence&quot;) df$Detour[df$Detour==&quot;De 30 minutes max&quot;]&lt;-&quot;Plus de 15 mn&quot; df$Detour[df$Detour==&quot;Autant que possible&quot;]&lt;-&quot;Plus de 15 mn&quot; g08&lt;-ggplot(df, aes(x=Detour))+geom_bar(fill=&quot;Chartreuse4&quot;) + coord_flip()+labs(title=&quot;Acceptation des détours à l&#39;arrivée&quot;, x=NULL) plot_grid( g07, g08, labels = &quot;AUTO&quot;, ncol = 2 ) 10.2 Notes, prix et taux doccupations Les notes sont en moyenne de 4.45 et fortement déviées à droite, réduisant la capacité de discrimination. mean&lt;-round(mean(df$Note,na.rm=TRUE),2) ggplot(df, aes(x=Note))+ geom_histogram(fill=&quot;coral2&quot;,binwidth = .2)+ theme_bw() + labs(title = &quot;Distribution des notes des conducteurs&quot;, subtitle =paste0(&quot;Moyenne=&quot;, mean)) 10.3 Analyse des prix Au premier examen la distribution des prix semble être multimodale, elle est étroitement associée à la distribution des distance et de notre stratégie déchantillonage des paires départ/destination. Le prix est une multiplication de la distance par un tarif kilométrique, même si la relation ne semble pas tout à faire linéaire. La convexité de la courbe signale une sorte de rendement croissant avec la distance (incorporation du prix des péages ou effet de rareté ?). Cest pourquoi on calcule un tarif au km, qui lui nest plus corrélé ou à peine à la distance parcourue. Voici les résultats principaux. g20&lt;-ggplot(df, aes(x=Prix))+geom_bar(fill=&quot;firebrick2&quot;) + labs(title = &quot;Distribution des prix&quot;, x=&quot;Prix du trajet&quot;) g21&lt;-ggplot(df, aes(x=Distance, y=Prix))+ geom_point(color=&quot;firebrick2&quot;, alpha =0.5) + geom_smooth(method=&quot;gam&quot;)+scale_x_log10()+ labs(title = &quot;Corrélation des distances et des prix&quot;, x= &quot;Distance en km&quot;) df$prix_km&lt;-df$Prix/df$Distance g22&lt;-ggplot(df, aes(x=prix_km))+ geom_histogram(fill=&quot;firebrick3&quot;, binwidth = 0.005) + labs(title = &quot;Distribution des prix au km&quot;, x=&quot;Prix au km (euros)&quot;)+xlim(0,0.15) g23&lt;-ggplot(df, aes(x=Distance, y=prix_km))+ geom_point(color=&quot;firebrick3&quot;) + geom_smooth(method=&quot;gam&quot;)+ scale_x_log10()+labs(x=&quot;Distance en km&quot;)+ ylim(0,0.15) plot_grid( g20, g21,g22,g23, labels = &quot;AUTO&quot;, ncol = 2 ) 10.4 Analyser la demande : quest ce qui détermine le taux doccupation ? On utilise la variable nombre de place restante quon traduit par un indicateur codé de 0 (tout est libre) à 1 (la voiture est pleine). En voici la distribution. ggplot(df, aes(x=Occup))+ geom_bar(fill=&quot;gold3&quot;) + labs(title=&quot;Distribution du taux d&#39;occupation des véhicules&quot;, y=&quot;Nombre d&#39;offres&quot;, x= &quot;Taux d&#39;occupation&quot;) 10.4.1 Un modèle OLS On commence par un modèle simple et linéaire où lon cherche à expliquer, prédire, le taux doccupation du véhicule en fonction des variables dont nous disposons. La flexibilité de lhoraire et la possibilité de détour naffecte pas vraiment le taux de réservation. Si les autres variables ont des relations significatives ( avec des valeur t très élevées), on notera que la variance expliquée est faible. Le remplissage des voiture est une affaire de loto. Ce qui se comprend, car la probabilité quune offre et une demande coïncide est relativement faible si on retient une plage horaire étroite pour le passager qui lélargira pour accroître le choix au prix dun effort de recherche supplémentaire. df$AgeClasse&lt;- round(df$Age/10,0)*10 df$AgeClasse[df$AgeClasse==80]&lt;-70 df$AgeClasse[df$AgeClasse==90]&lt;-70 df$AgeClasse[df$AgeClasse==100]&lt;-70 df$AgeClasse&lt;-as.factor(df$AgeClasse) fit0&lt;-lm(Occup~AgeClasse+log(Distance+1)+prix_km+Flex+Detour+Autoroute+log(Nombre+1)+Note, data=df) fit1&lt;-lm(Occup~AgeClasse+log(Distance+1)+prix_km+Flex+Detour+Autoroute+log(Nombre+1)*Note, data=df) fit_logit&lt;-lm(Occup~AgeClasse+ Marque+log(Distance+1)+prix_km+Flex+Detour+Autoroute+log(Nombre+1)*Note, data=df) export_summs(fit0, fit1,plot.distributions = TRUE,scale = FALSE, digits = 3) Table 10.1: Model 1Model 2 (Intercept)0.217 ***0.495 *** (0.053)&nbsp;&nbsp;&nbsp;(0.080)&nbsp;&nbsp;&nbsp; AgeClasse30-0.007&nbsp;&nbsp;&nbsp;&nbsp;-0.007&nbsp;&nbsp;&nbsp;&nbsp; (0.007)&nbsp;&nbsp;&nbsp;(0.007)&nbsp;&nbsp;&nbsp; AgeClasse40-0.054 ***-0.053 *** (0.008)&nbsp;&nbsp;&nbsp;(0.008)&nbsp;&nbsp;&nbsp; AgeClasse50-0.039 ***-0.038 *** (0.009)&nbsp;&nbsp;&nbsp;(0.009)&nbsp;&nbsp;&nbsp; AgeClasse60-0.018&nbsp;&nbsp;&nbsp;&nbsp;-0.017&nbsp;&nbsp;&nbsp;&nbsp; (0.010)&nbsp;&nbsp;&nbsp;(0.010)&nbsp;&nbsp;&nbsp; AgeClasse70-0.024&nbsp;&nbsp;&nbsp;&nbsp;-0.023&nbsp;&nbsp;&nbsp;&nbsp; (0.023)&nbsp;&nbsp;&nbsp;(0.023)&nbsp;&nbsp;&nbsp; log(Distance + 1)0.029 ***0.027 *** (0.006)&nbsp;&nbsp;&nbsp;(0.006)&nbsp;&nbsp;&nbsp; prix_km-3.896 ***-3.910 *** (0.386)&nbsp;&nbsp;&nbsp;(0.385)&nbsp;&nbsp;&nbsp; FlexPas d'indication0.012 *&nbsp;&nbsp;0.012 *&nbsp;&nbsp; (0.006)&nbsp;&nbsp;&nbsp;(0.006)&nbsp;&nbsp;&nbsp; FlexVoir avec le conducteur0.012&nbsp;&nbsp;&nbsp;&nbsp;0.013&nbsp;&nbsp;&nbsp;&nbsp; (0.025)&nbsp;&nbsp;&nbsp;(0.025)&nbsp;&nbsp;&nbsp; DetourDe 15 minutes max-0.004&nbsp;&nbsp;&nbsp;&nbsp;-0.004&nbsp;&nbsp;&nbsp;&nbsp; (0.006)&nbsp;&nbsp;&nbsp;(0.006)&nbsp;&nbsp;&nbsp; DetourPlus de 15 mn-0.065 *&nbsp;&nbsp;-0.064 *&nbsp;&nbsp; (0.027)&nbsp;&nbsp;&nbsp;(0.027)&nbsp;&nbsp;&nbsp; AutorouteOui0.119 ***0.121 *** (0.016)&nbsp;&nbsp;&nbsp;(0.016)&nbsp;&nbsp;&nbsp; log(Nombre + 1)-0.010 ***-0.152 *** (0.002)&nbsp;&nbsp;&nbsp;(0.030)&nbsp;&nbsp;&nbsp; Note0.062 ***0.002&nbsp;&nbsp;&nbsp;&nbsp; (0.008)&nbsp;&nbsp;&nbsp;(0.015)&nbsp;&nbsp;&nbsp; log(Nombre + 1):Note&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.032 *** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0.007)&nbsp;&nbsp;&nbsp; N11719&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;11719&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; R20.027&nbsp;&nbsp;&nbsp;&nbsp;0.029&nbsp;&nbsp;&nbsp;&nbsp; *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05. plot_summs(fit0, fit1,plot.distributions = TRUE, omit.coefs=c(&quot;MarqueAudi&quot;, &quot;MarqueAutres&quot;, &quot;MarqueAutres Asie&quot; , &quot;MarqueAutres US&quot;, &quot;MarqueBmw&quot;, &quot;MarqueCitroen&quot; , &quot;MarqueDacia&quot;,&quot;MarqueFiat&quot;, &quot;MarqueFord&quot;, &quot;MarqueHyundai&quot;, &quot;MarqueMercedes-benz&quot;,&quot;MarqueNissan&quot;,&quot;MarqueOpel&quot;, &quot;MarquePeugeot&quot;,&quot;MarqueRenault &quot;, &quot;MarqueRover, jaguar, mini&quot;, &quot;MarqueSeat&quot;, &quot;MarqueSkoda&quot;,&quot;MarqueToyota&quot;,&quot;MarqueVolkswagen&quot;,&quot;MarqueVolvo&quot;, &quot;Saab&quot;)) Le modèle tient même sil explique très peu de variance. Les tests sont cependant significatifs et même si leffet est faible il est notable. Voici des diagrammes deffets qui représentent la valeur de la variable dépendante pour une plage de variation des variables indépendantes prises une à une, en fonction du modèle. On voit ainsi plus clairement que le taux doccupation passe de 20% environ quand la note est de 3 (les notes inférieures sont rares!) à 45% quand la moyenne du conducteur frôle les 5. g10&lt;-effect_plot(fit1, pred=AgeClasse,interval = TRUE ,data=df)+ geom_line(color=&quot;gold3&quot;,size=2)+ labs(title=&quot;Effet de l&#39;âge sur le taux d&#39;occupation&quot;, x=&quot;Classe d&#39;âge&quot;, y=&quot;Taux d&#39;occupation&quot; ) g11&lt;-effect_plot(fit1, pred=Flex,interval = TRUE ,data=df)+ geom_line(color=&quot;gold3&quot;,size=2)+ labs(title=&quot;Effet de la flexibilité au départ&quot;, y=&quot;Taux d&#39;occupation&quot;) g12&lt;-effect_plot(fit1, pred=Autoroute,interval = TRUE ,data=df)+ geom_line(color=&quot;gold3&quot;,size=2)+ labs(title=&quot;Effet de l&#39;autoroute&quot;, y=&quot;Taux d&#39;occupation&quot;) g13&lt;-effect_plot(fit1, pred=Detour, interval = TRUE ,data=df)+ geom_line(color=&quot;gold3&quot;,size=2)+ labs(title=&quot;Effet des conditions d&#39;arrivée&quot;, x=&quot;Détour&quot;, y=&quot;Taux d&#39;occupation&quot;) plot_grid( g10, g11,g12,g13, labels = &quot;AUTO&quot;, ncol = 2 ) g16&lt;-effect_plot(fit1, pred=Note,interval = TRUE ,data=df)+ geom_line(color=&quot;gold3&quot;,size=2)+ labs(title=&quot;Effet de la note&quot;, y=&quot;Taux d&#39;occupation&quot;) g17&lt;-effect_plot(fit1, pred=Nombre,interval = TRUE ,data=df)+ geom_line(color=&quot;gold3&quot;,size=2)+ labs(title=&quot;Effet du nombre de trajets&quot;, y=&quot;Taux d&#39;occupation&quot;) g18&lt;-effect_plot(fit1, pred=Distance,interval = TRUE ,data=df)+ geom_line(color=&quot;gold3&quot;,size=2)+ labs(title=&quot;Effet de la distance&quot;, y=&quot;Taux d&#39;occupation&quot;) g19&lt;-interact_plot(fit1, pred=Nombre,modx=Note,interval = TRUE ,data=df)+ geom_line(color=&quot;gold3&quot;,size=2)+ labs(title=&quot;Effet d&#39;interaction nombre de trajets x note&quot;, y=&quot;Taux d&#39;occupation&quot;) plot_grid( g16, g17,g18,g19, labels = &quot;AUTO&quot;, ncol = 2 ) La particularité du modèle est le terme dinteraction. Lexpérience du conducteur traduite par le nombre de trajets quil a effectué est gage de sécurité, de confiance, pourvu que ses notes soient bonnes. On sattend à ce que si elle sont moins bonnes, le nombre de voyages réalisés en amplifiera limpact négatif. Lexpérience signale aussi la crédibilité de la note, et on peut raisonnablement penser que plus ce nombre est grand et plus le signal, positif ou négatif est crédible. Ici la note module leffet de crédibilité. Lanalyse de linteraction est claire : quand les contenus sont négatifs , lexpérience du conducteur aggrave la réticence et le taux doccupation se réduit. Quand les notes vont meilleures que la moyenne.Lamplification par lexpérience de leffet des notes sur la réservation est moindre. Linteraction est significative (t=8.76). Reste à comprendre pourquoi la tendance générale reste à une relation négative entre le taux de remplissage et le nombre de trajet effectué . Est-ce le résultat dune participation fréquente à la plateforme, à un comportement particulier des conducteurs très actifs (opportunisme) ? Une explication complémentaire peut venir de ceux qui pratique occasionnellement le covoiturage, leur activité étant plus rare , elle est peut être plus planifiée, et les offres sont présentes depuis plus longtemps que celles des utilisateurs fréquents qui publieraient leurs annonces de la veille au lendemain. Ceci mérite une analyse plus approfondie que nous ne ménerons pas ici. 10.5 Autres modèles Dans la pratique la construction dun modèle de régression dépend de trois éléments : la spécification fonctionnelle qui définit f(y, X) autrement dit ce qui relie ce quon observe à ce qui peut lui être corrélé de manière linéaires ou moins linéaires. La forme fonctionnelle dépend largement de la variable dépendante et de sa distribution : ** normale : ** log-normale voire exponentielle ** binaire ** proportionnelle ** dénombrement. Il sagit des données de comptages : un nombre dachat au cours dune période par exemple. poisson, NBD ** Les durées sont positives, Dans ce cas on va utiliser une méthode de modèle linéaire généralisé (GLM) qui reposent sur 3 éléments: Un prédicteur linéaire \\[\\eta\\] qui décrit la combinaison linéaire des variables explicatives. \\[\\eta=\\sum_{n=1}^{10}\\beta_{0}+\\beta_{i}x_{i}\\] Une fonction de lien : Contrairement aux modèles linéaires classiques, les valeurs prédites par le prédicteur linéaire ne correspondent pas à la prédiction moyenne dune observation, mais à une transformation mathématique. Les beta sont estimés après transformation des réponses selon la fonction de lien choisie. \\[g(\\mu_{y})=\\eta\\] Par exemple, pour les données de comptage : \\[log(\\mu_{y})=\\eta\\] ou dans le cas de données binaire ( modèle de régression logistique) \\[log(\\frac{\\mu_{y}}{1-\\mu_{y}})=\\eta\\] Le but de la fonction de lien est de contraindre les valeurs prédites à être dans léchelle des valeurs observées. Ainsi, dans le cas des données de comptage, qui sont obligatoirement positives, ou nulles, la fonction de lien log contraint les valeurs prédites par le prédicteur linéaire à devenir également positives ou nulles après lemploi de la fonction inverse du log. La structure derreur : A une fonction de lien donnée, correspond généralement une structure derreur particulière.Il sagit dune famille de distribution des erreurs. Par exemple, pour les données de comptage, la fonction de lien est le log et la structure derreur correspondante est la distribution de Poisson. Cette structure derreur, permet notamment de spécifier correctement la relation entre la moyenne et la variance. Cette relation est utilisée par lapproche de maximum de vraisemblance pour estimer les coefficients des paramètres (les beta) du GLM. Ici un tableau récapitulatif des structures derreurs, fonctions de lien, fonctions de moyennes et fonctions de variance des données de type numériques continues non bornée, de comptage et binaire. Estimation par une méthode de maximum de vraisemblance et de déviance qui est en quelque sorte une généralisation de la variance. 10.5.1 Régression logistique Sur les même données mais en considérant le taux doccupation plus sérieusement : il est compris entre 0 et 1 et se prête don à un modèle logistique, car le taux doccupation va de 0 à 100% même si nous navons que 4 niveaux. ça fait un modèle plus réaliste qui ajuste la valeur prédite entre 0 et 1. Pour mieux prendre en compte lhétérogénéité du set de données (composée par un échantillon de requêtes) on introduit les trajets comme composante aléatoire. lestimation est réalisée avec lme4 Les effets semblent être ici plus pertinent et sensibles mais surtout leffet dinteraction semble renforcé. Leffet de la note est là clairement amplifié pour les notes négatives mais aussi pour les notes positives. fit_logit&lt;-glm(Occup~Age+log(Distance+1)+prix_km+Flex+Detour+Autoroute+log(Nombre+1)*Note,family=&quot;binomial&quot;, data=df) summ(fit_logit) Observations 11719 (1586 missing obs. deleted) Dependent variable Occup Type Generalized linear model Family binomial Link logit ²(11) 106.04 Pseudo-R² (Cragg-Uhler) 0.02 Pseudo-R² (McFadden) 0.01 AIC 15784.58 BIC 15873.00 Est. S.E. z val. p (Intercept) 0.07 0.56 0.13 0.90 Age -0.00 0.00 -2.61 0.01 log(Distance + 1) 0.11 0.04 2.79 0.01 prix_km -16.30 2.72 -6.00 0.00 FlexPas dindication 0.05 0.04 1.20 0.23 FlexVoir avec le conducteur 0.05 0.17 0.29 0.77 DetourDe 15 minutes max -0.01 0.04 -0.24 0.81 DetourPlus de 15 mn -0.27 0.19 -1.39 0.16 AutorouteOui 0.50 0.11 4.50 0.00 log(Nombre + 1) -0.64 0.21 -2.97 0.00 Note 0.00 0.11 0.01 0.99 log(Nombre + 1):Note 0.13 0.05 2.77 0.01 Standard errors: MLE g10&lt;-effect_plot(fit_logit, pred=AgeClasse,interval = TRUE ,data=df)+ geom_line(color=&quot;gold3&quot;,size=2)+ labs(title=&quot;Effet de l&#39;âge sur le taux d&#39;occupation&quot;,x=&quot;Classe d&#39;âge&quot;, y=&quot;Taux d&#39;occupation&quot;) g11&lt;-effect_plot(fit_logit, pred=Flex,interval = TRUE ,data=df)+ geom_line(color=&quot;gold3&quot;,size=2)+ labs(title=&quot;Effet de la flexibilité au départ sur le taux d&#39;occupation&quot;, y=&quot;Taux d&#39;occupation&quot;) g12&lt;-effect_plot(fit_logit, pred=Autoroute,interval = TRUE ,data=df)+ geom_line(color=&quot;gold3&quot;,size=2)+ labs(title=&quot;Effet de l&#39;autoroute sur le taux d&#39;occupation&quot;, y=&quot;Taux d&#39;occupation&quot;) g13&lt;-effect_plot(fit_logit, pred=Detour, interval = TRUE ,data=df)+ geom_line(color=&quot;gold3&quot;,size=2)+ labs(title=&quot;Effet des conditions d&#39;arrivée sur le taux d&#39;occupation&quot;, x=&quot;Détour&quot;, y=&quot;Taux d&#39;occupation&quot;) plot_grid( g10, g11,g12,g13, labels = &quot;AUTO&quot;, ncol = 2 ) g16&lt;-effect_plot(fit_logit, pred=Note,interval = TRUE ,data=df)+ geom_line(color=&quot;gold3&quot;,size=2)+ labs(title=&quot;Effet de la note&quot;, y=&quot;Taux d&#39;occupation&quot;) g17&lt;-effect_plot(fit_logit, pred=Nombre,interval = TRUE ,data=df)+ geom_line(color=&quot;gold3&quot;,size=2)+ labs(title=&quot;Effet du nombre de trajets&quot;, y=&quot;Taux d&#39;occupation&quot;) g18&lt;-effect_plot(fit_logit, pred=Distance,interval = TRUE ,data=df)+ geom_line(color=&quot;gold3&quot;,size=2)+ labs(title=&quot;Effet de la distance&quot;, y=&quot;Taux d&#39;occupation&quot;) g19&lt;-interact_plot(fit_logit, pred=Nombre,modx=Note,interval = TRUE ,data=df)+ geom_line(color=&quot;gold3&quot;,size=2)+ labs(title=&quot;Effet d&#39;interaction nombre de trajets x note&quot;, y=&quot;Taux d&#39;occupation&quot;) plot_grid( g16, g17,g18,g19, labels = &quot;AUTO&quot;, ncol = 2 ) 10.5.2 Modèle de comptage et un dernier qui revient aux données originelles : le comptage des place disponibles. Dans ce type de situation (données de comptage), on considère que ce type de variable se distribue selon une loi de poisson, dont la propriété est légalité de la moyenne et de lécart-type. On sassure quil ny ait pas de surdispersion en calculant le ratio de la variance résiduelle par le nombre de degré de liberté. Il est ici de lordre de 1 et donc on conclura à une absence de surdispersion, même si en comparant la distribution empiriques du nombre de places restantes à la distribution théorique dune loi de poisson, lajustement nest pas parfait. mean_places&lt;-mean(df$Places) mean_sd&lt;-sd(df$Places) set.seed(1234) # permet de simuler toujours les mêmes comptages. theoretic_count &lt;-rpois(nrow(df),mean_places) # on incorpore ces comptages théoriques dans un data frame tc_df &lt;-data.frame(theoretic_count) ggplot(df,aes(Places))+ geom_bar(fill=&quot;#1E90FF&quot;, alpha=0.5)+ geom_bar(data=tc_df, aes(theoretic_count,fill=&quot;#1E90FF&quot;, alpha=0.5))+ theme_classic()#+ theme(legend.position=&quot;none&quot;) df$Occup[df$Occup==1]&lt;-.999 df$Occup[df$Occup==0]&lt;-.001 fit_poisson &lt;- glm(Places~Age+log(Distance+1)+prix_km+Flex+Detour+Autoroute+log(Nombre+1)*Note, family=&quot;poisson&quot;,data = df) summary(fit_poisson) ## ## Call: ## glm(formula = Places ~ Age + log(Distance + 1) + prix_km + Flex + ## Detour + Autoroute + log(Nombre + 1) * Note, family = &quot;poisson&quot;, ## data = df) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.51367 -0.74448 0.03874 0.60289 2.13066 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.6879101 0.1897188 3.626 0.000288 *** ## Age 0.0019560 0.0005095 3.839 0.000123 *** ## log(Distance + 1) -0.0513430 0.0137967 -3.721 0.000198 *** ## prix_km 7.6872057 0.9123308 8.426 &lt; 2e-16 *** ## FlexPas d&#39;indication -0.0236138 0.0140880 -1.676 0.093704 . ## FlexVoir avec le conducteur -0.0109685 0.0595949 -0.184 0.853974 ## DetourDe 15 minutes max 0.0023592 0.0137864 0.171 0.864125 ## DetourPlus de 15 mn 0.1349013 0.0620044 2.176 0.029580 * ## AutorouteOui -0.2434932 0.0347369 -7.010 2.39e-12 *** ## log(Nombre + 1) 0.2883459 0.0713807 4.040 5.36e-05 *** ## Note -0.0088294 0.0362270 -0.244 0.807444 ## log(Nombre + 1):Note -0.0592471 0.0159474 -3.715 0.000203 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 11105 on 11718 degrees of freedom ## Residual deviance: 10889 on 11707 degrees of freedom ## (1586 observations deleted due to missingness) ## AIC: 38248 ## ## Number of Fisher Scoring iterations: 5 fit_poisson2 &lt;- glm(Places~Age+log(Distance+1)+prix_km+Flex+Detour+Autoroute+log(Nombre+1)*Note, family=&quot;quasipoisson&quot;,data = df) export_summs(fit_logit,fit_poisson2, fit_poisson2) Table 10.2: Model 1Model 2Model 3 (Intercept)0.07&nbsp;&nbsp;&nbsp;&nbsp;0.69 ***0.69 *** (0.56)&nbsp;&nbsp;&nbsp;(0.15)&nbsp;&nbsp;&nbsp;(0.15)&nbsp;&nbsp;&nbsp; Age-0.00 **&nbsp;0.00 ***0.00 *** (0.00)&nbsp;&nbsp;&nbsp;(0.00)&nbsp;&nbsp;&nbsp;(0.00)&nbsp;&nbsp;&nbsp; log(Distance + 1)0.11 **&nbsp;-0.05 ***-0.05 *** (0.04)&nbsp;&nbsp;&nbsp;(0.01)&nbsp;&nbsp;&nbsp;(0.01)&nbsp;&nbsp;&nbsp; prix_km-16.30 ***7.69 ***7.69 *** (2.72)&nbsp;&nbsp;&nbsp;(0.75)&nbsp;&nbsp;&nbsp;(0.75)&nbsp;&nbsp;&nbsp; FlexPas d'indication0.05&nbsp;&nbsp;&nbsp;&nbsp;-0.02 *&nbsp;&nbsp;-0.02 *&nbsp;&nbsp; (0.04)&nbsp;&nbsp;&nbsp;(0.01)&nbsp;&nbsp;&nbsp;(0.01)&nbsp;&nbsp;&nbsp; FlexVoir avec le conducteur0.05&nbsp;&nbsp;&nbsp;&nbsp;-0.01&nbsp;&nbsp;&nbsp;&nbsp;-0.01&nbsp;&nbsp;&nbsp;&nbsp; (0.17)&nbsp;&nbsp;&nbsp;(0.05)&nbsp;&nbsp;&nbsp;(0.05)&nbsp;&nbsp;&nbsp; DetourDe 15 minutes max-0.01&nbsp;&nbsp;&nbsp;&nbsp;0.00&nbsp;&nbsp;&nbsp;&nbsp;0.00&nbsp;&nbsp;&nbsp;&nbsp; (0.04)&nbsp;&nbsp;&nbsp;(0.01)&nbsp;&nbsp;&nbsp;(0.01)&nbsp;&nbsp;&nbsp; DetourPlus de 15 mn-0.27&nbsp;&nbsp;&nbsp;&nbsp;0.13 **&nbsp;0.13 **&nbsp; (0.19)&nbsp;&nbsp;&nbsp;(0.05)&nbsp;&nbsp;&nbsp;(0.05)&nbsp;&nbsp;&nbsp; AutorouteOui0.50 ***-0.24 ***-0.24 *** (0.11)&nbsp;&nbsp;&nbsp;(0.03)&nbsp;&nbsp;&nbsp;(0.03)&nbsp;&nbsp;&nbsp; log(Nombre + 1)-0.64 **&nbsp;0.29 ***0.29 *** (0.21)&nbsp;&nbsp;&nbsp;(0.06)&nbsp;&nbsp;&nbsp;(0.06)&nbsp;&nbsp;&nbsp; Note0.00&nbsp;&nbsp;&nbsp;&nbsp;-0.01&nbsp;&nbsp;&nbsp;&nbsp;-0.01&nbsp;&nbsp;&nbsp;&nbsp; (0.11)&nbsp;&nbsp;&nbsp;(0.03)&nbsp;&nbsp;&nbsp;(0.03)&nbsp;&nbsp;&nbsp; log(Nombre + 1):Note0.13 **&nbsp;-0.06 ***-0.06 *** (0.05)&nbsp;&nbsp;&nbsp;(0.01)&nbsp;&nbsp;&nbsp;(0.01)&nbsp;&nbsp;&nbsp; N11719&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;11719&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;11719&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; AIC15784.58&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; BIC15873.00&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Pseudo R20.02&nbsp;&nbsp;&nbsp;&nbsp;0.02&nbsp;&nbsp;&nbsp;&nbsp;0.02&nbsp;&nbsp;&nbsp;&nbsp; *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05. 10.5.3 Modèle de régression beta Encore une autre variation avec le modèle de regression beta qui sattache à modéliser des variables de proportion par une loi de distribution beta dont la caractéristiques est dêtre souple et de sadapter à toute forme de distribution en respectant la contrainte de varier entre presque 0 et presque 1. Elle dépend de deux paramètres \\(\\alpha\\) et \\(\\beta\\) curve(dbeta(x, 4, 12), col = &quot;pink1&quot;, lwd = 2) curve(dbeta(x, 8, 4), add = TRUE, col = &quot;pink2&quot;, lwd = 2) curve(dbeta(x, 4, 4), add = TRUE, col = &quot;pink3&quot;, lwd = 2) La spécificité de la régression beta est de modéliser les deux paramètre de la fonction de distribution par deux équation linéraire. Lune ajustant la moyenne, lautre ajustant la dispersion. #recodage df$Occup[df$Occup==1]&lt;-.999 df$Occup[df$Occup==0]&lt;-.001 fit_beta1 &lt;- betareg(Occup~AgeClasse+log(Distance+1)+prix_km+Flex+Detour+Autoroute+log(Nombre+1)*Note, data = df) summary(fit_beta1) ## ## Call: ## betareg(formula = Occup ~ AgeClasse + log(Distance + 1) + prix_km + Flex + ## Detour + Autoroute + log(Nombre + 1) * Note, data = df) ## ## Standardized weighted residuals 2: ## Min 1Q Median 3Q Max ## -2.5648 -0.4721 -0.1841 0.1925 2.5672 ## ## Coefficients (mean model with logit link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.477618 0.346880 1.377 0.16854 ## AgeClasse30 -0.055681 0.030938 -1.800 0.07189 . ## AgeClasse40 -0.305290 0.035616 -8.572 &lt; 2e-16 *** ## AgeClasse50 -0.196017 0.039618 -4.948 7.51e-07 *** ## AgeClasse60 -0.134592 0.044980 -2.992 0.00277 ** ## AgeClasse70 -0.220373 0.099999 -2.204 0.02754 * ## log(Distance + 1) 0.119285 0.024997 4.772 1.82e-06 *** ## prix_km -17.809578 1.675785 -10.628 &lt; 2e-16 *** ## FlexPas d&#39;indication 0.029596 0.025342 1.168 0.24285 ## FlexVoir avec le conducteur -0.060868 0.108458 -0.561 0.57465 ## DetourDe 15 minutes max -0.008982 0.024985 -0.360 0.71922 ## DetourPlus de 15 mn -0.284942 0.119009 -2.394 0.01665 * ## AutorouteOui 0.540004 0.068146 7.924 2.30e-15 *** ## log(Nombre + 1) -0.846267 0.132436 -6.390 1.66e-10 *** ## Note -0.049875 0.066261 -0.753 0.45163 ## log(Nombre + 1):Note 0.179977 0.029493 6.102 1.04e-09 *** ## ## Phi coefficients (precision model with identity link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (phi) 1.08213 0.01123 96.36 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Type of estimator: ML (maximum likelihood) ## Log-likelihood: 2788 on 17 Df ## Pseudo R-squared: 0.0308 ## Number of iterations: 25 (BFGS) + 2 (Fisher scoring) Dans une régression bêta, a variable dépendante est distribuée selon la loi bêta avec une espérance  et une variance (1-)/(1+). Ainsi,  est un paramètre de précision : plus  est élevé, plus la variance est faible pour une moyenne  donnée. Le paramètre de précision  peut dépendre des régresseurs comme la moyenne. Cest ce qui est spécifié ci-dessous, en reliant le px au km à ce paramètre de dispersion. La valeur est élevée, la valeur p extrêmement faible, on en déduit queffectivement, plus le prix au kilomètre est élevé et plus faible est la variance de la distribution. fit_beta2 &lt;- betareg(Occup~AgeClasse+log(Distance+1)+prix_km+Flex+Detour+Autoroute+log(Nombre+1)*Note|prix_km, data = df) summary(fit_beta2) ## ## Call: ## betareg(formula = Occup ~ AgeClasse + log(Distance + 1) + prix_km + Flex + ## Detour + Autoroute + log(Nombre + 1) * Note | prix_km, data = df) ## ## Standardized weighted residuals 2: ## Min 1Q Median 3Q Max ## -2.8774 -0.4693 -0.1836 0.1952 4.4017 ## ## Coefficients (mean model with logit link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.36456 0.34639 1.052 0.29259 ## AgeClasse30 -0.04898 0.03093 -1.583 0.11332 ## AgeClasse40 -0.30011 0.03558 -8.436 &lt; 2e-16 *** ## AgeClasse50 -0.19509 0.03957 -4.930 8.20e-07 *** ## AgeClasse60 -0.13058 0.04488 -2.909 0.00362 ** ## AgeClasse70 -0.21690 0.09991 -2.171 0.02992 * ## log(Distance + 1) 0.12737 0.02497 5.100 3.39e-07 *** ## prix_km -16.90573 1.66364 -10.162 &lt; 2e-16 *** ## FlexPas d&#39;indication 0.03339 0.02530 1.320 0.18690 ## FlexVoir avec le conducteur -0.06949 0.10821 -0.642 0.52076 ## DetourDe 15 minutes max -0.00998 0.02496 -0.400 0.68922 ## DetourPlus de 15 mn -0.30605 0.11831 -2.587 0.00969 ** ## AutorouteOui 0.52620 0.06900 7.627 2.41e-14 *** ## log(Nombre + 1) -0.84903 0.13213 -6.426 1.31e-10 *** ## Note -0.04752 0.06600 -0.720 0.47155 ## log(Nombre + 1):Note 0.18104 0.02943 6.152 7.64e-10 *** ## ## Phi coefficients (precision model with log link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.6573 0.0877 -7.495 6.63e-14 *** ## prix_km 12.1320 1.4288 8.491 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Type of estimator: ML (maximum likelihood) ## Log-likelihood: 2826 on 18 Df ## Pseudo R-squared: 0.03075 ## Number of iterations: 29 (BFGS) + 3 (Fisher scoring) "],["modèle-de-survie.html", "Chapitre 11 10 - Modèle de survie", " Chapitre 11 10 - Modèle de survie voir étude de cas CartedeFidelité. "],["les-modèles-linéaires-hiérarchiques-hlm.html", "Chapitre 12 Les modèles linéaires hiérarchiques (HLM) 12.1 en guise dintroduction 12.2 Une application 12.3 Sem avec Lavaan", " Chapitre 12 Les modèles linéaires hiérarchiques (HLM) Les modèles de panels en économies, ou multi-niveaux en sociologie, sont caractérisés par le fait que les données sont un empilement de différents échantillons correspondant à une stratification. 12.1 en guise dintroduction Lexemple de la performance scolaire va nous éclairer. Supposons que lon veuille établir leffet dune mesure daptitude intellectuelle (par le exemple le score de QI) sur les notes obtenues. On va mesurer cette relation en recueillant les données deux classes et dans deux matières. La classe A est calme, la classe B est agitée, les matières sont les maths et le français. a=.1 foo_MC&lt;-as.data.frame(rnorm(20, mean=12, sd=10))%&gt;% rename(QI=1) %&gt;% group_by(row_number()) %&gt;% mutate(e=-5+10*runif(1))%&gt;% ungroup() %&gt;% mutate(Perf=a*QI-3+1+e,matiere=&quot;Math&quot;, classe=&quot;Calme&quot;) foo_MA&lt;-as.data.frame(rnorm(20, mean=12, sd=10))%&gt;% rename(QI=1) %&gt;% group_by(row_number()) %&gt;% mutate(e=-5+10*runif(1))%&gt;% ungroup() %&gt;% mutate(Perf=a*QI-3-3+e,matiere=&quot;Math&quot;, classe=&quot;Agité&quot;) foo_FC&lt;-as.data.frame(rnorm(20, mean=12, sd=10))%&gt;% rename(QI=1) %&gt;% group_by(row_number()) %&gt;% mutate(e=-5+10*runif(1))%&gt;% ungroup() %&gt;% mutate(Perf=a*QI+2+1+e,matiere=&quot;Français&quot;, classe=&quot;Calme&quot;) foo_FA&lt;-as.data.frame(rnorm(20, mean=12, sd=10))%&gt;% rename(QI=1) %&gt;% group_by(row_number()) %&gt;% mutate(e=-5+10*runif(1))%&gt;% ungroup() %&gt;% mutate(Perf=a*QI+2-4+1+e,matiere=&quot;Français&quot;, classe=&quot;Agité&quot;) foo&lt;-rbind(foo_MA, foo_MC, foo_FA,foo_FC) r =round(cor(foo$QI, foo$Perf),2) ggplot(foo, aes(x=QI, y=Perf))+ geom_point()+geom_smooth(method=&quot;lm&quot;)+ labs(title=paste(&quot;Corrélation :&quot;,r)) ggplot(foo, aes(x=QI, y=Perf, shape=matiere, color=classe))+ geom_point()+geom_smooth(method=&quot;lm&quot;)+facet_grid(matiere~classe)+ labs(title=paste(&quot;Corrélation :&quot;,r)) fit01&lt;-lm(Perf~QI, data=foo) fit02&lt;-lm(Perf~QI+matiere+classe, data=foo) fit03&lt;-lm(Perf~QI+matiere*classe, data=foo) export_summs(fit01,fit02, fit03, number_format = &quot;%.3g&quot;) Table 12.1: Model 1Model 2Model 3 (Intercept)-0.859&nbsp;&nbsp;-0.723&nbsp;&nbsp;&nbsp;-0.426&nbsp;&nbsp;&nbsp; (0.875)&nbsp;(0.823)&nbsp;&nbsp;(0.875)&nbsp;&nbsp; QI0.0435&nbsp;0.102 *&nbsp;0.105 ** (0.0585)(0.0395)&nbsp;(0.0396)&nbsp; matiereMath&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-5.78 ***-6.44 *** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0.667)&nbsp;&nbsp;(0.944)&nbsp;&nbsp; classeCalme&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.08 ***3.43 *** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0.696)&nbsp;&nbsp;(0.957)&nbsp;&nbsp; matiereMath:classeCalme&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.33&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1.34)&nbsp;&nbsp;&nbsp; N80&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;80&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;80&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; R20.007030.594&nbsp;&nbsp;&nbsp;0.599&nbsp;&nbsp;&nbsp; *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05. Dans ce petit exemple, les niveaux sont peu nombreux. On pourrait imaginer quils soient bien plus nombreux, par exemple en réalisant lenquête sur des dizaines de classes pour lesquelles le degré dagitation est variable et se distribue certainement de manière normale. Ne tenons plus en compte la matière pour le moment. On ne va prendre que la moyenne des maths. On peut écrire le modèle où \\[\\beta_{k}\\] représente le terme constant de chacune des k classes. \\[y_{ik}=\\beta_{k}+\\beta_{1}Aptitude_{i}+\\epsilon_{ik}\\] en supposant que les \\[\\beta_{k}\\] se distribue de manière normale avec une moyenne \\[\\overline{\\beta}\\] et une variance \\[\\mu_{k}\\], on peut réécrire léquation de la manière suivante \\[y_{ik}=\\overline{\\beta}+\\beta_{1}Aptitude_{i}+\\mu_{k}+\\epsilon_{ik}\\] Cest un modèle à composantes derreur où \\(\\mu_{k}\\) représente leffet de la classe. bbbba sffs 12.2 Une application voir données ESS df&lt;-readRDS(&quot;./data/ESS10fr.rds&quot;) library(ggcorrplot) foo&lt;-cbind(df[,5:26]) %&gt;% dplyr::select(-stfmjob)%&gt;% drop_na() r&lt;-cor(foo) ggcorrplot(r, hc.order = TRUE, type = &quot;lower&quot;, outline.col = &quot;white&quot;, colors = c(&quot;#6D9EC1&quot;, &quot;white&quot;, &quot;#E46726&quot;), lab=TRUE, lab_size=2) res.pca &lt;- PCA(foo, scale.unit = TRUE, ncp = 3, graph = TRUE) fviz_eig(res.pca, addlabels = TRUE, ylim = c(0, 50)) fviz_pca_var(res.pca, col.var = &quot;cos2&quot;, gradient.cols = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;), repel = TRUE # Avoid text overlapping ) foo&lt;-df%&gt;% dplyr::select(satisfaction_vie, satisfaction_care, satisfaction_institution, trust_institution, trust_personne, Year)%&gt;% drop_na() fit0&lt;-lm(satisfaction_vie~satisfaction_care+satisfaction_institution+trust_institution+trust_personne, data=df) fit1&lt;-lm(satisfaction_vie~satisfaction_care+satisfaction_institution+trust_institution+trust_personne+Year, data=df) fit2&lt;-lm(satisfaction_vie~age+genre+OP+satisfaction_care+satisfaction_institution+trust_institution+trust_personne+Year, data=df) export_summs(fit0, fit1,fit2, digits=3) Table 12.2: Model 1Model 2Model 3 (Intercept)3.909 ***4.028 ***4.883 *** (0.056)&nbsp;&nbsp;&nbsp;(0.070)&nbsp;&nbsp;&nbsp;(0.109)&nbsp;&nbsp;&nbsp; satisfaction_care0.153 ***0.153 ***0.136 *** (0.009)&nbsp;&nbsp;&nbsp;(0.009)&nbsp;&nbsp;&nbsp;(0.009)&nbsp;&nbsp;&nbsp; satisfaction_institution0.212 ***0.213 ***0.211 *** (0.009)&nbsp;&nbsp;&nbsp;(0.009)&nbsp;&nbsp;&nbsp;(0.009)&nbsp;&nbsp;&nbsp; trust_institution0.010&nbsp;&nbsp;&nbsp;&nbsp;0.012&nbsp;&nbsp;&nbsp;&nbsp;0.000&nbsp;&nbsp;&nbsp;&nbsp; (0.010)&nbsp;&nbsp;&nbsp;(0.010)&nbsp;&nbsp;&nbsp;(0.011)&nbsp;&nbsp;&nbsp; trust_personne0.583 ***0.574 ***0.611 *** (0.022)&nbsp;&nbsp;&nbsp;(0.022)&nbsp;&nbsp;&nbsp;(0.022)&nbsp;&nbsp;&nbsp; Year2004&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-0.150 *&nbsp;&nbsp;-0.191 **&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0.060)&nbsp;&nbsp;&nbsp;(0.061)&nbsp;&nbsp;&nbsp; Year2006&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-0.204 ***-0.256 *** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0.058)&nbsp;&nbsp;&nbsp;(0.059)&nbsp;&nbsp;&nbsp; Year2008&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-0.169 **&nbsp;-0.243 *** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0.058)&nbsp;&nbsp;&nbsp;(0.059)&nbsp;&nbsp;&nbsp; Year2010&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-0.191 **&nbsp;-0.226 *** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0.060)&nbsp;&nbsp;&nbsp;(0.061)&nbsp;&nbsp;&nbsp; Year2012&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-0.140 *&nbsp;&nbsp;-0.160 **&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0.059)&nbsp;&nbsp;&nbsp;(0.060)&nbsp;&nbsp;&nbsp; Year2014&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-0.007&nbsp;&nbsp;&nbsp;&nbsp;-0.055&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0.060)&nbsp;&nbsp;&nbsp;(0.061)&nbsp;&nbsp;&nbsp; Year2016&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.010&nbsp;&nbsp;&nbsp;&nbsp;-0.018&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0.058)&nbsp;&nbsp;&nbsp;(0.059)&nbsp;&nbsp;&nbsp; Year2018&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-0.102&nbsp;&nbsp;&nbsp;&nbsp;-0.105&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0.059)&nbsp;&nbsp;&nbsp;(0.060)&nbsp;&nbsp;&nbsp; age26-35&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-0.133 *&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0.055)&nbsp;&nbsp;&nbsp; age36-45&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-0.359 *** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0.054)&nbsp;&nbsp;&nbsp; age46-65&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-0.616 *** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0.048)&nbsp;&nbsp;&nbsp; age66-75&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-0.539 *** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0.057)&nbsp;&nbsp;&nbsp; age75&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-0.642 *** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0.063)&nbsp;&nbsp;&nbsp; genreH&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.039&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0.027)&nbsp;&nbsp;&nbsp; OPDroite&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-0.104&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0.082)&nbsp;&nbsp;&nbsp; OPCentre Droit&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-0.243 **&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0.077)&nbsp;&nbsp;&nbsp; OPNi G ni D&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-0.408 *** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0.075)&nbsp;&nbsp;&nbsp; OPCentre Gauche&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-0.437 *** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0.076)&nbsp;&nbsp;&nbsp; OPGauche&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-0.423 *** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0.082)&nbsp;&nbsp;&nbsp; OPExtrême gauche&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-0.227 *&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0.092)&nbsp;&nbsp;&nbsp; N15909&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;15909&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;15082&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; R20.187&nbsp;&nbsp;&nbsp;&nbsp;0.189&nbsp;&nbsp;&nbsp;&nbsp;0.205&nbsp;&nbsp;&nbsp;&nbsp; *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05. plot_summs(fit0, fit1,fit2) fit3&lt;-lmer(satisfaction_vie~satisfaction_care+satisfaction_institution+trust_institution+trust_personne+(1|Year)+(1|age)+(1|OP)+(1|genre),data = df ) summary(fit3) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: satisfaction_vie ~ satisfaction_care + satisfaction_institution + ## trust_institution + trust_personne + (1 | Year) + (1 | age) + ## (1 | OP) + (1 | genre) ## Data: df ## ## REML criterion at convergence: 58389.3 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -4.3979 -0.5826 0.0737 0.6521 3.4952 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Year (Intercept) 0.008060 0.08978 ## OP (Intercept) 0.025631 0.16010 ## age (Intercept) 0.068338 0.26141 ## genre (Intercept) 0.000441 0.02100 ## Residual 2.795825 1.67207 ## Number of obs: 15082, groups: Year, 9; OP, 7; age, 6; genre, 2 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 4.1081843 0.1403198 29.28 ## satisfaction_care 0.1364299 0.0088328 15.45 ## satisfaction_institution 0.2109299 0.0093703 22.51 ## trust_institution -0.0005267 0.0104985 -0.05 ## trust_personne 0.6105600 0.0220637 27.67 ## ## Correlation of Fixed Effects: ## (Intr) stsfctn_c stsfctn_n trst_n ## satsfctn_cr -0.204 ## stsfctn_nst 0.023 -0.227 ## trst_nstttn -0.066 -0.224 -0.439 ## trust_prsnn -0.217 -0.058 -0.101 -0.242 12.3 Sem avec Lavaan On reste sur le jeu de données précédent "],["arbres-de-décision.html", "Chapitre 13 Arbres de Décision 13.1 Construire un arbre de décision 13.2 Mise en oeuvre avec Partykit 13.3 forêts aléatoires", " Chapitre 13 Arbres de Décision Lobjectif de cette note est double. Le premier est une introduction aux méthodes darbres de décision et leur généralisation récente par les random forests. Le second est dintroduire à lapproche dapprentissage et de test, autrement aux machine learning avec le package caret qui facilite la condition des opérations déchantillonngage, de découpage des échanges et de production des indicateurs. 13.1 Construire un arbre de décision Les origines et le principe Cest une approche qui remonte à Morgan and Sonquist (1963) généralisés aux variables qualitatives avec Chaid (Kass (1980)) : Le principe général suis le pseudo algorithme suivant : pour chaque variable potentiellement explicative, trouver le meilleur découpage (dichotomique), cest à dire celui qui va différencier au mieux la variable de réponse. Choisir parmi les variables et leur dichotomitsation celle qui répond au même critère que précedemment recommencer lopération à 1 Il peut sappliquer à une variable quantitative ( regression) ou qualitative ( chaid) puis Cart avec breiman. Breiman (1998) 13.2 Mise en oeuvre avec Partykit Le package partykit a pour objectif de représenter les arbres de décisions. Il inclue cependant plusieurs méthodes darbres de decisions, en en particulier une approche ctree Hothorn et al. (2006) dont le principe est. La méthode est incluse dans partykit Hothorn and Zeileis (2015) Avec partykit on contrôle la construction de larbre sur différents critères, par exemple : * le type de test employé pour prendre la décision * le nombre minimum dindividus dans une feuille terminale https://apiacoa.org/blog/2014/02/initiation-a-rpart.fr.html https://apiacoa.org/blog/2014/02/initiation-a-rpart.fr.html knitr::opts_chunk$set(echo = TRUE, include=TRUE, cache=TRUE, message=FALSE, warning=FALSE) library(partykit) library(tidyverse) #lecture du fichier df&lt;-readRDS(&quot;./data/last.rds&quot;) %&gt;%drop_na() df$Age&lt;-as.factor(df$Age) df$Sexe&lt;-as.factor(df$Sexe) df$Education&lt;-factor(df$Education, ordered = FALSE ) df$Situation2&lt;-as.factor(df$Situation2) df$Situation3&lt;-as.factor(ifelse(df$Situation&lt;5,&quot;degradation&quot;,&quot; Amelioration&quot;)) table(df$Situation3) ## ## Amelioration degradation ## 20327 19071 fit &lt;-ctree(Situation3 ~ Age+Sexe+Education, data=df) print(fit) ## ## Model formula: ## Situation3 ~ Age + Sexe + Education ## ## Fitted party: ## [1] root ## | [2] Age in 18 - 24 ans, 25 - 34 ans ## | | [3] Age in 18 - 24 ans ## | | | [4] Sexe in Un homme ## | | | | [5] Education in Sans diplôme, Brevet des collèges, Bac +5 (Master, école d&#39;ingénieurs, d&#39;arts...): Amelioration (n = 725, err = 29.2%) ## | | | | [6] Education in CAP/BEP, Bac (général, pro et technologique), Bac +2 (BTS ou autre), Bac +3/4 (Licence, Maîtrise), Bac +7 et plus (doctorat, thèse, etc.): Amelioration (n = 1710, err = 36.2%) ## | | | [7] Sexe in Une femme: Amelioration (n = 3963, err = 38.3%) ## | | [8] Age in 25 - 34 ans ## | | | [9] Education in Sans diplôme, CAP/BEP, Bac (général, pro et technologique), Bac +2 (BTS ou autre), Bac +3/4 (Licence, Maîtrise): Amelioration (n = 4956, err = 43.9%) ## | | | [10] Education in Brevet des collèges, Bac +5 (Master, école d&#39;ingénieurs, d&#39;arts...), Bac +7 et plus (doctorat, thèse, etc.): Amelioration (n = 1347, err = 37.2%) ## | [11] Age in 35 - 44 ans, 45 - 54 ans, 55 - 64 ans, 65 ans et plus ## | | [12] Age in 35 - 44 ans ## | | | [13] Education in Sans diplôme, CAP/BEP, Bac (général, pro et technologique), Bac +2 (BTS ou autre), Bac +3/4 (Licence, Maîtrise) ## | | | | [14] Sexe in Un homme: Amelioration (n = 2588, err = 47.6%) ## | | | | [15] Sexe in Une femme: degradation (n = 4926, err = 48.8%) ## | | | [16] Education in Brevet des collèges, Bac +5 (Master, école d&#39;ingénieurs, d&#39;arts...), Bac +7 et plus (doctorat, thèse, etc.): Amelioration (n = 1323, err = 41.3%) ## | | [17] Age in 45 - 54 ans, 55 - 64 ans, 65 ans et plus ## | | | [18] Sexe in Un homme ## | | | | [19] Education in Sans diplôme, CAP/BEP, Bac (général, pro et technologique), Bac +2 (BTS ou autre), Bac +3/4 (Licence, Maîtrise), Bac +7 et plus (doctorat, thèse, etc.): degradation (n = 5675, err = 48.3%) ## | | | | [20] Education in Brevet des collèges, Bac +5 (Master, école d&#39;ingénieurs, d&#39;arts...): Amelioration (n = 1282, err = 45.6%) ## | | | [21] Sexe in Une femme ## | | | | [22] Education in Sans diplôme, Brevet des collèges, Bac +3/4 (Licence, Maîtrise), Bac +5 (Master, école d&#39;ingénieurs, d&#39;arts...): degradation (n = 4250, err = 45.6%) ## | | | | [23] Education in CAP/BEP, Bac (général, pro et technologique), Bac +2 (BTS ou autre), Bac +7 et plus (doctorat, thèse, etc.) ## | | | | | [24] Age in 45 - 54 ans: degradation (n = 2929, err = 43.3%) ## | | | | | [25] Age in 55 - 64 ans, 65 ans et plus: degradation (n = 3724, err = 39.4%) ## ## Number of inner nodes: 12 ## Number of terminal nodes: 13 pred &lt;- predict(fit, df) library(caret) cm = confusionMatrix(df$Situation3, pred) print(cm) ## Confusion Matrix and Statistics ## ## Reference ## Prediction Amelioration degradation ## Amelioration 10507 9820 ## degradation 7387 11684 ## ## Accuracy : 0.5633 ## 95% CI : (0.5583, 0.5682) ## No Information Rate : 0.5458 ## P-Value [Acc &gt; NIR] : 1.769e-12 ## ## Kappa : 0.129 ## ## Mcnemar&#39;s Test P-Value : &lt; 2.2e-16 ## ## Sensitivity : 0.5872 ## Specificity : 0.5433 ## Pos Pred Value : 0.5169 ## Neg Pred Value : 0.6127 ## Prevalence : 0.4542 ## Detection Rate : 0.2667 ## Detection Prevalence : 0.5159 ## Balanced Accuracy : 0.5653 ## ## &#39;Positive&#39; Class : Amelioration ## library(ggparty) autoplot(fit) #library(irks) #rules&lt;-ct_rules(fit) #rules %&gt;% # kable() %&gt;% # kable_styling(bootstrap_options = &quot;striped&quot;, font_size = 10) https://topepo.github.io/caret/ 13.3 forêts aléatoires voire le cas "],["premiers-éléments-de-machine-learning.html", "Chapitre 14 Premiers éléments de Machine Learning 14.1 une typologie de modèles 14.2 forêts aléatoires", " Chapitre 14 Premiers éléments de Machine Learning Le Machine Learning ou en français lapprentissage automatique désigne un processus par lequel on construit un modèle prédictif. Cest en général un modèle qui permet de classifier des observations à parti de leurs caractéristiques. Distinguer les spam du ham, les mauvais emprunteurs des bons emprunteurs, des cookies brulés de ceux bien cuits sur chaine de production. Trois éléments essentiels le caractérise. un modèle qui asssocie à une variable y que lon cherche à prédire un vecteur de caractéristiques x. cest une fonction où y=f(x|theta)+e. Aujourdhui des dizaines de fonction sont disponibles un processus de validation. Su le modèle est entrainé sur un set de données dit dapprentissage, sa qualité prédictive est établie sur un set de données qui na pas contribué à lestimation des paramètres. 14.1 une typologie de modèles 14.1.1 le modèle linéaire 14.1.2 le modèle logit 14.1.3 les modèles à régularisation 14.1.4 les random forest 14.2 forêts aléatoires voire le cas et pour le texte : et surtout celui-ci "],["annexes.html", "Chapitre 15 20 Annexes 15.1 Données Eric-ESS 15.2 fichier Airbnb Bruxelles 15.3 ", " Chapitre 15 20 Annexes 15.1 Données Eric-ESS Extraction France Le fichier est disponible sous format ESS10fr.rds df &lt;- read_csv(&quot;Data/ESS-Data-Wizard-subset-2022-10-06.csv&quot;) df$Year&lt;-2000 #recodage des variables independantes df$Year[df$essround==1]&lt;-2002 df$Year[df$essround==2]&lt;-2004 df$Year[df$essround==3]&lt;-2006 df$Year[df$essround==4]&lt;-2008 df$Year[df$essround==5]&lt;-2010 df$Year[df$essround==6]&lt;-2012 df$Year[df$essround==7]&lt;-2014 df$Year[df$essround==8]&lt;-2016 df$Year[df$essround==9]&lt;-2018 df$Year&lt;-as.factor(df$Year) df$OP&lt;-&quot; &quot; #ggplot(df,aes(x=lrscale))+geom_histogram() df$OP[df$lrscale==0] &lt;- &quot;Extrême gauche&quot; df$OP[df$lrscale==1] &lt;- &quot;Gauche&quot; df$OP[df$lrscale==2] &lt;- &quot;Gauche&quot; df$OP[df$lrscale==3] &lt;- &quot;Centre Gauche&quot; df$OP[df$lrscale==4] &lt;- &quot;Centre Gauche&quot; df$OP[df$lrscale==5] &lt;- &quot;Ni G ni D&quot; df$OP[df$lrscale==6] &lt;- &quot;Centre Droit&quot; df$OP[df$lrscale==7] &lt;- &quot;Centre Droit&quot; df$OP[df$lrscale==8] &lt;- &quot;Droite&quot; df$OP[df$lrscale==9] &lt;- &quot;Droite&quot; df$OP[df$lrscale==10] &lt;- &quot;Extrême droite&quot; #la ligne suivante est pour ordonner les modalités de la variables df$OP&lt;-factor(df$OP,levels=c(&quot;Extrême droite&quot;,&quot;Droite&quot;,&quot;Centre Droit&quot;,&quot;Ni G ni D&quot;,&quot;Centre Gauche&quot;,&quot;Gauche&quot;,&quot;Extrême gauche&quot;)) df$genre&lt;-&quot; &quot; df$genre[df$gndr==1]&lt;-&quot;H&quot; df$genre[df$gndr==2]&lt;-&quot;F&quot; df$age&lt;-&quot; &quot; df$age[df$agea&lt;26]&lt;-&quot;25&lt;&quot; df$age[df$agea&gt;25 &amp; df$agea&lt;36]&lt;-&quot;26-35&quot; df$age[df$agea&gt;35 &amp; df$agea&lt;46]&lt;-&quot;36-45&quot; df$age[df$agea&gt;45 &amp; df$agea&lt;66]&lt;-&quot;46-65&quot; df$age[df$agea&gt;65 &amp; df$agea&lt;76]&lt;-&quot;66-75&quot; df$age[df$agea&gt;75]&lt;-&quot;75&gt;&quot; df$age&lt;-factor(df$age,levels=c(&quot;25&lt;&quot;,&quot;26-35&quot;,&quot;36-45&quot;,&quot;46-65&quot;,&quot;66-75&quot;, &quot;75&gt;&quot;)) df_confiance&lt;-df%&gt;% dplyr::select(trstep, trstlgl,trstplc,trstplt,trstprl,trstprt,trstun,pplfair,pplhlp, ppltrst) %&gt;% mutate(trstep=ifelse(trstep==77 |trstep==88| trstep==66,NA,trstep), trstlgl=ifelse(trstlgl==77 |trstlgl==88| trstlgl==66,NA,trstlgl), trstplc=ifelse(trstplc==77 |trstplc==88| trstplc==66,NA,trstplc), trstplt=ifelse(trstplt==77 |trstplt==88| trstplt==66,NA,trstplt), trstprt=ifelse(trstprt==77 |trstprt==88| trstprt==66,NA,trstprt), trstun=ifelse(trstun==77 |trstun==88| trstun==66,NA,trstun), trstprl=ifelse(trstprl==77 |trstprl==88| trstprl==66,NA,trstprl), pplfair=ifelse(pplfair==77 |pplfair==88| pplfair==66,NA,pplfair), pplhlp=ifelse(pplhlp==77 |pplhlp==88| pplhlp==66,NA,pplhlp), ppltrst=ifelse(ppltrst==77 |ppltrst==88| ppltrst==66,NA,ppltrst), trust_institution= (trstep+trstlgl+trstplc+trstplt+trstprl+trstprt+trstun)/7, trust_personne= (pplfair+pplhlp+ppltrst)/7 ) df_satisfaction&lt;-df%&gt;% dplyr::select(stfeco,stfedu,stfgov, stfhlth, stflife,stfmjob, happy) %&gt;% mutate(stfeco=ifelse(stfeco==77 |stfeco==88| stfeco==66,NA,stfeco), stfedu=ifelse(stfedu==77 |stfedu==88 | stfedu==66,NA,stfedu), stfgov=ifelse(stfgov==77 |stfgov==88 | stfgov==66, NA, stfgov), stfhlth=ifelse(stfhlth==77 |stfhlth==88|stfhlth==66,NA,stfhlth), stflife=ifelse(stflife==77 |stflife==88|stflife==66,NA,stflife), happy=ifelse(happy==77 |happy==88|happy==66,NA,happy), stfmjob=ifelse(stfmjob==77 |stfmjob==88|stfmjob==66,NA,stfmjob), satisfaction_vie=(stflife+happy)/2, #à examiner satisfaction_institution=(stfeco+stfgov)/2, satisfaction_care=(stfhlth+stfedu)/2 ) df_sample&lt;-df%&gt;% dplyr::select(Year, age, OP, genre) df&lt;-cbind(df_sample, df_satisfaction, df_confiance) saveRDS(df, &quot;./data/ESS10fr.rds&quot;) 15.2 fichier Airbnb Bruxelles La source est InsideAirbnb. Lextraction est de 2020. 15.3 "]]
